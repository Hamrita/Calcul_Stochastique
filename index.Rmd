--- 
title: "Calcul stochastique"
author: "__Mohamed Essaied Hamrita__"
date: "__2021__"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
biblio-style: plain
link-citations: yes
nocite: '@*'
description: ""
---
```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'markovchain'), 'packages.bib')

```

```{r include=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    #sprintf("\\textcolor{%s}{%s}", color, x)
    paste("\\textcolor{",color,"}{\\textbf{",x,"}}",sep="")
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```

# Rappel sur le calcul des probabilités

## Notion de probabilité

Une notion basique dans la théorie des probabilités est l'__expérience aléatoire__ dont on ne connait pas son résultat en avance. L'ensemble de tous les résultats de l'expérience est l'__ensemble des possibles__ ou encore l'__univers__ et noté $\Omega$.

Un __évènement__ est un sous ensemble de l'univers. 

On donne quelques exemples:

  __1.__ Si l'expérience consiste à jeter une pièce de monnaie, alors $\Omega=\{P,F\}$.
  
  __2.__ Si l'expérience consiste à jeter un dé cubique dont ses faces sont numérotées de 1 à 6, alors \(\Omega=\{1,2,3,4,5,6 \} \).
   
Pour chaque évènement $E$ de l'univers $\Omega$, on définit un nombre $P(E)$ qui satisfait les axiomes suivants:

  * __Axiome 1:__ $0 \leq P(E) \leq 1$;
  * __Axiome 2:__ $P(\Omega)=1$;
  * __Axiome 3:__ Pour toute séquence des évènements \( E_1 , E_2, \ldots , E_n \) qui sont mutuellement exclusifs (\( E_i \cap E_j=\emptyset , \; \forall \; i \neq j \) et $\displaystyle{\bigcup\limits_{i=1}^nE_i=\Omega}$), on a 
$$
P \left(\bigcup\limits_{i=1}^nE_i \right)=\sum_{i=1}^n P(E_i)
$$

Quelques conséquences de ces axiomes sont tirées:

   * Si \( E \subset F \), alors \( P(E) \leq P(F) \).
   *  \( P ( \bar{E})=1-P(E) \) où \( \bar{E} \) est le complémentaire de \( E \).
   * \( P \left(\bigcup\limits_{i=1}^nE_i \right)=\displaystyle\sum_{i=1}^n P(E_i) \), lorsque \( E_i\) sont mutuellement exclusifs.
   * \( P \left(\bigcup\limits_{i=1}^nE_i \right)\leq\displaystyle \sum_{i=1}^n P(E_i) \) (Inégalité de Boole).
   
```{example}
L'expérience consiste à lancer une pièce de monnaie équilibrée. Donc \(P(\{P\})= P(\{F\})=0.5\)
```

```{example}
On lance un dé cubique équilibré dont ses faces sont numérotées de 1 à 6.

\(P(\{i \})= \dfrac{1}{6}, \; \forall\, i=1,2,\ldots,6 \).

La probabilité d'obtenir un nombre pair est

\(P(\{2,4,6\})= P(\{2 \})+P(\{4 \}) +P(\{6 \}) = \dfrac{1}{2}\).
```

### Probabilité conditionnelle

```{definition}
Soient $A$ et $B$ deux évènements tels que $P(B)\neq0$. La probabilité de __A sachant B__ est le nombre
$$
  P(A|B)=\frac{P(A \cap B)}{P(B)}
$$
```

```{example}
Une urne contient 10 boules numérotées de 1 à 10 indiscernables au toucher. On tire au hasard une boule. Sachant que le numéro de la boule tirée est au moins égale à 5, quelle est la probabilité qu'il soit égale à 10?

Soit $A$ l'évènement d'avoir une boule portant le numéro 10. Soit $B$ l'évènement d'avoir une boule portant un numéro supèrieur ou égale à 5. La probabilité demandée est $P(A|B)$
$$
P(A|B)=\frac{P(A \cap B)}{P(B)}=\frac{P(A)}{P(B)}=\frac{1/10}{6/10}=1/6
$$
```

### Indépendance

```{definition}
On dit que deux évènements, A et B, sont __indépendants__ si $P(A\cap B)=P(A)\times P(B)$
```

Si $A$ et $B$ deux évènements indépendants, alors $P(A|B)=P(A)$ et $P(B|A)=P(B)$.

```{example}
On lance deux dés cubiques équilibrés numérotés de 1 à 6. Soit $A$ l'événement d'obtenir une somme égale six et $B$ désigne l'événement où le premier dé est égal à quatre. 

Vérifier que les deux évènements $A$ et $B$ sont indépendants.

$P(A\cap B)=p(\{4,2 \})=1/36$ et $P(A)=1/6$, $P(B)=1/6$. Donc $P(A)\times P(B)=1/6 \times 1/6 =1/36 =P(A\cap B)$. Ainsi, les évènements $A$ et $B$ sont indépendants.
```


## Variables aléatoires

```{definition}
Soit une expérience aléatoire d'univers $\Omega$.

Une variable aléatoire $X$ est une application de l'ensemble $\Omega$ vers un ensemble de réalisations.

Pour tout évènement $A$, on $P(X \in A)=P(X^{-1}(A))$ où $X^{-1}(A)$ est l'évènement comprenant tous les éléments $\omega \in \Omega$ tels que $X(\omega) \in A$.
```

La __fonction de répartition__ $F$ d'une variable aléatoire $X$ est définie par 
$$
F(x)=P(X \leq x)=P(X \in ]-\infty, x]), \; \forall \, x \in \mathbb{R}
$$
Une variable aléatoire $X$ est dite __discrète__ si son ensemble des valeurs possibles est dénombrables. Dans ce cas, on a
$$
F(x)=\sum_{k \leq x }P(X=k)
$$
Une variable aléatoire $X$ est dite __continue__ s'il existe une fonction $f(x)$, appelée *densité de probabilité*, telle que
$$
P(X\in B)=\int_Bf(x)dx \; \text{ pour tout ensemble  }B
$$
Puisque $F(x)=\displaystyle \int_{-\infty}^x f(x) dx$, alors
$$
f(x)=\frac{d}{dx}F(x)
$$
La fonction de répartition __jointe__ d'un couple aléatoire $X$ et $Y$ est $F(x,y)=P(X \leq x, Y \leq y)$.

Les fonctions de répartition de $X$ et $Y$,
$$
F_X(x)=P(X\leq x) \text{ et }F_Y(y)=P(Y\leq y)
$$
peuvent être déduites de $F(x,y)$. En effet,
$$
F_X(x)=\lim_{y \longrightarrow \infty}F(x,y) \; \text{ et }\;F_Y(y)=\lim_{x \longrightarrow \infty}F(x,y)
$$
Les variables aléatoires $X$ et $Y$ sont __indépendantes__ si
$$
F(x,y)=F_X(x) F_Y(y)
$$
$X$ et $Y$ sont __continues__ s'il existe une fonction $f(x,y)$, dite densité de probabilité *jointe*, telle que
$$
P(X\in A, Y\in B)=\int_A\int_B f(x,y)dxdy\;\; \forall A,B
$$
La fonction de répartition d'une suite de $n$ variables aléatoires $X_1, X_2, \ldots, X_n$ est définie par:
$$
F(X_1, X_2, \ldots, X_n)=P(X_1\leq x_1, X_2\leq x_2, \ldots, X_n \leq x_n)
$$
et sont indépendantes si
$$
F(X_1, X_2, \ldots, X_n)=F_{X_1}(x_1)F_{X_2}(x_2)\ldots F_{X_n}(x_n)
$$

## Espérance mathématique

```{definition}
L'__espérance mathématique__ ou __moyenne__ d'une variavle aléatoire $X$, notée $\mathbb{E}(X)$, est définie par:

   \begin{align*}
\mathbb{E}(X) & = \int_{\mathbb{R}}x dF(x)\\
 & = \begin{cases}
   \displaystyle \sum_x xP(X=x)\; \text{si }X \text{ est discrète}\\
 \displaystyle  \int_{\mathbb{R}} xf(x)dx \; \text{si }X \text{ est continue}
     \end{cases}
  \end{align*}

```

De même, on définit l'espérance d'une fonction de $X$, $g(X)$, par:
$$
\mathbb{E}\left[ g(X)\right]=\int_{\mathbb{R}}x dF_g(x)=\int_{\mathbb{R}}g(x) dF(x)
$$
L'espérance d'une somme de variables aléatoires est la somme des espérances:
$$
\mathbb{E}\left[\sum_{i=1}^n X_i \right]=\sum_{i=1}^n\mathbb{E}(X_i)
$$
La __variance__ d'une variable aléatoire, $X$, est définie par
$$
\mathbb{V}(X)=\mathbb{E}\left[{(X-\mathbb{E}(X))}^2 \right]=\mathbb{E}(X^2)-{\mathbb{E}(X)}^2
$$
Deux variables aléatoires, $X$ et $Y$, sont dites __non corréllées__ si leur covariance, définie par:
$$
\text{Cov}(X,Y)=\mathbb{E}\left[(X-\mathbb{E}(X))(Y-\mathbb{E}(Y))\right]=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
$$
est __nulle__. Noter que si $X$ et $Y$ sont indépendantes, alors elles sont non corréllées ($\text{Cov}(X,Y)=0$).

### Propriétés de la covariance{-}

Pour toutes variables aléatoires $X, Y, Z$ et $a \in \mathbb{R}$, on a:
  
__1.__ Cov$(X,X)=\mathbb{V}(X)$ et Cov$(X,Y)=$ Cov$(Y,X) $

__2.__ Cov$(aX,Y)=a$Cov$(X,Y)$.

__3.__ Cov$(X,Y+Z)=$ Cov$(X,Y)+$ Cov$(X,Z)$.

Une généralisation de la troisième propriétés est donnée par:
$$
\text{Cov}\left( \sum_{i=1}^nX_i,\sum_{j=1}^m Y_i \right)=\sum_{i=1}^n \sum_{j=1}^m \text{Cov}(X_i,Y_j)
$$
Une expression utile pour la variance de la somme des variables aléatoires peut être déduite comme suit:
\begin{align*}
\mathbb{V}\left(\sum_{i=1}^nX_i \right)& =\text{Cov}\left(\sum_{i=1}^nX_i,\sum_{i=1}^nX_i\right)\\
&=\sum_{i=1}^n \sum_{j=1}^n\text{Cov}(X_i,X_j)\\
&=\sum_{i=1}^n\text{Cov}(X_i,X_i)+\sum_{i=1}^n\sum_{i\neq j}\text{Cov}(X_i,X_j)\\
&=\sum_{i=1}^n\mathbb{V}(X_i)+2\sum_{i=1}^n\sum_{j <i} \text{Cov}(X_i,X_j)
\end{align*}

```{definition}
Si $X_1,X_2,\ldots, X_n$ sont *indépendantes et identiquement distribuées*, noté $X_i \sim iid$, d'espérance $m$ et de variance $\sigma^2$, alors:

   * $\overline{X}=\displaystyle \frac{1}{n}\sum_{i=1}^n X_i$ est appelée moyenne empirique.
   * $\mathbb{E}(\overline{X})=m$ et $\mathbb{V}(\overline{X})=\displaystyle \frac{\sigma^2}{n}$.
   * Cov$(\overline{X},X_i-\overline{X})=0,$ $i=1,2,\ldots,n$.


```

```{example}
Calculer la variance d'une variable aléatoire $X$ suivant une loi binomiale de paramètres $n$ et $p$.

Puisqu'une telle variable aléatoire représente le nombre de succès dans $n$ essais indépendants lorsque chaque essai a une probabilité commune p d'être un succès,
nous pouvons écrire
$$
X=X_1+X_2+\ldots+X_n
$$
où $X_i \stackrel{iid}{\sim}B(p)$ telle que
```

$$
X_i=\begin{cases}
1 \text{ si le ième issue est un succés}\\
0 \text{ sinon}
\end{cases}
$$
Par conséquent, on aura $\mathbb{V}(X)=\displaystyle \sum_{i=1}^n\mathbb{V}(X_i)$, Or
\begin{align*}
\mathbb{V}(X_i)&=\mathbb{E}(X_i^2)-{\mathbb{E}(X_i)}^2\\
&=\mathbb{E}(X_i)-{\mathbb{E}(X_i)}^2 \text{ car } X_i^2=X_i\\
&= p-p^2= p(1-p)
\end{align*}
Donc $\mathbb{V}(X)=np(1-p)$.

## Fonctions génératrices des moments

```{definition}
La fonction génératrice des moments $\phi(t)$ de la variable aléatoire $X$ est définie pour tout $t \in \mathbb{R}$ par
\begin{align*}
\phi(t)&=\mathbb{E}\left[e^{tX} \right]\\
&= \begin{cases}
\displaystyle \sum_x e^{tx}P(X=x)\text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R}}e^{tx} f(x)dx \text{ si } X \text{ est continue}
\end{cases}
\end{align*}

```

$\phi(t)$ est appelée fonction génératrice des moments car tous les moments de $X$ peuvent être obtenues par les dérivées successives de $\phi(t)$. Par exemple

\begin{align*}
\phi'(t)&=\dfrac{d}{dt}\mathbb{E}\left[e^{tX} \right] \\
&=\mathbb{E}\left[\dfrac{d}{dt}(e^{tX}) \right]=\mathbb{E}\left[Xe^{tX} \right]
\end{align*}

Par conséquent $\phi'(0)= \mathbb{E}(X)$.

D'une manière plus générale, $\phi^n(0)=\mathbb{E}\left( {X}^n\right),\; n \geq 1$.

Une propriété importante des fonctions génératrices des moments est que la fonction génératrice des moments de la somme des variables aléatoires indépendantes est simplement le produit des fonctions génératrices des moments individuelles. Pour voir cela, supposons que $X$ et $Y$ sont indépendantes et ont respectivement des fonctions génératrices des moments $\phi_X(t)$ et $\phi_Y(t)$. Alors la fonction génératrice des moments de $X+Y$ est donnée par:
\begin{align*}
\phi_{X+Y}(t)&=\mathbb{E}\left(e^{t(X+Y)} \right)=\mathbb{E}\left(e^{tX}e^{tY)} \right)\\
&=\mathbb{E}\left(e^{tX}\right) \mathbb{E}\left(e^{tY}\right)=\phi_X(t)\phi_Y(t)
\end{align*}

```{example, name="Loi Binomiale de paramètres n et p"}
\begin{align*}
\phi(t)&=\mathbb{E}\left[e^{tX} \right]=\sum_{k=0}^ne^{tk}C^n_kp^k{(1-p)}^{n-k}\\
&=\sum_{k=0}^nC^n_k{\left(pe^t\right)}^k{(1-p)}^{n-k}
\end{align*}

```
Or, d'après la formule de Binôme, on a
$$
{(a+b)}^n=\sum_{k=0}^nC_n^ka^kb^{n-k}
$$
d'où \(\phi(t)={\left( pe^t+(1-p)\right)}^n\) et par conséquent
$$
\phi'(t)=n{\left(pe^t+1-p \right)}^{n-1}pe^t
$$
D'où $\mathbb{E}(X)=\phi'(0)=np$.

Dérivons une deuxième fois la fonction $\phi(t)$, on obtient

$$
\phi''(t)=n(n-1){\left(pe^t+1-p \right)}^{n-2}{\left(pe^t\right)}^2+n{\left(pe^t+1-p \right)}^{n-1}pe^t
$$
En déduit, alors
$$
\mathbb{E}(X^2)=\phi''(0)=n(n-1)p^2+np
$$
Donc, on peut déduire la variance de $X$.
$$
\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}{(X)}^2=\phi''(0)-{\left( \phi'(0)\right)}^2=np(1-p)
$$
```{example, name="Loi Normale standard"}
```
\begin{align*}
\mathbb{E}\left(e^{tX} \right)&=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{tx-x^2/2}dx\\
&=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x^2-2tx)/2}dx\\
&=e^{t^2/2}\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x-t)^2/2}dx\\
&= e^{t^2/2}
\end{align*}
Si $Y \sim N(m,\sigma^2)$, alors
$$
\phi_Y(t)=\mathbb{E}\left[e^{t(\sigma X+m)} \right]=\exp\left[\dfrac{\sigma^2t^2}{2}+m \right]
$$
Sous R, on peut déterminer et évaluer les fonctions génératrices des moments en utilisant l'extension __`MGF`__ qui est téléchargeable depuis l'adresse suivante: <https://github.com/alexandernel14/MGF>.

Une description de l'installation et l'utilisation de cette extension est donnée comme suit:

```{r,MGF, comment="", warning=FALSE, message=FALSE}
# installer 'devtools': install.packages("devtools")
# installer 'MGF' depuis github
#devtools::install_github("alexandernel14/MGF",force = T)
# charger l'extension 'MGF'
library(MGF)
# fonction génératrice de la loi binomiale
mgf("Binomial")

# t=0, ordre 1
MGF_evaluator("Binomial",t=0, order_of_moment=1, n=10, p=0.4)
# t=0, ordre 2
MGF_evaluator("Binomial",t=0, order_of_moment=2, n=10, p=0.4)

# fonction génératrice de la loi normale
mgf("Normal")

# t=0, ordre 1
MGF_evaluator("Normal",t=0, order_of_moment=1)
# t=0, ordre 2
MGF_evaluator("Normal",t=0, order_of_moment=2)

```

## Fonctions caractéristiques
Il existe des variables aléatoires pour lesquelles la fonction génératrice des moments n'existe pas. Dans ce cas, on peut faire recours à la fonction caractéristique définie ci-dessous.
```{definition}
La __fonction caractéristique__ d'une variable aléatoire $X$ est la fonction à valeurs complexes définie sur $\mathbb {R}$ par

\begin{align*}\psi_{X}(t)&=\mathbb{E} \left[{e} ^{\mathrm{i} tX}\right]\\
&=\begin{cases}
\displaystyle \sum_{i=1}^n {e} ^{\mathrm{i} tX }P(X=x_i) \text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R} }e^{\mathrm{i} tx} f(x)\,\mathrm{d} x \text{ si } X \text{ est continue}
\end{cases}
\end{align*}
où $\mathrm{i}=\sqrt{-1}$. 

```

```{proposition}
Soit $X$ une variable aléatoire, $a$ et $b$ deux réels.
Alors les propriétés suivantes sont toujours vraies :

__1.__ Pour tout $t \in \mathbb{R}$, $\left|\psi_X(t)\right| \leq 1$.

__2.__ $\psi_X(0)=1$.

__3.__ Pour tout $t \in \mathbb{R}$, $\psi_X(−t) = \overline{\psi_X(t)}$.

__4.__ Pour tout $t \in \mathbb{R}$, $\psi_{aX+b}(t) = e^{\mathrm{i}tb} \psi_X(at)$.

__5.__ $\psi_X(t)$ est continue sur $\mathbb{R}$.

```

```{proposition}
Si le moment d'ordre $n$ d'une variable aléatoire $X$ existe, alors la fonction caractéristique de
$X$ est $n$ fois dérivable et :
  $$
  \mathbb{E}\left({X}^n \right)=\dfrac{1}{{\mathrm{i}}^n}{\psi}^n(0)
  $$
```

```{example, name="Loi normal standard"}
La fonction caractéristique de la loi normale standard est: $$\psi(t)={e}^{-t^2/2}$$
```

```{example, name="Loi exponentielle"}
La fonction caractéristique de la loi exponentielle de paramètre $\lambda$ est:
  $$
  \psi(t)=\dfrac{\lambda}{\mathrm{i}t-\lambda}
  $$
```

Si \(X_1,X_2,\ldots , X_n\) sont des variables indépendantes, alors 
\[
\psi_{\sum_iX_i}(t)=\prod_i\psi_{X_i}(t)
\]

## Espérance conditionnelle - Variance conditionnelle

L'un des concepts les plus utiles de la théorie des probabilités est celui de la probabilité conditionnelle et de l'espérance conditionnelle. La raison est double. Premièrement, dans la pratique, nous nous intéressons souvent au calcul des probabilités et des espérances lorsqu'une information partielle est
disponible; par conséquent, les probabilités et les espérances souhaitées sont conditionnelles. Deuxièmement, pour calculer une probabilité , il est souvent extrêmement utile d'abord *conditionner* une variable aléatoire appropriée.

### Densité conditionnelle

Soient $X$ et $Y$ deux variables aléatoires de densité jointe $f(x,y)$.

```{definition}
On définit la densité __conditionnelle__ de $X$ sachant $Y=y$ par:
$$
  f_{X|Y}(x,y)=\begin{cases}
\dfrac{P(X=x,Y=y)}{P_Y(Y=y)} \text{ si } X \text{ et } Y \text{ sont discrètes}\\
\dfrac{f(x,y)}{f_Y(y)}\text{ si } X \text{ et } Y \text{ sont continues}
\end{cases}
$$
  
```

```{example}
Soit un couple aléatoire discrète $(X,Y)$ définie par la densité jointe suivante:
  $$
  P(X=1,Y=1)=0.5;\,P(X=1,Y=2)=0.1;\,P(X=2,Y=1)=0.1;\,P(X=2,Y=2)=0.3
  $$
Déterminer la loi conditionnelle $f_{X|Y=1}(x,y)$.
```

La densité conditionnelle est par définition
$$
{f}_{X|Y=1}(x,y)=\dfrac{P(X=x,Y=1)}{{P}_Y(Y=1)}
$$
Or, $P_y(Y=1)=\displaystyle \sum_x P(X=x,Y=1)=p(1,1)+p(2,1)=0.6$.
D'où
$$
{f}_{X|Y=1}(1,1)=\dfrac{P(X=1,Y=1)}{{P}_Y(Y=1)}=\dfrac{p(1,1)}{{P}_Y(1)}=\dfrac{5}{6}
$$
$$
{f}_{X|Y=1}(2,1)=\dfrac{P(X=2,Y=1)}{{P}_Y(Y=1)}=\dfrac{p(2,1)}{{P}_Y(1)}=\dfrac{1}{6}
$$

### Espérance conditionnelle

Une espérance conditionnelle est une expression calculée depuis une distribution conditionnelle. On écrit \(\mathbb{E}\left(Y|X=x \right) \) pour l'espérance de \(Y\) sachant \(X=x\).

```{definition}
L'espérance conditionnelle de \(Y\) sachant \(X=x\) est
\[
\mathbb{E}\left(Y|X=x \right)=\left\{
\begin{array}{ll}
\displaystyle \sum_y y\mathbb{P}\left(Y=y|X=x \right) & \text{ cas discret}\\
\displaystyle \int_{\mathbb{R}}yf_{Y|X}(x,y)dy & \text{ cas continu}
\end{array}
\right.
\]
```

```{example,exple11}
On donne \(Y=\begin{cases}
           1 \text{ avec une probabilité }\frac{1}{8}\\
           2 \text{ avec une probabilité }\frac{7}{8}
  \end{cases}
           \)

et \(X|Y=\begin{cases}
           2Y \text{ avec une probabilité }\frac{3}{4}\\
           3Y \text{ avec une probabilité }\frac{1}{4}
  \end{cases}
           \)

Déterminer \(\mathbb{E}\left( X|Y=y\right)\).
```

Si \(Y= \), alors \(\left(X|Y= \right)= 
\begin{cases}
           2 \text{ avec une probabilité }\frac{3}{4}\\
           3 \text{ avec une probabilité }\frac{1}{4}
  \end{cases}
\)

D'où \( \mathbb{E}\left( X|Y=1\right)=\displaystyle \sum_x x\mathbb{P}\left( X|Y=1\right)=2\times\frac{3}{4}+3\times \frac{1}{4}=\frac{9}{4} \).

Si \(Y=2 \), alors \(\left(X|Y=2 \right)= 
\begin{cases}
           4 \text{ avec une probabilité }\frac{3}{4}\\
           6 \text{ avec une probabilité }\frac{1}{4}
  \end{cases}
\)

D'où \( \mathbb{E}\left( X|Y=2\right)=\displaystyle \sum_x x\mathbb{P}\left( X|Y=2\right)=4\times\frac{3}{4}+6\times \frac{1}{4}=\frac{18}{4} \).

Donc \(\mathbb{E}\left( X|Y=y\right)= \begin{cases}
           \frac{9}{4} \text{ si }Y=1 \text{ avec une prob} =\frac{1}{8}\\
           \frac{18}{4} \text{ si } Y=2 \text{ avec une prob }=\frac{7}{8}
  \end{cases}\).

```{example}
Soit la densité jointe d'un couple aléatoire définie par: \[f(x,y)=\frac{2}{xy},\; \text{ pour }\;1<y<x<e \]
Déterminer \(\mathbb{E}\left( Y|X=x\right)\)
```

Par définition \(\mathbb{E}\left( Y|X=x\right)=\displaystyle \int_{\mathbb{R}}y{f}_{X|Y}(x,y)dy \).

Or \( {f}_{X|Y}(x,y)=\dfrac{f(x,y)}{{f}_X(x)}\) avec
\begin{align*}
{f}_X(x)=&\int_1^x\dfrac{2}{xy}dy=\dfrac{2}{x}\biggl[\ln(y) \biggr]_1^x\\
=&\dfrac{2\ln(x)}{x},\;\text{ pour }\; 1<x<e.
\end{align*}
D'où \({f}_{X|Y}(x,y)=\dfrac{2}{xy}\dfrac{x}{\ln(x)}=\dfrac{1}{y\ln(x)} \) pour \(1<y<x \), et par conséquent,
\[ 
\mathbb{E}\left( Y|X=x\right)=\int_1^x \dfrac{y}{y\ln(x)}dy=\dfrac{1}{\ln(x)}\biggl[ y\biggr]_1^x=\dfrac{x-1}{\ln(x)}
\]

#### Propriétés de l'espérance conditionnelle {-}

   __1. Linéarité:__ Pour toutes constantes \(a,b \) et \(X,Y \) et \(Z \) des variables aléatoires,
   \[\mathbb{E}\left(aY+bZ|X=x\right)=a \mathbb{E}\left(Y|X=x\right)+b\mathbb{E}\left(Z|X=x\right)\]
   __2. Indépendance:__ Si \(X\) et \(Y\) sont deux variables aléatoires indépendantes, alors \( \mathbb{E}\left(Y|X=x\right)=\mathbb{E}\left(Y\right)\)
   
   __3.__ \(\mathbb{E}\left(g(X)|X=x\right)=\mathbb{E}\left(X\right) \) où $g$ est une transformation.
   
   __4. Espérance totale:__ \(\mathbb{E}\left(Y\right)=\mathbb{E}\bigl(\mathbb{E}\left(Y|X=x\right)\bigr) \): La moyenne totale est la moyenne des moyennes. 

```{example}
Reprenons l'exemple \@ref(exm:exple11). 

\(\mathbb{E}\left( X|Y=y\right)= \begin{cases}
           \frac{9}{4} \text{  avec une prob } =\frac{1}{8}\\
           \frac{18}{4} \text{ avec une prob  }=\frac{7}{8}
  \end{cases}\).

Donc, l'espérance totale est:

  \(\mathbb{E}\left(X\right)= \frac{9}{4} \times \frac{1}{8} + \frac{18}{4} \times \frac{7}{8} = \frac{135}{32} \)

```

```{exercise}
On jette une pièce de monnaie équilibrée. Soit \(Y\) la variable aléatoire désignant le nombre des lancers avant d'obtenir Face pour la première fois.

   1. Déterminer la loi de \(Y\) ainsi que son espérance et sa variance.
   2. Simuler 10 000 réalisations de \(Y\).
   3. Soit \(\left(X|Y=y\right)\sim \mathcal{P}(\lambda Y)\).
      a) Calculer \(\mathbb{E}\left(X|Y=y\right) \) et \(\mathbb{E}\left(X\right) \).
      b) En prenant \(\lambda=2\) et à l'aide des simulations, donner la valeur de
\(\mathbb{E}\left(X|Y=y\right) \).

```

```{solution}
1. Soit \( P(F)=p\) et \(P(P)=1-p\). On a \((n-1)\) premières épreuves donnant $P$ et la nième épreuve donne $F$, et puisque les épreuves sont indépendantes, alors
\[
\mathbb{P}\bigl(Y=n \bigr)={(1-p)}^{n}p\;,\;\; n=0,1,2,\ldots  
  \]
Donc \( Y\sim G(p)\).

2. Soit \(Z=(X|Y=y)\sim \mathcal{P}(\lambda Y) \), \(\mathbb{E}\left(Z\right)=\mathbb{E}\left(X|Y=y\right)= \lambda Y\).

\begin{align*}
\mathbb{E}\left(X\right)&=\mathbb{E}\bigl(\mathbb{E}\left(X|Y=y\right)\bigr)=\mathbb{E}(\lambda Y)\\
&= \lambda \mathbb{E}(Y)=\lambda \dfrac{1-p}{p}
\end{align*}
```

```{r, geom}
set.seed(1)
y=rgeom(10000, prob=0.5)
```

```{r, comment=""}
set.seed(1)
x=rpois(10000,lambda = 2*y)
mean(x)
```
La figure suivante donne l'évolution de \(\mathbb{E}\left(X|Y=y\right) \) en fonction du nombre des simulations.
```{r,echo=F, fig.align='center', fig.width=5, fig.height=4, warning=FALSE, message=FALSE, comment=NA}
library(ggfortify)
mx=cumsum(x[-(1:10)])/(11:10000)
autoplot(as.ts(mx), size=0.6)+geom_hline(yintercept = 2, colour="red",size=0.8 )+xlab("Nombre des simulations")+labs(y=paste(expression(E), "(",expression(X),"|", expression(Y),")"))
```

### Variance conditionnelle

Similairement à l'espérance conditionnelle, la variance conditionnelle est une variance prise par rapport à une distribution conditionnelle. Étant donné les variables aléatoires \(X\) et \(Y\), soit \(m_X=\mathbb{E}\left(Y|X=x \right)\). La variance conditionnelle \(\mathbb{V}(Y | X = x) \)est définie comme suit:


```{definition}
La variance conditionnelle de \(Y\) sachant \(X=x\) est
\[
\mathbb{V}\left(Y|X=x \right)=\left\{
\begin{array}{ll}
\displaystyle \sum_y (y-m_X)\mathbb{P}\left(Y=y|X=x \right) & \text{ cas discret}\\
\displaystyle \int_{\mathbb{R}}(y-m_X)^2f_{Y|X}(x,y)dy & \text{ cas continu}
\end{array}
\right.
\]
```

La variance conditinnelle possède les propriétés suivantes:

#### Propriétes de la variance conditionnelle {-}

__1.__ \(\mathbb{V}\left(Y|X=x \right)=\mathbb{E}\left(Y^2|X=x \right)-\mathbb{E}\left(Y|X=x \right)^2 \).

__2.__ \(\mathbb{V}(aY + b|X = x) = a^2\mathbb{V}(Y|X = x)\).

__3. Variance totale:__ \(\mathbb{V}(Y) = \mathbb{E}\bigl(\mathbb{V}(Y|X)\bigr) + \mathbb{V}\bigl(\mathbb{E}(Y|X)\bigr)\).

```{example}
Soit \(X\) une variable aléatoire uniforme sur \(]0,1[\). Si \(X=x\), alors \(Y\) suit une loi uniforme sur \(]0,x[\). Déterminer la variance de $Y$.
```
La distribution conditionnelle de \(Y|X=x\) est uniforme sur \(]0,x[\). On déduit alors,
\[\mathbb{E}(Y|X = x) = \dfrac{x}{2} \; \text{ et }\; \mathbb{V}(Y|X = x) = \dfrac{x^2}{12}\]
La propriété de la variance totale donne, 
\begin{align*}\mathbb{V}(Y) &= \mathbb{E}(\mathbb{V}(Y|X)) + \mathbb{V}(\mathbb{E}(Y|X)) = \mathbb{E}
\left(\frac{X^2}{12}\right)
+ \mathbb{V}\left(\frac{X}{2}
\right)\qquad \qquad \\
&=\frac{1}{12}\mathbb{E}
\left(X^2\right)+\frac{1}{4}\mathbb{V}\left(X\right)=\frac{1}{12}\times \frac{1}{3}+\frac{1}{4}\times \frac{1}{12}=\frac{7}{144}=0.04861.
\end{align*}

Sous R, on peut procéder comme suit:
```{r, varCond, comment=""}
set.seed(1)
x=runif(10000,0,1)  # réalisations uniformes sur ]0,1[
y=runif(10000,0,x)  # réalisations uniformes sur ]0,x[
var(y)  
```
L'évolution de la variance conditionnelle suivant le nombre des simulations est donnée par le graphique suivant:
```{r,varCfig,echo=FALSE,fig.align='center', fig.width=5, fig.height=4, warning=FALSE, message=FALSE}
vCond=NULL
for(i in 11:10000){
  vCond[i]=var(y[1:i])
}
autoplot(as.ts(vCond[-(1:10)]), size=0.6)+geom_hline(yintercept = 0.048611, colour="red",size=0.8 )+xlab("Nombre des simulations")+labs(y=paste(expression(V), "(",expression(X),"|", expression(Y),")"))
```

### Théorèmes limites

```{theorem, name="Loi forte des grands nombres"}
Si \(X_1,X_2,\ldots, X_n \) sont des variables aléatoires \(iid\) de moyenne \(m\), alors
\[
\mathbb{P}\left[\lim_{n \rightarrow \infty}\overline{X}_n=m \right]=1
\]
```
La moyenne des $n$ premiers termes d'une suite de variables aléatoires $iid$ converge presque sûrement vers l'espérance mathématique $\mathbb{E}(X_i)=m$, lorsque $n$ tend vers l'infini.

Vérifions ce théorème à l'aide des simulations. On considère l'expérience suivante: On lance une pièce de monnaie équilibrée $n$ fois. Soit $X_i=1$ si Face et $X_i=0$ si Pile. D'après la loi des grands nombres, \(\overline{X}_n \) converge vers \(\mathbb{E}(X_i)=p=0.5 \).

```{r, grandNombres, fig.align='center', fig.height=5,fig.width=6}
set.seed(1)
s=sample(c(0,1),10000, replace = T)
x_bar=cumsum(s)/(1:10000)
plot(x_bar, type="l",col="blue", xlab="Nombre de simulations", 
     ylab="")
abline(h=0.5, col="red", lwd=2)
legend("bottomright", legend=c(expression(bar(X)),expression(p==0.5)),
       lty=1,col=c("blue","red"),bty="n")
```


```{theorem, name="Loi faible des grands nombres"}
Si \(X_1,X_2,\ldots, X_n \) sont des variables aléatoires \(iid\) de moyenne \(m\), alors
\[
\lim_{n \rightarrow \infty}\mathbb{P}\left[|\overline{X}_n-m| < \epsilon \right]=1, \;\; \forall \, \epsilon > 0
\]
```

```{theorem, name="Théorème central limite"}
Si \(X_1,X_2,\ldots, X_n \) sont des variables aléatoires \(iid(m,\sigma^2)\), alors
\[
\lim_{n \rightarrow \infty}\mathbb{P}\left[\frac{\overline{X}_n-m}{\sigma/\sqrt{n}} < t \right]=\mathbb{P}(Z <), \;\; \text{ avec } \; Z \sim N(0,1)
\]
```

__Illustration sous R__

Soit \(X_1, X_2,\ldots, X_{100} \) une suite de variables aléatoires telle que \(X_i \stackrel{iid}{\sim}\mathcal{P(\lambda=4)} \). On sait que \(\mathbb{E}(X_i)=\mathbb{V}(X_i)=\lambda=4 \).

1. Simuler \(100\) réalisations du Poisson de paramètre \(\lambda=4 \) puis déduire la variable de $z$
\[z=\dfrac{\overline{X}-m}{\sigma/\sqrt{n}} \]

2. Répéter \(100000 \) fois les instructions précédentes et stocker le résultat dans un objet que l'on appelle `mu`.

3. Représenter l'histogramme de `mu`. Ajouter sur le même graphique la courbe de la densité de la loi normale standard.

```{r, tcl, fig.align='center', fig.height=5, fig.width=6}
# 1
set.seed(1)
simPois=rpois(n=100, lambda=4)
z=(mean(simPois)-4)/(2/sqrt(100))
# 2
Z=function(){
 simPois=rpois(n=100, lambda=4)
(mean(simPois)-4)/(2/sqrt(100)) 
}
set.seed(1)
mu=replicate(100000, Z())
# 3
hist(mu,freq = F)
curve(dnorm,-4,4,add=T,col=4,lwd=2)
```


## Processus stochastique

Un processus stochastique est simplement une collection de variables aléatoires \(\{X_t, t \in I\}\). L'indice \(t\) représente souvent le temps et l'ensemble \(I\) est l'ensemble d'indices du processus appelé aussi espace de temps. Les ensembles d'indices les plus courants sont \(I = \{0, 1, 2,\ldots\}\), représentant le temps __discret__, et \(I = [0, +\infty[\), représentant le temps __continu__. Les processus stochastiques en temps discret sont des séquences de variables aléatoires. Les processus en temps continu sont des collections non dénombrables de variables aléatoires.

Les variables aléatoires d'un processus stochastique prennent des valeurs dans un espace d'états commun \(E\), discret ou continu. Un processus stochastique est spécifié par ses espaces de temps et d'état, et par les relations de dépendance entre ses variables aléatoires.

## Exercices {-}

### Exercice 1 {-}

Pour chacune des variables aléatoires suivantes, déterminer la fonction génératrices des moments et déduire  son espérance et sa variance.

1. \( X\) est de densité de probabilité définie par: \( \mathbb{P}(X=1)=\frac{1}{3}\) et \( \mathbb{P}(X=2)=\frac{2}{3}\).

2. \( Y \sim \mathcal{U}[0,1]\).

### Exercice 2 {-}

Déterminer \(\mathbb{E}\left(X|Y=y \right)\) lorsque la densité jointe du couple \((X,Y)\) est:

   i) \(\displaystyle f(x,y)=\frac{y^2-x^2}{8}e^{-x},\;\; 0 < y < \infty\, , \;\; -y<x<y. \)
   ii) \(\displaystyle f(x,y)=\frac{e^{-x/y} e^{-y}}{y},\;\; 0 < x < \infty\, , \;\; 0<y<\infty. \)

### Exercice 3 {-}

Les habitants de Sousse retirent de l'argent d'un distributeur de billets selon la fonction de probabilité suivante:

|                      |        |       |       |
|----------------------|:------:|:-----:|:-----:|
|  $x_i$ en DT         |  100   |   200 |  500  |
|  $\mathbb{P}(X=x_i)$ | 0.25   | 0.55  | 0.2   |

Le nombre des clients par jour, \(N\), suit une distribution de poisson de paramètre $\lambda = 0.5$, i.e. \(N \sim \mathcal{P}(0.5) \).

Soit \(S_N=X_1+X_2+\ldots +X_N \) le montant d'argent total retiré par jour, où les \(X_i \) sont indépentantes entre eux et avec la variable \(N \).

1. Calculer \(\mathbb{E}(X) \) et \(\mathbb{V}(X) \).

2. Déterminer \(\mathbb{E}(S_N) \) et \(\mathbb{V}(S_N) \).

3. Reprendre les questions précédentes en utilisants des simulations sous R.

### Exercice 4 {-}

Soit \(X_1, X_2,\ldots, X_{100} \) une suite de variables aléatoires telle que \(X_i \stackrel{iid}{\sim}\mathcal{E(\lambda=4)} \). On sait que \(\mathbb{E}(X_i)=\frac{1}{\lambda}\) et \(\mathbb{V}(X_i)=\frac{1}{\lambda^2} \).

1. Simuler \(100\) réalisations de la loi exponentielle de paramètre \(\lambda=4 \) (`rexp()`) puis déduire la variable de $z$
\[z=\dfrac{\overline{X}-m}{\sigma/\sqrt{n}} \]

2. Répéter \(100000 \) fois les instructions précédentes et stocker le résultat dans un objet que l'on appelle `mu`.

3. Représenter l'histogramme de `mu`. Ajouter sur le même graphique la courbe de la densité de la loi normale standard.

### Exercice 5 {-}

On lance un dé équilibré, puis une pièce de monnaie équilibrée un nombre de fois égal au résultat
du dé. Soit $X$ le résultat du dé et $Y$ le nombre de _Pile_ amenés par la pièce de monnaie.

1. Déterminer la loi jointe du couple $(X, Y )$.

2. Soit $n \in \{1, \ldots, 6\}$. Quelle est la loi de $Y$ sachant $X = n$ ?

3. En déduire $\mathbb{E}\bigl[Y |X = n\bigr]$, puis $\mathbb{E}\bigl[Y |X\bigr]$.

4. Calculer $E[Y]$.

5. Reprendre les questions 2 à 4 à l'aide des simulations sous R.

### Exercice 6 {-}

Le temps que Sarra passe à parler au téléphone suit une distribution exponentielle de moyenne 7 minutes. Quelle est la durée moyenne de son appel téléphonique si elle parle pendant plus de 3 minutes?
<!--
Soit $X\sim \mathcal{E}(1/7)$.
$\mathbb{P}\bigl(X|X > 3 \bigr)=\displaystyle \frac{1}{\mathbb{P}(X > 3)}\int_2 ^{+\infty}x \frac{1}{7}e^{-\frac{1}{7} x}dx$
!-->

