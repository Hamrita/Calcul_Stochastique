--- 
title: "Calcul stochastique"
author: "__Mohamed Essaied Hamrita__"
date: "__2021__"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: plain
link-citations: yes
nocite: '@*'
description: ""
---
```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'markovchain'), 'packages.bib')

```

```{r include=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    #sprintf("\\textcolor{%s}{%s}", color, x)
    paste("\\textcolor{",color,"}{\\textbf{",x,"}}",sep="")
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```

# Rappel sur le calcul des probabilités

## Notion de probabilité

Une notion basique dans la théorie des probabilités est l'__expérience aléatoire__ dont on ne connait pas son résultat en avance. L'ensemble de tous les résultats de l'expérience est l'__ensemble des possibles__ ou encore l'__univers__ et noté $\Omega$.

Un __évènement__ est un sous ensemble de l'univers. 

On donne quelques exemples:

  __1.__ Si l'expérience consiste à jeter une pièce de monnaie, alors $\Omega=\{P,F\}$.
  
  __2.__ Si l'expérience consiste à jeter un dé cubique dont ses faces sont numérotées de 1 à 6, alors \(\Omega=\{1,2,3,4,5,6 \} \).
   
Pour chaque évènement $E$ de l'univers $\Omega$, on définit un nombre $P(E)$ qui satisfait les axiomes suivants:

  * __Axiome 1:__ $0 \leq P(E) \leq 1$;
  * __Axiome 2:__ $P(\Omega)=1$;
  * __Axiome 3:__ Pour toute séquence des évènements \( E_1 , E_2, \ldots , E_n \) qui sont mutuellement exclusifs (\( E_i \cap E_j=\emptyset , \; \forall \; i \neq j \) et $\displaystyle{\bigcup\limits_{i=1}^nE_i=\Omega}$), on a 
$$
P \left(\bigcup\limits_{i=1}^nE_i \right)=\sum_{i=1}^n P(E_i)
$$

Quelques conséquences de ces axiomes sont tirées:

   * Si \( E \subset F \), alors \( P(E) \leq P(F) \).
   *  \( P ( \bar{E})=1-P(E) \) où \( \bar{E} \) est le complémentaire de \( E \).
   * \( P \left(\bigcup\limits_{i=1}^nE_i \right)=\displaystyle\sum_{i=1}^n P(E_i) \), lorsque \( E_i\) sont mutuellement exclusifs.
   * \( P \left(\bigcup\limits_{i=1}^nE_i \right)\leq\displaystyle \sum_{i=1}^n P(E_i) \) (Inégalité de Boole).
   
```{example}
L'expérience consiste à lancer une pièce de monnaie équilibrée. Donc \(P(\{P\})= P(\{F\})=0.5\)
```

```{example}
On lance un dé cubique équilibré dont ses faces sont numérotées de 1 à 6.

\(P(\{i \})= \dfrac{1}{6}, \; \forall\, i=1,2,\ldots,6 \).

La probabilité d'obtenir un nombre pair est

\(P(\{2,4,6\})= P(\{2 \})+P(\{4 \}) +P(\{6 \}) = \dfrac{1}{2}\).
```

### Probabilité conditionnelle

```{definition}
Soient $A$ et $B$ deux évènements tels que $P(B)\neq0$. La probabilité de __A sachant B__ est le nombre
$$
  P(A|B)=\frac{P(A \cap B)}{P(B)}
$$
```

```{example}
Une urne contient 10 boules numérotées de 1 à 10 indiscernables au toucher. On tire au hasard une boule. Sachant que le numéro de la boule tirée est au moins égale à 5, quelle est la probabilité qu'il soit égale à 10?

Soit $A$ l'évènement d'avoir une boule portant le numéro 10. Soit $B$ l'évènement d'avoir une boule portant un numéro supèrieur ou égale à 5. La probabilité demandée est $P(A|B)$
$$
P(A|B)=\frac{P(A \cap B)}{P(B)}=\frac{P(A)}{P(B)}=\frac{1/10}{6/10}=1/6
$$
```

### Indépendance

```{definition}
On dit que deux évènements, A et B, sont __indépendants__ si $P(A\cap B)=P(A)\times P(B)$
```

Si $A$ et $B$ deux évènements indépendants, alors $P(A|B)=P(A)$ et $P(B|A)=P(B)$.

```{example}
On lance deux dés cubiques équilibrés numérotés de 1 à 6. Soit $A$ l'événement d'obtenir une somme égale six et $B$ désigne l'événement où le premier dé est égal à quatre. 

Vérifier que les deux évènements $A$ et $B$ sont indépendants.

$P(A\cap B)=p(\{4,2 \})=1/36$ et $P(A)=1/6$, $P(B)=1/6$. Donc $P(A)\times P(B)=1/6 \times 1/6 =1/36 =P(A\cap B)$. Ainsi, les évènements $A$ et $B$ sont indépendants.
```


## Variables aléatoires

```{definition}
Soit une expérience aléatoire d'univers $\Omega$.

Une variable aléatoire $X$ est une application de l'ensemble $\Omega$ vers un ensemble de réalisations.

Pour tout évènement $A$, on $P(X \in A)=P(X^{-1}(A))$ où $X^{-1}(A)$ est l'évènement comprenant tous les éléments $\omega \in \Omega$ tels que $X(\omega) \in A$.
```

La __fonction de répartition__ $F$ d'une variable aléatoire $X$ est définie par 
$$
F(x)=P(X \leq x)=P(X \in ]-\infty, x]), \; \forall \, x \in \mathbb{R}
$$
Une variable aléatoire $X$ est dite __discrète__ si son ensemble des valeurs possibles est dénombrables. Dans ce cas, on a
$$
F(x)=\sum_{k \leq x }P(X=k)
$$
Une variable aléatoire $X$ est dite __continue__ s'il existe une fonction $f(x)$, appelée *densité de probabilité*, telle que
$$
P(X\in B)=\int_Bf(x)dx \; \text{ pour tout ensemble  }B
$$
Puisque $F(x)=\displaystyle \int_{-\infty}^x f(x) dx$, alors
$$
f(x)=\frac{d}{dx}F(x)
$$
La fonction de répartition __jointe__ d'un couple aléatoire $X$ et $Y$ est $F(x,y)=P(X \leq x, Y \leq y)$.

Les fonctions de répartition de $X$ et $Y$,
$$
F_X(x)=P(X\leq x) \text{ et }F_Y(y)=P(Y\leq y)
$$
peuvent être déduites de $F(x,y)$. En effet,
$$
F_X(x)=\lim_{y \longrightarrow \infty}F(x,y) \; \text{ et }\;F_Y(y)=\lim_{x \longrightarrow \infty}F(x,y)
$$
Les variables aléatoires $X$ et $Y$ sont __indépendantes__ si
$$
F(x,y)=F_X(x) F_Y(y)
$$
$X$ et $Y$ sont __continues__ s'il existe une fonction $f(x,y)$, dite densité de probabilité *jointe*, telle que
$$
P(X\in A, Y\in B)=\int_A\int_B f(x,y)dxdy\;\; \forall A,B
$$
La fonction de répartition d'une suite de $n$ variables aléatoires $X_1, X_2, \ldots, X_n$ est définie par:
$$
F(X_1, X_2, \ldots, X_n)=P(X_1\leq x_1, X_2\leq x_2, \ldots, X_n \leq x_n)
$$
et sont indépendantes si
$$
F(X_1, X_2, \ldots, X_n)=F_{X_1}(x_1)F_{X_2}(x_2)\ldots F_{X_n}(x_n)
$$

## Espérance mathématique

```{definition}
L'__espérance mathématique__ ou __moyenne__ d'une variavle aléatoire $X$, notée $\mathbb{E}(X)$, est définie par:

   \begin{align*}
\mathbb{E}(X) & = \int_{\mathbb{R}}x dF(x)\\
 & = \begin{cases}
   \displaystyle \sum_x xP(X=x)\; \text{si }X \text{ est discrète}\\
 \displaystyle  \int_{\mathbb{R}} xf(x)dx \; \text{si }X \text{ est continue}
     \end{cases}
  \end{align*}

```

De même, on définit l'espérance d'une fonction de $X$, $g(X)$, par:
$$
\mathbb{E}\left[ g(X)\right]=\int_{\mathbb{R}}x dF_g(x)=\int_{\mathbb{R}}g(x) dF(x)
$$
L'espérance d'une somme de variables aléatoires est la somme des espérances:
$$
\mathbb{E}\left[\sum_{i=1}^n X_i \right]=\sum_{i=1}^n\mathbb{E}(X_i)
$$
La __variance__ d'une variable aléatoire, $X$, est définie par
$$
\mathbb{V}(X)=\mathbb{E}\left[{(X-\mathbb{E}(X))}^2 \right]=\mathbb{E}(X^2)-{\mathbb{E}(X)}^2
$$
Deux variables aléatoires, $X$ et $Y$, sont dites __non corréllées__ si leur covariance, définie par:
$$
\text{Cov}(X,Y)=\mathbb{E}\left[(X-\mathbb{E}(X))(Y-\mathbb{E}(Y))\right]=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
$$
est __nulle__. Noter que si $X$ et $Y$ sont indépendantes, alors elles sont non corréllées ($\text{Cov}(X,Y)=0$).

### Propriétés de la covariance{-}

Pour toutes variables aléatoires $X, Y, Z$ et $a \in \mathbb{R}$, on a:
  
__1.__ Cov$(X,X)=\mathbb{V}(X)$ et Cov$(X,Y)=$ Cov$(Y,X) $

__2.__ Cov$(aX,Y)=a$Cov$(X,Y)$.

__3.__ Cov$(X,Y+Z)=$ Cov$(X,Y)+$ Cov$(X,Z)$.

Une généralisation de la troisième propriétés est donnée par:
$$
\text{Cov}\left( \sum_{i=1}^nX_i,\sum_{j=1}^m Y_i \right)=\sum_{i=1}^n \sum_{j=1}^m \text{Cov}(X_i,Y_j)
$$
Une expression utile pour la variance de la somme des variables aléatoires peut être déduite comme suit:
\begin{align*}
\mathbb{V}\left(\sum_{i=1}^nX_i \right)& =\text{Cov}\left(\sum_{i=1}^nX_i,\sum_{i=1}^nX_i\right)\\
&=\sum_{i=1}^n \sum_{j=1}^n\text{Cov}(X_i,X_j)\\
&=\sum_{i=1}^n\text{Cov}(X_i,X_i)+\sum_{i=1}^n\sum_{i\neq j}\text{Cov}(X_i,X_j)\\
&=\sum_{i=1}^n\mathbb{V}(X_i)+2\sum_{i=1}^n\sum_{j <i} \text{Cov}(X_i,X_j)
\end{align*}

```{definition}
Si $X_1,X_2,\ldots, X_n$ sont *indépendantes et identiquement distribuées*, noté $X_i \sim iid$, d'espérance $m$ et de variance $\sigma^2$, alors:

   * $\overline{X}=\displaystyle \frac{1}{n}\sum_{i=1}^n X_i$ est appelée moyenne empirique.
   * $\mathbb{E}(\overline{X})=m$ et $\mathbb{V}(\overline{X})=\displaystyle \frac{\sigma^2}{n}$.
   * Cov$(\overline{X},X_i-\overline{X})=0,$ $i=1,2,\ldots,n$.


```

```{example}
Calculer la variance d'une variable aléatoire $X$ suivant une loi binomiale de paramètres $n$ et $p$.

Puisqu'une telle variable aléatoire représente le nombre de succès dans $n$ essais indépendants lorsque chaque essai a une probabilité commune p d'être un succès,
nous pouvons écrire
$$
X=X_1+X_2+\ldots+X_n
$$
où $X_i \stackrel{iid}{\sim}B(p)$ telle que
```

$$
X_i=\begin{cases}
1 \text{ si le ième issue est un succés}\\
0 \text{ sinon}
\end{cases}
$$
Par conséquent, on aura $\mathbb{V}(X)=\displaystyle \sum_{i=1}^n\mathbb{V}(X_i)$, Or
\begin{align*}
\mathbb{V}(X_i)&=\mathbb{E}(X_i^2)-{\mathbb{E}(X_i)}^2\\
&=\mathbb{E}(X_i)-{\mathbb{E}(X_i)}^2 \text{ car } X_i^2=X_i\\
&= p-p^2= p(1-p)
\end{align*}
Donc $\mathbb{V}(X)=np(1-p)$.

## Fonctions génératrices des moments

```{definition}
La fonction génératrice des moments $\phi(t)$ de la variable aléatoire $X$ est définie pour tout $t \in \mathbb{R}$ par
\begin{align*}
\phi(t)&=\mathbb{E}\left[e^{tX} \right]\\
&= \begin{cases}
\displaystyle \sum_x e^{tx}P(X=x)\text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R}}e^{tx} f(x)dx \text{ si } X \text{ est continue}
\end{cases}
\end{align*}

```

$\phi(t)$ est appelée fonction génératrice des moments car tous les moments de $X$ peuvent être obtenues par les dérivées successives de $\phi(t)$. Par exemple

\begin{align*}
\phi'(t)&=\dfrac{d}{dt}\mathbb{E}\left[e^{tX} \right] \\
&=\mathbb{E}\left[\dfrac{d}{dt}(e^{tX}) \right]=\mathbb{E}\left[Xe^{tX} \right]
\end{align*}

Par conséquent $\phi'(0)= \mathbb{E}(X)$.

D'une manière plus générale, $\phi^n(0)=\mathbb{E}\left( {X}^n\right),\; n \geq 1$.

Une propriété importante des fonctions génératrices des moments est que la fonction génératrice des moments de la somme des variables aléatoires indépendantes est simplement le produit des fonctions génératrices des moments individuelles. Pour voir cela, supposons que $X$ et $Y$ sont indépendantes et ont respectivement des fonctions génératrices des moments $\phi_X(t)$ et $\phi_Y(t)$. Alors la fonction génératrice des moments de $X+Y$ est donnée par:
\begin{align*}
\phi_{X+Y}(t)&=\mathbb{E}\left(e^{t(X+Y)} \right)=\mathbb{E}\left(e^{tX}e^{tY)} \right)\\
&=\mathbb{E}\left(e^{tX}\right) \mathbb{E}\left(e^{tY}\right)=\phi_X(t)\phi_Y(t)
\end{align*}

```{example, name="Loi Binomiale de paramètres n et p"}
\begin{align*}
\phi(t)&=\mathbb{E}\left[e^{tX} \right]=\sum_{k=0}^ne^{tk}C^n_kp^k{(1-p)}^{n-k}\\
&=\sum_{k=0}^nC^n_k{\left(pe^t\right)}^k{(1-p)}^{n-k}
\end{align*}

```
Or, d'après la formule de Binôme, on a
$$
{(a+b)}^n=\sum_{k=0}^nC_n^ka^kb^{n-k}
$$
d'où \(\phi(t)={\left( pe^t+(1-p)\right)}^n\) et par conséquent
$$
\phi'(t)=n{\left(pe^t+1-p \right)}^{n-1}pe^t
$$
D'où $\mathbb{E}(X)=\phi'(0)=np$.

Dérivons une deuxième fois la fonction $\phi(t)$, on obtient

$$
\phi''(t)=n(n-1){\left(pe^t+1-p \right)}^{n-2}{\left(pe^t\right)}^2+n{\left(pe^t+1-p \right)}^{n-1}pe^t
$$
En déduit, alors
$$
\mathbb{E}(X^2)=\phi''(0)=n(n-1)p^2+np
$$
Donc, on peut déduire la variance de $X$.
$$
\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}{(X)}^2=\phi''(0)-{\left( \phi'(0)\right)}^2=np(1-p)
$$
```{example, name="Loi Normale standard"}
```
\begin{align*}
\mathbb{E}\left(e^{tX} \right)&=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{tx-x^2/2}dx\\
&=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x^2-2tx)/2}dx\\
&=e^{t^2/2}\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x-t)^2/2}dx\\
&= e^{t^2/2}
\end{align*}
Si $Y \sim N(m,\sigma^2)$, alors
$$
\phi_Y(t)=\mathbb{E}\left[e^{t(\sigma X+m)} \right]=\exp\left[\dfrac{\sigma^2t^2}{2}+m \right]
$$
Sous R, on peut déterminer et évaluer les fonctions génératrices des moments en utilisant l'extension __`MGF`__ qui est téléchargeable depuis l'adresse suivante: <https://github.com/alexandernel14/MGF>.

Une description de l'installation et l'utilisation de cette extension est donnée comme suit:

```{r,MGF, comment="", warning=FALSE, message=FALSE}
# installer 'devtools': install.packages("devtools")
# installer 'MGF' depuis github
#devtools::install_github("alexandernel14/MGF",force = T)
# charger l'extension 'MGF'
library(MGF)
# fonction génératrice de la loi binomiale
mgf("Binomial")

# t=0, ordre 1
MGF_evaluator("Binomial",t=0, order_of_moment=1, n=10, p=0.4)
# t=0, ordre 2
MGF_evaluator("Binomial",t=0, order_of_moment=2, n=10, p=0.4)

# fonction génératrice de la loi normale
mgf("Normal")

# t=0, ordre 1
MGF_evaluator("Normal",t=0, order_of_moment=1)
# t=0, ordre 2
MGF_evaluator("Normal",t=0, order_of_moment=2)

```

## Fonctions caractéristiques

```{definition}
La __fonction caractéristique__ d'une variable aléatoire $X$ est la fonction à valeurs complexes définie sur $\mathbb {R}$ par

\begin{align*}\psi_{X}(t)&=\mathbb{E} \left[{e} ^{\mathrm{i} tX}\right]\\
&=\begin{cases}
\displaystyle \sum_{i=1}^n {e} ^{\mathrm{i} tX }P(X=x_i) \text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R} }e^{\mathrm{i} tx} f(x)\,\mathrm{d} x \text{ si } X \text{ est continue}
\end{cases}
\end{align*}
où $\mathrm{i}=\sqrt{-1}$. 

```

```{proposition}
Soit $X$ une variable aléatoire, $a$ et $b$ deux réels.
Alors les propriétés suivantes sont toujours vraies :

__1.__ Pour tout $t \in \mathbb{R}$, $\left|\psi_X(t)\right| \leq 1$.

__2.__ $\psi_X(0)=1$.

__3.__ Pour tout $t \in \mathbb{R}$, $\psi_X(−t) = \overline{\psi_X(t)}$.

__4.__ Pour tout $t \in \mathbb{R}$, $\psi_{aX+b}(t) = e^{\mathrm{i}tb} \psi_X(at)$.

__5.__ $\psi_X(t)$ est continue sur $\mathbb{R}$.

```

```{proposition}
Si le moment d'ordre $n$ d'une variable aléatoire $X$ existe, alors la fonction caractéristique de
$X$ est $n$ fois dérivable et :
  $$
  \mathbb{E}\left({X}^n \right)=\dfrac{1}{{\mathrm{i}}^n}{\psi}^n(0)
  $$
```

```{example, name="Loi normal standard"}
La fonction caractéristique de la loi normale standard est: $\psi(t)={e}^{-t^2/2}$.
```

```{example, name="Loi exponentielle"}
La fonction caractéristique de la loi exponentielle de paramètre $\lambda$ est:
  $$
  \psi(t)=\dfrac{\lambda}{\lambda-\mathrm{i}t}
  $$
```

## Espérance conditionnelle
