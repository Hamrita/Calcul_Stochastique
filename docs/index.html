<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Calcul stochastique</title>
  <meta name="description" content="Calcul stochastique" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Calcul stochastique" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Calcul stochastique" />
  
  
  

<meta name="author" content="Mohamed Essaied Hamrita" />


<meta name="date" content="2021-01-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="chaînes-de-markov.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Calcul stochastique</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Rappel sur le calcul des probabilités</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#notion-de-probabilité"><i class="fa fa-check"></i><b>1.1</b> Notion de probabilité</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#probabilité-conditionnelle"><i class="fa fa-check"></i><b>1.1.1</b> Probabilité conditionnelle</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#indépendance"><i class="fa fa-check"></i><b>1.1.2</b> Indépendance</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#variables-aléatoires"><i class="fa fa-check"></i><b>1.2</b> Variables aléatoires</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#espérance-mathématique"><i class="fa fa-check"></i><b>1.3</b> Espérance mathématique</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#propriétés-de-la-covariance"><i class="fa fa-check"></i>Propriétés de la covariance</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#fonctions-génératrices-des-moments"><i class="fa fa-check"></i><b>1.4</b> Fonctions génératrices des moments</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#fonctions-caractéristiques"><i class="fa fa-check"></i><b>1.5</b> Fonctions caractéristiques</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#espérance-conditionnelle"><i class="fa fa-check"></i><b>1.6</b> Espérance conditionnelle</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html"><i class="fa fa-check"></i><b>2</b> Chaînes de Markov</a><ul>
<li class="chapter" data-level="2.1" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#définitions-et-exemples"><i class="fa fa-check"></i><b>2.1</b> Définitions et exemples</a></li>
<li class="chapter" data-level="2.2" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#loi-des-x_n"><i class="fa fa-check"></i><b>2.2</b> Loi des <span class="math inline">\(X_n\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#classification-des-états"><i class="fa fa-check"></i><b>2.3</b> Classification des états</a></li>
<li class="chapter" data-level="2.4" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#temps-dabsorption"><i class="fa fa-check"></i><b>2.4</b> Temps d’absorption</a><ul>
<li class="chapter" data-level="2.4.1" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#temps-darrêt"><i class="fa fa-check"></i><b>2.4.1</b> Temps d’arrêt</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#simulation-dune-chaîne-de-markov-discrète"><i class="fa fa-check"></i><b>2.5</b> Simulation d’une chaîne de Markov discrète</a><ul>
<li class="chapter" data-level="2.5.1" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#exemple-illustratif"><i class="fa fa-check"></i><b>2.5.1</b> Exemple illustratif</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="processus-de-poisson.html"><a href="processus-de-poisson.html"><i class="fa fa-check"></i><b>3</b> Processus de poisson</a></li>
<li class="chapter" data-level="4" data-path="processus-gaussien.html"><a href="processus-gaussien.html"><i class="fa fa-check"></i><b>4</b> Processus Gaussien</a></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a><ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>5.1</b> Example one</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>5.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliographie.html"><a href="bibliographie.html"><i class="fa fa-check"></i>Bibliographie</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Calcul stochastique</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Calcul stochastique</h1>
<p class="author"><em><strong>Mohamed Essaied Hamrita</strong></em></p>
<p class="date"><em><strong>2021</strong></em></p>
</div>
<div id="rappel-sur-le-calcul-des-probabilités" class="section level1">
<h1><span class="header-section-number">Chapitre 1</span> Rappel sur le calcul des probabilités</h1>
<div id="notion-de-probabilité" class="section level2">
<h2><span class="header-section-number">1.1</span> Notion de probabilité</h2>
<p>Une notion basique dans la théorie des probabilités est l’<strong>expérience aléatoire</strong> dont on ne connait pas son résultat en avance. L’ensemble de tous les résultats de l’expérience est l’<strong>ensemble des possibles</strong> ou encore l’<strong>univers</strong> et noté <span class="math inline">\(\Omega\)</span>.</p>
<p>Un <strong>évènement</strong> est un sous ensemble de l’univers.</p>
<p>On donne quelques exemples:</p>
<p><strong>1.</strong> Si l’expérience consiste à jeter une pièce de monnaie, alors <span class="math inline">\(\Omega=\{P,F\}\)</span>.</p>
<p><strong>2.</strong> Si l’expérience consiste à jeter un dé cubique dont ses faces sont numérotées de 1 à 6, alors <span class="math inline">\(\Omega=\{1,2,3,4,5,6 \}\)</span>.</p>
<p>Pour chaque évènement <span class="math inline">\(E\)</span> de l’univers <span class="math inline">\(\Omega\)</span>, on définit un nombre <span class="math inline">\(P(E)\)</span> qui satisfait les axiomes suivants:</p>
<ul>
<li><strong>Axiome 1:</strong> <span class="math inline">\(0 \leq P(E) \leq 1\)</span>;</li>
<li><strong>Axiome 2:</strong> <span class="math inline">\(P(\Omega)=1\)</span>;</li>
<li><strong>Axiome 3:</strong> Pour toute séquence des évènements <span class="math inline">\(E_1 , E_2, \ldots , E_n\)</span> qui sont mutuellement exclusifs (<span class="math inline">\(E_i \cap E_j=\emptyset , \; \forall \; i \neq j\)</span> et <span class="math inline">\(\displaystyle{\bigcup\limits_{i=1}^nE_i=\Omega}\)</span>), on a
<span class="math display">\[
P \left(\bigcup\limits_{i=1}^nE_i \right)=\sum_{i=1}^n P(E_i)
\]</span></li>
</ul>
<p>Quelques conséquences de ces axiomes sont tirées:</p>
<ul>
<li>Si <span class="math inline">\(E \subset F\)</span>, alors <span class="math inline">\(P(E) \leq P(F)\)</span>.</li>
<li><span class="math inline">\(P ( \bar{E})=1-P(E)\)</span> où <span class="math inline">\(\bar{E}\)</span> est le complémentaire de <span class="math inline">\(E\)</span>.</li>
<li><span class="math inline">\(P \left(\bigcup\limits_{i=1}^nE_i \right)=\displaystyle\sum_{i=1}^n P(E_i)\)</span>, lorsque <span class="math inline">\(E_i\)</span> sont mutuellement exclusifs.</li>
<li><span class="math inline">\(P \left(\bigcup\limits_{i=1}^nE_i \right)\leq\displaystyle \sum_{i=1}^n P(E_i)\)</span> (Inégalité de Boole).</li>
</ul>

<div class="example">
<span id="exm:unnamed-chunk-3" class="example"><strong>Exemple 1.1  </strong></span>L’expérience consiste à lancer une pièce de monnaie équilibrée. Donc <span class="math inline">\(P(\{P\})= P(\{F\})=0.5\)</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-4" class="example"><strong>Exemple 1.2  </strong></span>On lance un dé cubique équilibré dont ses faces sont numérotées de 1 à 6.</p>
<p><span class="math inline">\(P(\{i \})= \dfrac{1}{6}, \; \forall\, i=1,2,\ldots,6\)</span>.</p>
<p>La probabilité d’obtenir un nombre pair est</p>
<span class="math inline">\(P(\{2,4,6\})= P(\{2 \})+P(\{4 \}) +P(\{6 \}) = \dfrac{1}{2}\)</span>.
</div>

<div id="probabilité-conditionnelle" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Probabilité conditionnelle</h3>

<div class="definition">
<span id="def:unnamed-chunk-5" class="definition"><strong>Définition 1.1  </strong></span>Soient <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> deux évènements tels que <span class="math inline">\(P(B)\neq0\)</span>. La probabilité de <strong>A sachant B</strong> est le nombre
<span class="math display">\[
  P(A|B)=\frac{P(A \cap B)}{P(B)}
\]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-6" class="example"><strong>Exemple 1.3  </strong></span>Une urne contient 10 boules numérotées de 1 à 10 indiscernables au toucher. On tire au hasard une boule. Sachant que le numéro de la boule tirée est au moins égale à 5, quelle est la probabilité qu’il soit égale à 10?</p>
Soit <span class="math inline">\(A\)</span> l’évènement d’avoir une boule portant le numéro 10. Soit <span class="math inline">\(B\)</span> l’évènement d’avoir une boule portant un numéro supèrieur ou égale à 5. La probabilité demandée est <span class="math inline">\(P(A|B)\)</span>
<span class="math display">\[
P(A|B)=\frac{P(A \cap B)}{P(B)}=\frac{P(A)}{P(B)}=\frac{1/10}{6/10}=1/6
\]</span>
</div>

</div>
<div id="indépendance" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Indépendance</h3>

<div class="definition">
<span id="def:unnamed-chunk-7" class="definition"><strong>Définition 1.2  </strong></span>On dit que deux évènements, A et B, sont <strong>indépendants</strong> si <span class="math inline">\(P(A\cap B)=P(A)\times P(B)\)</span>
</div>

<p>Si <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> deux évènements indépendants, alors <span class="math inline">\(P(A|B)=P(A)\)</span> et <span class="math inline">\(P(B|A)=P(B)\)</span>.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-8" class="example"><strong>Exemple 1.4  </strong></span>On lance deux dés cubiques équilibrés numérotés de 1 à 6. Soit <span class="math inline">\(A\)</span> l’événement d’obtenir une somme égale six et <span class="math inline">\(B\)</span> désigne l’événement où le premier dé est égal à quatre.</p>
<p>Vérifier que les deux évènements <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> sont indépendants.</p>
<span class="math inline">\(P(A\cap B)=p(\{4,2 \})=1/36\)</span> et <span class="math inline">\(P(A)=1/6\)</span>, <span class="math inline">\(P(B)=1/6\)</span>. Donc <span class="math inline">\(P(A)\times P(B)=1/6 \times 1/6 =1/36 =P(A\cap B)\)</span>. Ainsi, les évènements <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> sont indépendants.
</div>

</div>
</div>
<div id="variables-aléatoires" class="section level2">
<h2><span class="header-section-number">1.2</span> Variables aléatoires</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-9" class="definition"><strong>Définition 1.3  </strong></span>Soit une expérience aléatoire d’univers <span class="math inline">\(\Omega\)</span>.</p>
<p>Une variable aléatoire <span class="math inline">\(X\)</span> est une application de l’ensemble <span class="math inline">\(\Omega\)</span> vers un ensemble de réalisations.</p>
Pour tout évènement <span class="math inline">\(A\)</span>, on <span class="math inline">\(P(X \in A)=P(X^{-1}(A))\)</span> où <span class="math inline">\(X^{-1}(A)\)</span> est l’évènement comprenant tous les éléments <span class="math inline">\(\omega \in \Omega\)</span> tels que <span class="math inline">\(X(\omega) \in A\)</span>.
</div>

<p>La <strong>fonction de répartition</strong> <span class="math inline">\(F\)</span> d’une variable aléatoire <span class="math inline">\(X\)</span> est définie par
<span class="math display">\[
F(x)=P(X \leq x)=P(X \in ]-\infty, x]), \; \forall \, x \in \mathbb{R}
\]</span>
Une variable aléatoire <span class="math inline">\(X\)</span> est dite <strong>discrète</strong> si son ensemble des valeurs possibles est dénombrables. Dans ce cas, on a
<span class="math display">\[
F(x)=\sum_{k \leq x }P(X=k)
\]</span>
Une variable aléatoire <span class="math inline">\(X\)</span> est dite <strong>continue</strong> s’il existe une fonction <span class="math inline">\(f(x)\)</span>, appelée <em>densité de probabilité</em>, telle que
<span class="math display">\[
P(X\in B)=\int_Bf(x)dx \; \text{ pour tout ensemble  }B
\]</span>
Puisque <span class="math inline">\(F(x)=\displaystyle \int_{-\infty}^x f(x) dx\)</span>, alors
<span class="math display">\[
f(x)=\frac{d}{dx}F(x)
\]</span>
La fonction de répartition <strong>jointe</strong> d’un couple aléatoire <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> est <span class="math inline">\(F(x,y)=P(X \leq x, Y \leq y)\)</span>.</p>
<p>Les fonctions de répartition de <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>,
<span class="math display">\[
F_X(x)=P(X\leq x) \text{ et }F_Y(y)=P(Y\leq y)
\]</span>
peuvent être déduites de <span class="math inline">\(F(x,y)\)</span>. En effet,
<span class="math display">\[
F_X(x)=\lim_{y \longrightarrow \infty}F(x,y) \; \text{ et }\;F_Y(y)=\lim_{x \longrightarrow \infty}F(x,y)
\]</span>
Les variables aléatoires <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont <strong>indépendantes</strong> si
<span class="math display">\[
F(x,y)=F_X(x) F_Y(y)
\]</span>
<span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont <strong>continues</strong> s’il existe une fonction <span class="math inline">\(f(x,y)\)</span>, dite densité de probabilité <em>jointe</em>, telle que
<span class="math display">\[
P(X\in A, Y\in B)=\int_A\int_B f(x,y)dxdy\;\; \forall A,B
\]</span>
La fonction de répartition d’une suite de <span class="math inline">\(n\)</span> variables aléatoires <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> est définie par:
<span class="math display">\[
F(X_1, X_2, \ldots, X_n)=P(X_1\leq x_1, X_2\leq x_2, \ldots, X_n \leq x_n)
\]</span>
et sont indépendantes si
<span class="math display">\[
F(X_1, X_2, \ldots, X_n)=F_{X_1}(x_1)F_{X_2}(x_2)\ldots F_{X_n}(x_n)
\]</span></p>
</div>
<div id="espérance-mathématique" class="section level2">
<h2><span class="header-section-number">1.3</span> Espérance mathématique</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-10" class="definition"><strong>Définition 1.4  </strong></span>L’<strong>espérance mathématique</strong> ou <strong>moyenne</strong> d’une variavle aléatoire <span class="math inline">\(X\)</span>, notée <span class="math inline">\(\mathbb{E}(X)\)</span>, est définie par:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}(X) &amp; = \int_{\mathbb{R}}x dF(x)\\
 &amp; = \begin{cases}
   \displaystyle \sum_x xP(X=x)\; \text{si }X \text{ est discrète}\\
 \displaystyle  \int_{\mathbb{R}} xf(x)dx \; \text{si }X \text{ est continue}
     \end{cases}
  \end{align*}\]</span></p>
</div>

<p>De même, on définit l’espérance d’une fonction de <span class="math inline">\(X\)</span>, <span class="math inline">\(g(X)\)</span>, par:
<span class="math display">\[
\mathbb{E}\left[ g(X)\right]=\int_{\mathbb{R}}x dF_g(x)=\int_{\mathbb{R}}g(x) dF(x)
\]</span>
L’espérance d’une somme de variables aléatoires est la somme des espérances:
<span class="math display">\[
\mathbb{E}\left[\sum_{i=1}^n X_i \right]=\sum_{i=1}^n\mathbb{E}(X_i)
\]</span>
La <strong>variance</strong> d’une variable aléatoire, <span class="math inline">\(X\)</span>, est définie par
<span class="math display">\[
\mathbb{V}(X)=\mathbb{E}\left[{(X-\mathbb{E}(X))}^2 \right]=\mathbb{E}(X^2)-{\mathbb{E}(X)}^2
\]</span>
Deux variables aléatoires, <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>, sont dites <strong>non corréllées</strong> si leur covariance, définie par:
<span class="math display">\[
\text{Cov}(X,Y)=\mathbb{E}\left[(X-\mathbb{E}(X))(Y-\mathbb{E}(Y))\right]=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
\]</span>
est <strong>nulle</strong>. Noter que si <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont indépendantes, alors elles sont non corréllées (<span class="math inline">\(\text{Cov}(X,Y)=0\)</span>).</p>
<div id="propriétés-de-la-covariance" class="section level3 unnumbered">
<h3>Propriétés de la covariance</h3>
<p>Pour toutes variables aléatoires <span class="math inline">\(X, Y, Z\)</span> et <span class="math inline">\(a \in \mathbb{R}\)</span>, on a:</p>
<p><strong>1.</strong> Cov<span class="math inline">\((X,X)=\mathbb{V}(X)\)</span> et Cov<span class="math inline">\((X,Y)=\)</span> Cov$(Y,X) $</p>
<p><strong>2.</strong> Cov<span class="math inline">\((aX,Y)=a\)</span>Cov<span class="math inline">\((X,Y)\)</span>.</p>
<p><strong>3.</strong> Cov<span class="math inline">\((X,Y+Z)=\)</span> Cov<span class="math inline">\((X,Y)+\)</span> Cov<span class="math inline">\((X,Z)\)</span>.</p>
<p>Une généralisation de la troisième propriétés est donnée par:
<span class="math display">\[
\text{Cov}\left( \sum_{i=1}^nX_i,\sum_{j=1}^m Y_i \right)=\sum_{i=1}^n \sum_{j=1}^m \text{Cov}(X_i,Y_j)
\]</span>
Une expression utile pour la variance de la somme des variables aléatoires peut être déduite comme suit:
<span class="math display">\[\begin{align*}
\mathbb{V}\left(\sum_{i=1}^nX_i \right)&amp; =\text{Cov}\left(\sum_{i=1}^nX_i,\sum_{i=1}^nX_i\right)\\
&amp;=\sum_{i=1}^n \sum_{j=1}^n\text{Cov}(X_i,X_j)\\
&amp;=\sum_{i=1}^n\text{Cov}(X_i,X_i)+\sum_{i=1}^n\sum_{i\neq j}\text{Cov}(X_i,X_j)\\
&amp;=\sum_{i=1}^n\mathbb{V}(X_i)+2\sum_{i=1}^n\sum_{j &lt;i} \text{Cov}(X_i,X_j)
\end{align*}\]</span></p>

<div class="definition">
<p><span id="def:unnamed-chunk-11" class="definition"><strong>Définition 1.5  </strong></span>Si <span class="math inline">\(X_1,X_2,\ldots, X_n\)</span> sont <em>indépendantes et identiquement distribuées</em>, noté <span class="math inline">\(X_i \sim iid\)</span>, d’espérance <span class="math inline">\(m\)</span> et de variance <span class="math inline">\(\sigma^2\)</span>, alors:</p>
<ul>
<li><span class="math inline">\(\overline{X}=\displaystyle \frac{1}{n}\sum_{i=1}^n X_i\)</span> est appelée moyenne empirique.</li>
<li><span class="math inline">\(\mathbb{E}(\overline{X})=m\)</span> et <span class="math inline">\(\mathbb{V}(\overline{X})=\displaystyle \frac{\sigma^2}{n}\)</span>.</li>
<li>Cov<span class="math inline">\((\overline{X},X_i-\overline{X})=0,\)</span> <span class="math inline">\(i=1,2,\ldots,n\)</span>.</li>
</ul>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-12" class="example"><strong>Exemple 1.5  </strong></span>Calculer la variance d’une variable aléatoire <span class="math inline">\(X\)</span> suivant une loi binomiale de paramètres <span class="math inline">\(n\)</span> et <span class="math inline">\(p\)</span>.</p>
Puisqu’une telle variable aléatoire représente le nombre de succès dans <span class="math inline">\(n\)</span> essais indépendants lorsque chaque essai a une probabilité commune p d’être un succès,
nous pouvons écrire
<span class="math display">\[
X=X_1+X_2+\ldots+X_n
\]</span>
où <span class="math inline">\(X_i \stackrel{iid}{\sim}B(p)\)</span> telle que
</div>

<p><span class="math display">\[
X_i=\begin{cases}
1 \text{ si le ième issue est un succés}\\
0 \text{ sinon}
\end{cases}
\]</span>
Par conséquent, on aura <span class="math inline">\(\mathbb{V}(X)=\displaystyle \sum_{i=1}^n\mathbb{V}(X_i)\)</span>, Or
<span class="math display">\[\begin{align*}
\mathbb{V}(X_i)&amp;=\mathbb{E}(X_i^2)-{\mathbb{E}(X_i)}^2\\
&amp;=\mathbb{E}(X_i)-{\mathbb{E}(X_i)}^2 \text{ car } X_i^2=X_i\\
&amp;= p-p^2= p(1-p)
\end{align*}\]</span>
Donc <span class="math inline">\(\mathbb{V}(X)=np(1-p)\)</span>.</p>
</div>
</div>
<div id="fonctions-génératrices-des-moments" class="section level2">
<h2><span class="header-section-number">1.4</span> Fonctions génératrices des moments</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-13" class="definition"><strong>Définition 1.6  </strong></span>La fonction génératrice des moments <span class="math inline">\(\phi(t)\)</span> de la variable aléatoire <span class="math inline">\(X\)</span> est définie pour tout <span class="math inline">\(t \in \mathbb{R}\)</span> par
<span class="math display">\[\begin{align*}
\phi(t)&amp;=\mathbb{E}\left[e^{tX} \right]\\
&amp;= \begin{cases}
\displaystyle \sum_x e^{tx}P(X=x)\text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R}}e^{tx} f(x)dx \text{ si } X \text{ est continue}
\end{cases}
\end{align*}\]</span></p>
</div>

<p><span class="math inline">\(\phi(t)\)</span> est appelée fonction génératrice des moments car tous les moments de <span class="math inline">\(X\)</span> peuvent être obtenues par les dérivées successives de <span class="math inline">\(\phi(t)\)</span>. Par exemple</p>
<p><span class="math display">\[\begin{align*}
\phi&#39;(t)&amp;=\dfrac{d}{dt}\mathbb{E}\left[e^{tX} \right] \\
&amp;=\mathbb{E}\left[\dfrac{d}{dt}(e^{tX}) \right]=\mathbb{E}\left[Xe^{tX} \right]
\end{align*}\]</span></p>
<p>Par conséquent <span class="math inline">\(\phi&#39;(0)= \mathbb{E}(X)\)</span>.</p>
<p>D’une manière plus générale, <span class="math inline">\(\phi^n(0)=\mathbb{E}\left( {X}^n\right),\; n \geq 1\)</span>.</p>
<p>Une propriété importante des fonctions génératrices des moments est que la fonction génératrice des moments de la somme des variables aléatoires indépendantes est simplement le produit des fonctions génératrices des moments individuelles. Pour voir cela, supposons que <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont indépendantes et ont respectivement des fonctions génératrices des moments <span class="math inline">\(\phi_X(t)\)</span> et <span class="math inline">\(\phi_Y(t)\)</span>. Alors la fonction génératrice des moments de <span class="math inline">\(X+Y\)</span> est donnée par:
<span class="math display">\[\begin{align*}
\phi_{X+Y}(t)&amp;=\mathbb{E}\left(e^{t(X+Y)} \right)=\mathbb{E}\left(e^{tX}e^{tY)} \right)\\
&amp;=\mathbb{E}\left(e^{tX}\right) \mathbb{E}\left(e^{tY}\right)=\phi_X(t)\phi_Y(t)
\end{align*}\]</span></p>

<div class="example">
<p><span id="exm:unnamed-chunk-14" class="example"><strong>Exemple 1.6  (Loi Binomiale de paramètres n et p)  </strong></span><span class="math display">\[\begin{align*}
\phi(t)&amp;=\mathbb{E}\left[e^{tX} \right]=\sum_{k=0}^ne^{tk}C^n_kp^k{(1-p)}^{n-k}\\
&amp;=\sum_{k=0}^nC^n_k{\left(pe^t\right)}^k{(1-p)}^{n-k}
\end{align*}\]</span></p>
</div>

<p>Or, d’après la formule de Binôme, on a
<span class="math display">\[
{(a+b)}^n=\sum_{k=0}^nC_n^ka^kb^{n-k}
\]</span>
d’où <span class="math inline">\(\phi(t)={\left( pe^t+(1-p)\right)}^n\)</span> et par conséquent
<span class="math display">\[
\phi&#39;(t)=n{\left(pe^t+1-p \right)}^{n-1}pe^t
\]</span>
D’où <span class="math inline">\(\mathbb{E}(X)=\phi&#39;(0)=np\)</span>.</p>
<p>Dérivons une deuxième fois la fonction <span class="math inline">\(\phi(t)\)</span>, on obtient</p>
<span class="math display">\[
\phi&#39;&#39;(t)=n(n-1){\left(pe^t+1-p \right)}^{n-2}{\left(pe^t\right)}^2+n{\left(pe^t+1-p \right)}^{n-1}pe^t
\]</span>
En déduit, alors
<span class="math display">\[
\mathbb{E}(X^2)=\phi&#39;&#39;(0)=n(n-1)p^2+np
\]</span>
Donc, on peut déduire la variance de <span class="math inline">\(X\)</span>.
<span class="math display">\[
\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}{(X)}^2=\phi&#39;&#39;(0)-{\left( \phi&#39;(0)\right)}^2=np(1-p)
\]</span>

<div class="example">
<span id="exm:unnamed-chunk-15" class="example"><strong>Exemple 1.7  (Loi Normale standard)  </strong></span>
</div>

<p><span class="math display">\[\begin{align*}
\mathbb{E}\left(e^{tX} \right)&amp;=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{tx-x^2/2}dx\\
&amp;=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x^2-2tx)/2}dx\\
&amp;=e^{t^2/2}\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x-t)^2/2}dx\\
&amp;= e^{t^2/2}
\end{align*}\]</span>
Si <span class="math inline">\(Y \sim N(m,\sigma^2)\)</span>, alors
<span class="math display">\[
\phi_Y(t)=\mathbb{E}\left[e^{t(\sigma X+m)} \right]=\exp\left[\dfrac{\sigma^2t^2}{2}+m \right]
\]</span>
Sous R, on peut déterminer et évaluer les fonctions génératrices des moments en utilisant l’extension <strong><code>MGF</code></strong> qui est téléchargeable depuis l’adresse suivante: <a href="https://github.com/alexandernel14/MGF" class="uri">https://github.com/alexandernel14/MGF</a>.</p>
<p>Une description de l’installation et l’utilisation de cette extension est donnée comme suit:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># installer &#39;devtools&#39;: install.packages(&quot;devtools&quot;)</span></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="co"># installer &#39;MGF&#39; depuis github</span></a>
<a class="sourceLine" id="cb1-3" title="3"><span class="co">#devtools::install_github(&quot;alexandernel14/MGF&quot;,force = T)</span></a>
<a class="sourceLine" id="cb1-4" title="4"><span class="co"># charger l&#39;extension &#39;MGF&#39;</span></a>
<a class="sourceLine" id="cb1-5" title="5"><span class="kw">library</span>(MGF)</a>
<a class="sourceLine" id="cb1-6" title="6"><span class="co"># fonction génératrice de la loi binomiale</span></a>
<a class="sourceLine" id="cb1-7" title="7"><span class="kw">mgf</span>(<span class="st">&quot;Binomial&quot;</span>)</a></code></pre></div>
<pre><code>[1] &quot;(1-p+p*exp(t))^n&quot;</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" title="1"><span class="co"># t=0, ordre 1</span></a>
<a class="sourceLine" id="cb3-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Binomial&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">1</span>, <span class="dt">n=</span><span class="dv">10</span>, <span class="dt">p=</span><span class="fl">0.4</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Binomial distribution is: 
[1] &quot;(1-p+p*exp(t))^n&quot;
The formula for the 1st moment is: 
(1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t)))
The value of the 1st moment is: 
[1] 4
The formula of the 1st centralized moment is: 
(1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t)))/(1 - p + p * 
    exp(t))^n
The 1st centralized moment&#39;s value is: 
[1] 4</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># t=0, ordre 2</span></a>
<a class="sourceLine" id="cb5-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Binomial&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">2</span>, <span class="dt">n=</span><span class="dv">10</span>, <span class="dt">p=</span><span class="fl">0.4</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Binomial distribution is: 
[1] &quot;(1-p+p*exp(t))^n&quot;
The formula for the 2nd moment is: 
(1 - p + p * exp(t))^((n - 1) - 1) * ((n - 1) * (p * exp(t))) * 
    (n * (p * exp(t))) + (1 - p + p * exp(t))^(n - 1) * (n * 
    (p * exp(t)))
The value of the 2nd moment is: 
[1] 18.4
The formula of the 2nd centralized moment is: 
((1 - p + p * exp(t))^((n - 1) - 1) * ((n - 1) * (p * exp(t))) * 
    (n * (p * exp(t))) + (1 - p + p * exp(t))^(n - 1) * (n * 
    (p * exp(t))))/(1 - p + p * exp(t))^n - (1 - p + p * exp(t))^(n - 
    1) * (n * (p * exp(t))) * ((1 - p + p * exp(t))^(n - 1) * 
    (n * (p * exp(t))))/((1 - p + p * exp(t))^n)^2
The 2nd centralized moment&#39;s value is: 
[1] 2.4</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># fonction génératrice de la loi normale</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="kw">mgf</span>(<span class="st">&quot;Normal&quot;</span>)</a></code></pre></div>
<pre><code>[1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot;</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" title="1"><span class="co"># t=0, ordre 1</span></a>
<a class="sourceLine" id="cb9-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Normal&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Normal distribution is: 
[1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot;
The formula for the 1st moment is: 
exp(mu * t + 0.5 * sigma^2 * t^2) * (mu + 0.5 * sigma^2 * (2 * 
    t))
The value of the 1st moment is: 
[1] 0
The formula of the 1st centralized moment is: 
mu + 0.5 * sigma^2 * (2 * t)
The 1st centralized moment&#39;s value is: 
[1] 0</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" title="1"><span class="co"># t=0, ordre 2</span></a>
<a class="sourceLine" id="cb11-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Normal&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Normal distribution is: 
[1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot;
The formula for the 2nd moment is: 
exp(mu * t + 0.5 * sigma^2 * t^2) * (mu + 0.5 * sigma^2 * (2 * 
    t)) * (mu + 0.5 * sigma^2 * (2 * t)) + exp(mu * t + 0.5 * 
    sigma^2 * t^2) * (0.5 * sigma^2 * 2)
The value of the 2nd moment is: 
[1] 1
The formula of the 2nd centralized moment is: 
0.5 * sigma^2 * 2
The 2nd centralized moment&#39;s value is: 
[1] 1</code></pre>
</div>
<div id="fonctions-caractéristiques" class="section level2">
<h2><span class="header-section-number">1.5</span> Fonctions caractéristiques</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-16" class="definition"><strong>Définition 1.7  </strong></span>La <strong>fonction caractéristique</strong> d’une variable aléatoire <span class="math inline">\(X\)</span> est la fonction à valeurs complexes définie sur <span class="math inline">\(\mathbb {R}\)</span> par</p>
<p><span class="math display">\[\begin{align*}\psi_{X}(t)&amp;=\mathbb{E} \left[{e} ^{\mathrm{i} tX}\right]\\
&amp;=\begin{cases}
\displaystyle \sum_{i=1}^n {e} ^{\mathrm{i} tX }P(X=x_i) \text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R} }e^{\mathrm{i} tx} f(x)\,\mathrm{d} x \text{ si } X \text{ est continue}
\end{cases}
\end{align*}\]</span>
où <span class="math inline">\(\mathrm{i}=\sqrt{-1}\)</span>.</p>
</div>


<div class="proposition">
<p><span id="prp:unnamed-chunk-17" class="proposition"><strong>Proposition 1.1  </strong></span>Soit <span class="math inline">\(X\)</span> une variable aléatoire, <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> deux réels.
Alors les propriétés suivantes sont toujours vraies :</p>
<p><strong>1.</strong> Pour tout <span class="math inline">\(t \in \mathbb{R}\)</span>, <span class="math inline">\(\left|\psi_X(t)\right| \leq 1\)</span>.</p>
<p><strong>2.</strong> <span class="math inline">\(\psi_X(0)=1\)</span>.</p>
<p><strong>3.</strong> Pour tout <span class="math inline">\(t \in \mathbb{R}\)</span>, <span class="math inline">\(\psi_X(−t) = \overline{\psi_X(t)}\)</span>.</p>
<p><strong>4.</strong> Pour tout <span class="math inline">\(t \in \mathbb{R}\)</span>, <span class="math inline">\(\psi_{aX+b}(t) = e^{\mathrm{i}tb} \psi_X(at)\)</span>.</p>
<p><strong>5.</strong> <span class="math inline">\(\psi_X(t)\)</span> est continue sur <span class="math inline">\(\mathbb{R}\)</span>.</p>
</div>


<div class="proposition">
<span id="prp:unnamed-chunk-18" class="proposition"><strong>Proposition 1.2  </strong></span>Si le moment d’ordre <span class="math inline">\(n\)</span> d’une variable aléatoire <span class="math inline">\(X\)</span> existe, alors la fonction caractéristique de
<span class="math inline">\(X\)</span> est <span class="math inline">\(n\)</span> fois dérivable et :
<span class="math display">\[
  \mathbb{E}\left({X}^n \right)=\dfrac{1}{{\mathrm{i}}^n}{\psi}^n(0)
  \]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-19" class="example"><strong>Exemple 1.8  (Loi normal standard)  </strong></span>La fonction caractéristique de la loi normale standard est: <span class="math inline">\(\psi(t)={e}^{-t^2/2}\)</span>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-20" class="example"><strong>Exemple 1.9  (Loi exponentielle)  </strong></span>La fonction caractéristique de la loi exponentielle de paramètre <span class="math inline">\(\lambda\)</span> est:
<span class="math display">\[
  \psi(t)=\dfrac{\lambda}{\lambda-\mathrm{i}t}
  \]</span>
</div>

</div>
<div id="espérance-conditionnelle" class="section level2">
<h2><span class="header-section-number">1.6</span> Espérance conditionnelle</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="chaînes-de-markov.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Calcul_Stochastique.pdf", "Calcul_Stochastique.epub", "Calcul_Stochastique.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
