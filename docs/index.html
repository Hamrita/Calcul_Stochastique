<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Calcul stochastique</title>
  <meta name="description" content="Calcul stochastique" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Calcul stochastique" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Calcul stochastique" />
  
  
  

<meta name="author" content="Mohamed Essaied Hamrita" />


<meta name="date" content="2021-01-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="chaînes-de-markov.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Calcul stochastique</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Rappel sur le calcul des probabilités</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#notion-de-probabilité"><i class="fa fa-check"></i><b>1.1</b> Notion de probabilité</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#probabilité-conditionnelle"><i class="fa fa-check"></i><b>1.1.1</b> Probabilité conditionnelle</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#indépendance"><i class="fa fa-check"></i><b>1.1.2</b> Indépendance</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#variables-aléatoires"><i class="fa fa-check"></i><b>1.2</b> Variables aléatoires</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#espérance-mathématique"><i class="fa fa-check"></i><b>1.3</b> Espérance mathématique</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#propriétés-de-la-covariance"><i class="fa fa-check"></i>Propriétés de la covariance</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#fonctions-génératrices-des-moments"><i class="fa fa-check"></i><b>1.4</b> Fonctions génératrices des moments</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#fonctions-caractéristiques"><i class="fa fa-check"></i><b>1.5</b> Fonctions caractéristiques</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#espérance-conditionnelle---variance-conditionnelle"><i class="fa fa-check"></i><b>1.6</b> Espérance conditionnelle - Variance conditionnelle</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#densité-conditionnelle"><i class="fa fa-check"></i><b>1.6.1</b> Densité conditionnelle</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#espérance-conditionnelle"><i class="fa fa-check"></i><b>1.6.2</b> Espérance conditionnelle</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#variance-conditionnelle"><i class="fa fa-check"></i><b>1.6.3</b> Variance conditionnelle</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#théorèmes-limites"><i class="fa fa-check"></i><b>1.6.4</b> Théorèmes limites</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#processus-stochastique"><i class="fa fa-check"></i><b>1.7</b> Processus stochastique</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercices"><i class="fa fa-check"></i>Exercices</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercice-1"><i class="fa fa-check"></i>Exercice 1</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercice-2"><i class="fa fa-check"></i>Exercice 2</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercice-3"><i class="fa fa-check"></i>Exercice 3</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercice-4"><i class="fa fa-check"></i>Exercice 4</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercice-5"><i class="fa fa-check"></i>Exercice 5</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercice-6"><i class="fa fa-check"></i>Exercice 6</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html"><i class="fa fa-check"></i><b>2</b> Chaînes de Markov</a><ul>
<li class="chapter" data-level="2.1" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#définitions-et-exemples"><i class="fa fa-check"></i><b>2.1</b> Définitions et exemples</a></li>
<li class="chapter" data-level="2.2" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#loi-des-x_n"><i class="fa fa-check"></i><b>2.2</b> Loi des <span class="math inline">\(X_n\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#classification-des-états"><i class="fa fa-check"></i><b>2.3</b> Classification des états</a></li>
<li class="chapter" data-level="2.4" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#temps-dabsorption"><i class="fa fa-check"></i><b>2.4</b> Temps d’absorption</a><ul>
<li class="chapter" data-level="2.4.1" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#temps-darrêt"><i class="fa fa-check"></i><b>2.4.1</b> Temps d’arrêt</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#distribution-limite"><i class="fa fa-check"></i><b>2.5</b> Distribution limite</a><ul>
<li class="chapter" data-level="2.5.1" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#détermination-de-la-distribution-stationnaire"><i class="fa fa-check"></i><b>2.5.1</b> Détermination de la distribution stationnaire</a></li>
<li class="chapter" data-level="2.5.2" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#comportement-de-long-terme"><i class="fa fa-check"></i><b>2.5.2</b> Comportement de long terme</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#simulation-dune-chaîne-de-markov-discrète"><i class="fa fa-check"></i><b>2.6</b> Simulation d’une chaîne de Markov discrète</a><ul>
<li class="chapter" data-level="2.6.1" data-path="chaînes-de-markov.html"><a href="chaînes-de-markov.html#exemple-illustratif"><i class="fa fa-check"></i><b>2.6.1</b> Exemple illustratif</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="processus-de-poisson.html"><a href="processus-de-poisson.html"><i class="fa fa-check"></i><b>3</b> Processus de poisson</a></li>
<li class="chapter" data-level="4" data-path="processus-gaussien.html"><a href="processus-gaussien.html"><i class="fa fa-check"></i><b>4</b> Processus Gaussien</a></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a><ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>5.1</b> Example one</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>5.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliographie.html"><a href="bibliographie.html"><i class="fa fa-check"></i>Bibliographie</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Calcul stochastique</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Calcul stochastique</h1>
<p class="author"><em><strong>Mohamed Essaied Hamrita</strong></em></p>
<p class="date"><em><strong>2021</strong></em></p>
</div>
<div id="rappel-sur-le-calcul-des-probabilités" class="section level1">
<h1><span class="header-section-number">Chapitre 1</span> Rappel sur le calcul des probabilités</h1>
<div id="notion-de-probabilité" class="section level2">
<h2><span class="header-section-number">1.1</span> Notion de probabilité</h2>
<p>Une notion basique dans la théorie des probabilités est l’<strong>expérience aléatoire</strong> dont on ne connait pas son résultat en avance. L’ensemble de tous les résultats de l’expérience est l’<strong>ensemble des possibles</strong> ou encore l’<strong>univers</strong> et noté <span class="math inline">\(\Omega\)</span>.</p>
<p>Un <strong>évènement</strong> est un sous ensemble de l’univers.</p>
<p>On donne quelques exemples:</p>
<p><strong>1.</strong> Si l’expérience consiste à jeter une pièce de monnaie, alors <span class="math inline">\(\Omega=\{P,F\}\)</span>.</p>
<p><strong>2.</strong> Si l’expérience consiste à jeter un dé cubique dont ses faces sont numérotées de 1 à 6, alors <span class="math inline">\(\Omega=\{1,2,3,4,5,6 \}\)</span>.</p>
<p>Pour chaque évènement <span class="math inline">\(E\)</span> de l’univers <span class="math inline">\(\Omega\)</span>, on définit un nombre <span class="math inline">\(P(E)\)</span> qui satisfait les axiomes suivants:</p>
<ul>
<li><strong>Axiome 1:</strong> <span class="math inline">\(0 \leq P(E) \leq 1\)</span>;</li>
<li><strong>Axiome 2:</strong> <span class="math inline">\(P(\Omega)=1\)</span>;</li>
<li><strong>Axiome 3:</strong> Pour toute séquence des évènements <span class="math inline">\(E_1 , E_2, \ldots , E_n\)</span> qui sont mutuellement exclusifs (<span class="math inline">\(E_i \cap E_j=\emptyset , \; \forall \; i \neq j\)</span> et <span class="math inline">\(\displaystyle{\bigcup\limits_{i=1}^nE_i=\Omega}\)</span>), on a
<span class="math display">\[
P \left(\bigcup\limits_{i=1}^nE_i \right)=\sum_{i=1}^n P(E_i)
\]</span></li>
</ul>
<p>Quelques conséquences de ces axiomes sont tirées:</p>
<ul>
<li>Si <span class="math inline">\(E \subset F\)</span>, alors <span class="math inline">\(P(E) \leq P(F)\)</span>.</li>
<li><span class="math inline">\(P ( \bar{E})=1-P(E)\)</span> où <span class="math inline">\(\bar{E}\)</span> est le complémentaire de <span class="math inline">\(E\)</span>.</li>
<li><span class="math inline">\(P \left(\bigcup\limits_{i=1}^nE_i \right)=\displaystyle\sum_{i=1}^n P(E_i)\)</span>, lorsque <span class="math inline">\(E_i\)</span> sont mutuellement exclusifs.</li>
<li><span class="math inline">\(P \left(\bigcup\limits_{i=1}^nE_i \right)\leq\displaystyle \sum_{i=1}^n P(E_i)\)</span> (Inégalité de Boole).</li>
</ul>

<div class="example">
<span id="exm:unnamed-chunk-3" class="example"><strong>Exemple 1.1  </strong></span>L’expérience consiste à lancer une pièce de monnaie équilibrée. Donc <span class="math inline">\(P(\{P\})= P(\{F\})=0.5\)</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-4" class="example"><strong>Exemple 1.2  </strong></span>On lance un dé cubique équilibré dont ses faces sont numérotées de 1 à 6.</p>
<p><span class="math inline">\(P(\{i \})= \dfrac{1}{6}, \; \forall\, i=1,2,\ldots,6\)</span>.</p>
<p>La probabilité d’obtenir un nombre pair est</p>
<span class="math inline">\(P(\{2,4,6\})= P(\{2 \})+P(\{4 \}) +P(\{6 \}) = \dfrac{1}{2}\)</span>.
</div>

<div id="probabilité-conditionnelle" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Probabilité conditionnelle</h3>

<div class="definition">
<span id="def:unnamed-chunk-5" class="definition"><strong>Définition 1.1  </strong></span>Soient <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> deux évènements tels que <span class="math inline">\(P(B)\neq0\)</span>. La probabilité de <strong>A sachant B</strong> est le nombre
<span class="math display">\[
  P(A|B)=\frac{P(A \cap B)}{P(B)}
\]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-6" class="example"><strong>Exemple 1.3  </strong></span>Une urne contient 10 boules numérotées de 1 à 10 indiscernables au toucher. On tire au hasard une boule. Sachant que le numéro de la boule tirée est au moins égale à 5, quelle est la probabilité qu’il soit égale à 10?</p>
Soit <span class="math inline">\(A\)</span> l’évènement d’avoir une boule portant le numéro 10. Soit <span class="math inline">\(B\)</span> l’évènement d’avoir une boule portant un numéro supèrieur ou égale à 5. La probabilité demandée est <span class="math inline">\(P(A|B)\)</span>
<span class="math display">\[
P(A|B)=\frac{P(A \cap B)}{P(B)}=\frac{P(A)}{P(B)}=\frac{1/10}{6/10}=1/6
\]</span>
</div>

</div>
<div id="indépendance" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Indépendance</h3>

<div class="definition">
<span id="def:unnamed-chunk-7" class="definition"><strong>Définition 1.2  </strong></span>On dit que deux évènements, A et B, sont <strong>indépendants</strong> si <span class="math inline">\(P(A\cap B)=P(A)\times P(B)\)</span>
</div>

<p>Si <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> deux évènements indépendants, alors <span class="math inline">\(P(A|B)=P(A)\)</span> et <span class="math inline">\(P(B|A)=P(B)\)</span>.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-8" class="example"><strong>Exemple 1.4  </strong></span>On lance deux dés cubiques équilibrés numérotés de 1 à 6. Soit <span class="math inline">\(A\)</span> l’événement d’obtenir une somme égale six et <span class="math inline">\(B\)</span> désigne l’événement où le premier dé est égal à quatre.</p>
<p>Vérifier que les deux évènements <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> sont indépendants.</p>
<span class="math inline">\(P(A\cap B)=p(\{4,2 \})=1/36\)</span> et <span class="math inline">\(P(A)=1/6\)</span>, <span class="math inline">\(P(B)=1/6\)</span>. Donc <span class="math inline">\(P(A)\times P(B)=1/6 \times 1/6 =1/36 =P(A\cap B)\)</span>. Ainsi, les évènements <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> sont indépendants.
</div>

</div>
</div>
<div id="variables-aléatoires" class="section level2">
<h2><span class="header-section-number">1.2</span> Variables aléatoires</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-9" class="definition"><strong>Définition 1.3  </strong></span>Soit une expérience aléatoire d’univers <span class="math inline">\(\Omega\)</span>.</p>
<p>Une variable aléatoire <span class="math inline">\(X\)</span> est une application de l’ensemble <span class="math inline">\(\Omega\)</span> vers un ensemble de réalisations.</p>
Pour tout évènement <span class="math inline">\(A\)</span>, on <span class="math inline">\(P(X \in A)=P(X^{-1}(A))\)</span> où <span class="math inline">\(X^{-1}(A)\)</span> est l’évènement comprenant tous les éléments <span class="math inline">\(\omega \in \Omega\)</span> tels que <span class="math inline">\(X(\omega) \in A\)</span>.
</div>

<p>La <strong>fonction de répartition</strong> <span class="math inline">\(F\)</span> d’une variable aléatoire <span class="math inline">\(X\)</span> est définie par
<span class="math display">\[
F(x)=P(X \leq x)=P(X \in ]-\infty, x]), \; \forall \, x \in \mathbb{R}
\]</span>
Une variable aléatoire <span class="math inline">\(X\)</span> est dite <strong>discrète</strong> si son ensemble des valeurs possibles est dénombrables. Dans ce cas, on a
<span class="math display">\[
F(x)=\sum_{k \leq x }P(X=k)
\]</span>
Une variable aléatoire <span class="math inline">\(X\)</span> est dite <strong>continue</strong> s’il existe une fonction <span class="math inline">\(f(x)\)</span>, appelée <em>densité de probabilité</em>, telle que
<span class="math display">\[
P(X\in B)=\int_Bf(x)dx \; \text{ pour tout ensemble  }B
\]</span>
Puisque <span class="math inline">\(F(x)=\displaystyle \int_{-\infty}^x f(x) dx\)</span>, alors
<span class="math display">\[
f(x)=\frac{d}{dx}F(x)
\]</span>
La fonction de répartition <strong>jointe</strong> d’un couple aléatoire <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> est <span class="math inline">\(F(x,y)=P(X \leq x, Y \leq y)\)</span>.</p>
<p>Les fonctions de répartition de <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>,
<span class="math display">\[
F_X(x)=P(X\leq x) \text{ et }F_Y(y)=P(Y\leq y)
\]</span>
peuvent être déduites de <span class="math inline">\(F(x,y)\)</span>. En effet,
<span class="math display">\[
F_X(x)=\lim_{y \longrightarrow \infty}F(x,y) \; \text{ et }\;F_Y(y)=\lim_{x \longrightarrow \infty}F(x,y)
\]</span>
Les variables aléatoires <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont <strong>indépendantes</strong> si
<span class="math display">\[
F(x,y)=F_X(x) F_Y(y)
\]</span>
<span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont <strong>continues</strong> s’il existe une fonction <span class="math inline">\(f(x,y)\)</span>, dite densité de probabilité <em>jointe</em>, telle que
<span class="math display">\[
P(X\in A, Y\in B)=\int_A\int_B f(x,y)dxdy\;\; \forall A,B
\]</span>
La fonction de répartition d’une suite de <span class="math inline">\(n\)</span> variables aléatoires <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> est définie par:
<span class="math display">\[
F(X_1, X_2, \ldots, X_n)=P(X_1\leq x_1, X_2\leq x_2, \ldots, X_n \leq x_n)
\]</span>
et sont indépendantes si
<span class="math display">\[
F(X_1, X_2, \ldots, X_n)=F_{X_1}(x_1)F_{X_2}(x_2)\ldots F_{X_n}(x_n)
\]</span></p>
</div>
<div id="espérance-mathématique" class="section level2">
<h2><span class="header-section-number">1.3</span> Espérance mathématique</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-10" class="definition"><strong>Définition 1.4  </strong></span>L’<strong>espérance mathématique</strong> ou <strong>moyenne</strong> d’une variavle aléatoire <span class="math inline">\(X\)</span>, notée <span class="math inline">\(\mathbb{E}(X)\)</span>, est définie par:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}(X) &amp; = \int_{\mathbb{R}}x dF(x)\\
 &amp; = \begin{cases}
   \displaystyle \sum_x xP(X=x)\; \text{si }X \text{ est discrète}\\
 \displaystyle  \int_{\mathbb{R}} xf(x)dx \; \text{si }X \text{ est continue}
     \end{cases}
  \end{align*}\]</span></p>
</div>

<p>De même, on définit l’espérance d’une fonction de <span class="math inline">\(X\)</span>, <span class="math inline">\(g(X)\)</span>, par:
<span class="math display">\[
\mathbb{E}\left[ g(X)\right]=\int_{\mathbb{R}}x dF_g(x)=\int_{\mathbb{R}}g(x) dF(x)
\]</span>
L’espérance d’une somme de variables aléatoires est la somme des espérances:
<span class="math display">\[
\mathbb{E}\left[\sum_{i=1}^n X_i \right]=\sum_{i=1}^n\mathbb{E}(X_i)
\]</span>
La <strong>variance</strong> d’une variable aléatoire, <span class="math inline">\(X\)</span>, est définie par
<span class="math display">\[
\mathbb{V}(X)=\mathbb{E}\left[{(X-\mathbb{E}(X))}^2 \right]=\mathbb{E}(X^2)-{\mathbb{E}(X)}^2
\]</span>
Deux variables aléatoires, <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>, sont dites <strong>non corréllées</strong> si leur covariance, définie par:
<span class="math display">\[
\text{Cov}(X,Y)=\mathbb{E}\left[(X-\mathbb{E}(X))(Y-\mathbb{E}(Y))\right]=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
\]</span>
est <strong>nulle</strong>. Noter que si <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont indépendantes, alors elles sont non corréllées (<span class="math inline">\(\text{Cov}(X,Y)=0\)</span>).</p>
<div id="propriétés-de-la-covariance" class="section level3 unnumbered">
<h3>Propriétés de la covariance</h3>
<p>Pour toutes variables aléatoires <span class="math inline">\(X, Y, Z\)</span> et <span class="math inline">\(a \in \mathbb{R}\)</span>, on a:</p>
<p><strong>1.</strong> Cov<span class="math inline">\((X,X)=\mathbb{V}(X)\)</span> et Cov<span class="math inline">\((X,Y)=\)</span> Cov$(Y,X) $</p>
<p><strong>2.</strong> Cov<span class="math inline">\((aX,Y)=a\)</span>Cov<span class="math inline">\((X,Y)\)</span>.</p>
<p><strong>3.</strong> Cov<span class="math inline">\((X,Y+Z)=\)</span> Cov<span class="math inline">\((X,Y)+\)</span> Cov<span class="math inline">\((X,Z)\)</span>.</p>
<p>Une généralisation de la troisième propriétés est donnée par:
<span class="math display">\[
\text{Cov}\left( \sum_{i=1}^nX_i,\sum_{j=1}^m Y_i \right)=\sum_{i=1}^n \sum_{j=1}^m \text{Cov}(X_i,Y_j)
\]</span>
Une expression utile pour la variance de la somme des variables aléatoires peut être déduite comme suit:
<span class="math display">\[\begin{align*}
\mathbb{V}\left(\sum_{i=1}^nX_i \right)&amp; =\text{Cov}\left(\sum_{i=1}^nX_i,\sum_{i=1}^nX_i\right)\\
&amp;=\sum_{i=1}^n \sum_{j=1}^n\text{Cov}(X_i,X_j)\\
&amp;=\sum_{i=1}^n\text{Cov}(X_i,X_i)+\sum_{i=1}^n\sum_{i\neq j}\text{Cov}(X_i,X_j)\\
&amp;=\sum_{i=1}^n\mathbb{V}(X_i)+2\sum_{i=1}^n\sum_{j &lt;i} \text{Cov}(X_i,X_j)
\end{align*}\]</span></p>

<div class="definition">
<p><span id="def:unnamed-chunk-11" class="definition"><strong>Définition 1.5  </strong></span>Si <span class="math inline">\(X_1,X_2,\ldots, X_n\)</span> sont <em>indépendantes et identiquement distribuées</em>, noté <span class="math inline">\(X_i \sim iid\)</span>, d’espérance <span class="math inline">\(m\)</span> et de variance <span class="math inline">\(\sigma^2\)</span>, alors:</p>
<ul>
<li><span class="math inline">\(\overline{X}=\displaystyle \frac{1}{n}\sum_{i=1}^n X_i\)</span> est appelée moyenne empirique.</li>
<li><span class="math inline">\(\mathbb{E}(\overline{X})=m\)</span> et <span class="math inline">\(\mathbb{V}(\overline{X})=\displaystyle \frac{\sigma^2}{n}\)</span>.</li>
<li>Cov<span class="math inline">\((\overline{X},X_i-\overline{X})=0,\)</span> <span class="math inline">\(i=1,2,\ldots,n\)</span>.</li>
</ul>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-12" class="example"><strong>Exemple 1.5  </strong></span>Calculer la variance d’une variable aléatoire <span class="math inline">\(X\)</span> suivant une loi binomiale de paramètres <span class="math inline">\(n\)</span> et <span class="math inline">\(p\)</span>.</p>
Puisqu’une telle variable aléatoire représente le nombre de succès dans <span class="math inline">\(n\)</span> essais indépendants lorsque chaque essai a une probabilité commune p d’être un succès,
nous pouvons écrire
<span class="math display">\[
X=X_1+X_2+\ldots+X_n
\]</span>
où <span class="math inline">\(X_i \stackrel{iid}{\sim}B(p)\)</span> telle que
</div>

<p><span class="math display">\[
X_i=\begin{cases}
1 \text{ si le ième issue est un succés}\\
0 \text{ sinon}
\end{cases}
\]</span>
Par conséquent, on aura <span class="math inline">\(\mathbb{V}(X)=\displaystyle \sum_{i=1}^n\mathbb{V}(X_i)\)</span>, Or
<span class="math display">\[\begin{align*}
\mathbb{V}(X_i)&amp;=\mathbb{E}(X_i^2)-{\mathbb{E}(X_i)}^2\\
&amp;=\mathbb{E}(X_i)-{\mathbb{E}(X_i)}^2 \text{ car } X_i^2=X_i\\
&amp;= p-p^2= p(1-p)
\end{align*}\]</span>
Donc <span class="math inline">\(\mathbb{V}(X)=np(1-p)\)</span>.</p>
</div>
</div>
<div id="fonctions-génératrices-des-moments" class="section level2">
<h2><span class="header-section-number">1.4</span> Fonctions génératrices des moments</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-13" class="definition"><strong>Définition 1.6  </strong></span>La fonction génératrice des moments <span class="math inline">\(\phi(t)\)</span> de la variable aléatoire <span class="math inline">\(X\)</span> est définie pour tout <span class="math inline">\(t \in \mathbb{R}\)</span> par
<span class="math display">\[\begin{align*}
\phi(t)&amp;=\mathbb{E}\left[e^{tX} \right]\\
&amp;= \begin{cases}
\displaystyle \sum_x e^{tx}P(X=x)\text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R}}e^{tx} f(x)dx \text{ si } X \text{ est continue}
\end{cases}
\end{align*}\]</span></p>
</div>

<p><span class="math inline">\(\phi(t)\)</span> est appelée fonction génératrice des moments car tous les moments de <span class="math inline">\(X\)</span> peuvent être obtenues par les dérivées successives de <span class="math inline">\(\phi(t)\)</span>. Par exemple</p>
<p><span class="math display">\[\begin{align*}
\phi&#39;(t)&amp;=\dfrac{d}{dt}\mathbb{E}\left[e^{tX} \right] \\
&amp;=\mathbb{E}\left[\dfrac{d}{dt}(e^{tX}) \right]=\mathbb{E}\left[Xe^{tX} \right]
\end{align*}\]</span></p>
<p>Par conséquent <span class="math inline">\(\phi&#39;(0)= \mathbb{E}(X)\)</span>.</p>
<p>D’une manière plus générale, <span class="math inline">\(\phi^n(0)=\mathbb{E}\left( {X}^n\right),\; n \geq 1\)</span>.</p>
<p>Une propriété importante des fonctions génératrices des moments est que la fonction génératrice des moments de la somme des variables aléatoires indépendantes est simplement le produit des fonctions génératrices des moments individuelles. Pour voir cela, supposons que <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont indépendantes et ont respectivement des fonctions génératrices des moments <span class="math inline">\(\phi_X(t)\)</span> et <span class="math inline">\(\phi_Y(t)\)</span>. Alors la fonction génératrice des moments de <span class="math inline">\(X+Y\)</span> est donnée par:
<span class="math display">\[\begin{align*}
\phi_{X+Y}(t)&amp;=\mathbb{E}\left(e^{t(X+Y)} \right)=\mathbb{E}\left(e^{tX}e^{tY)} \right)\\
&amp;=\mathbb{E}\left(e^{tX}\right) \mathbb{E}\left(e^{tY}\right)=\phi_X(t)\phi_Y(t)
\end{align*}\]</span></p>

<div class="example">
<p><span id="exm:unnamed-chunk-14" class="example"><strong>Exemple 1.6  (Loi Binomiale de paramètres n et p)  </strong></span><span class="math display">\[\begin{align*}
\phi(t)&amp;=\mathbb{E}\left[e^{tX} \right]=\sum_{k=0}^ne^{tk}C^n_kp^k{(1-p)}^{n-k}\\
&amp;=\sum_{k=0}^nC^n_k{\left(pe^t\right)}^k{(1-p)}^{n-k}
\end{align*}\]</span></p>
</div>

<p>Or, d’après la formule de Binôme, on a
<span class="math display">\[
{(a+b)}^n=\sum_{k=0}^nC_n^ka^kb^{n-k}
\]</span>
d’où <span class="math inline">\(\phi(t)={\left( pe^t+(1-p)\right)}^n\)</span> et par conséquent
<span class="math display">\[
\phi&#39;(t)=n{\left(pe^t+1-p \right)}^{n-1}pe^t
\]</span>
D’où <span class="math inline">\(\mathbb{E}(X)=\phi&#39;(0)=np\)</span>.</p>
<p>Dérivons une deuxième fois la fonction <span class="math inline">\(\phi(t)\)</span>, on obtient</p>
<span class="math display">\[
\phi&#39;&#39;(t)=n(n-1){\left(pe^t+1-p \right)}^{n-2}{\left(pe^t\right)}^2+n{\left(pe^t+1-p \right)}^{n-1}pe^t
\]</span>
En déduit, alors
<span class="math display">\[
\mathbb{E}(X^2)=\phi&#39;&#39;(0)=n(n-1)p^2+np
\]</span>
Donc, on peut déduire la variance de <span class="math inline">\(X\)</span>.
<span class="math display">\[
\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}{(X)}^2=\phi&#39;&#39;(0)-{\left( \phi&#39;(0)\right)}^2=np(1-p)
\]</span>

<div class="example">
<span id="exm:unnamed-chunk-15" class="example"><strong>Exemple 1.7  (Loi Normale standard)  </strong></span>
</div>

<p><span class="math display">\[\begin{align*}
\mathbb{E}\left(e^{tX} \right)&amp;=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{tx-x^2/2}dx\\
&amp;=\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x^2-2tx)/2}dx\\
&amp;=e^{t^2/2}\dfrac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}e^{-(x-t)^2/2}dx\\
&amp;= e^{t^2/2}
\end{align*}\]</span>
Si <span class="math inline">\(Y \sim N(m,\sigma^2)\)</span>, alors
<span class="math display">\[
\phi_Y(t)=\mathbb{E}\left[e^{t(\sigma X+m)} \right]=\exp\left[\dfrac{\sigma^2t^2}{2}+m \right]
\]</span>
Sous R, on peut déterminer et évaluer les fonctions génératrices des moments en utilisant l’extension <strong><code>MGF</code></strong> qui est téléchargeable depuis l’adresse suivante: <a href="https://github.com/alexandernel14/MGF" class="uri">https://github.com/alexandernel14/MGF</a>.</p>
<p>Une description de l’installation et l’utilisation de cette extension est donnée comme suit:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># installer &#39;devtools&#39;: install.packages(&quot;devtools&quot;)</span></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="co"># installer &#39;MGF&#39; depuis github</span></a>
<a class="sourceLine" id="cb1-3" title="3"><span class="co">#devtools::install_github(&quot;alexandernel14/MGF&quot;,force = T)</span></a>
<a class="sourceLine" id="cb1-4" title="4"><span class="co"># charger l&#39;extension &#39;MGF&#39;</span></a>
<a class="sourceLine" id="cb1-5" title="5"><span class="kw">library</span>(MGF)</a>
<a class="sourceLine" id="cb1-6" title="6"><span class="co"># fonction génératrice de la loi binomiale</span></a>
<a class="sourceLine" id="cb1-7" title="7"><span class="kw">mgf</span>(<span class="st">&quot;Binomial&quot;</span>)</a></code></pre></div>
<pre><code>[1] &quot;(1-p+p*exp(t))^n&quot;</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" title="1"><span class="co"># t=0, ordre 1</span></a>
<a class="sourceLine" id="cb3-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Binomial&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">1</span>, <span class="dt">n=</span><span class="dv">10</span>, <span class="dt">p=</span><span class="fl">0.4</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Binomial distribution is: 
[1] &quot;(1-p+p*exp(t))^n&quot;
The formula for the 1st moment is: 
(1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t)))
The value of the 1st moment is: 
[1] 4
The formula of the 1st centralized moment is: 
(1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t)))/(1 - p + p * 
    exp(t))^n
The 1st centralized moment&#39;s value is: 
[1] 4</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># t=0, ordre 2</span></a>
<a class="sourceLine" id="cb5-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Binomial&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">2</span>, <span class="dt">n=</span><span class="dv">10</span>, <span class="dt">p=</span><span class="fl">0.4</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Binomial distribution is: 
[1] &quot;(1-p+p*exp(t))^n&quot;
The formula for the 2nd moment is: 
(1 - p + p * exp(t))^((n - 1) - 1) * ((n - 1) * (p * exp(t))) * 
    (n * (p * exp(t))) + (1 - p + p * exp(t))^(n - 1) * (n * 
    (p * exp(t)))
The value of the 2nd moment is: 
[1] 18.4
The formula of the 2nd centralized moment is: 
((1 - p + p * exp(t))^((n - 1) - 1) * ((n - 1) * (p * exp(t))) * 
    (n * (p * exp(t))) + (1 - p + p * exp(t))^(n - 1) * (n * 
    (p * exp(t))))/(1 - p + p * exp(t))^n - (1 - p + p * exp(t))^(n - 
    1) * (n * (p * exp(t))) * ((1 - p + p * exp(t))^(n - 1) * 
    (n * (p * exp(t))))/((1 - p + p * exp(t))^n)^2
The 2nd centralized moment&#39;s value is: 
[1] 2.4</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># fonction génératrice de la loi normale</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="kw">mgf</span>(<span class="st">&quot;Normal&quot;</span>)</a></code></pre></div>
<pre><code>[1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot;</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" title="1"><span class="co"># t=0, ordre 1</span></a>
<a class="sourceLine" id="cb9-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Normal&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Normal distribution is: 
[1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot;
The formula for the 1st moment is: 
exp(mu * t + 0.5 * sigma^2 * t^2) * (mu + 0.5 * sigma^2 * (2 * 
    t))
The value of the 1st moment is: 
[1] 0
The formula of the 1st centralized moment is: 
mu + 0.5 * sigma^2 * (2 * t)
The 1st centralized moment&#39;s value is: 
[1] 0</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" title="1"><span class="co"># t=0, ordre 2</span></a>
<a class="sourceLine" id="cb11-2" title="2"><span class="kw">MGF_evaluator</span>(<span class="st">&quot;Normal&quot;</span>,<span class="dt">t=</span><span class="dv">0</span>, <span class="dt">order_of_moment=</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>The Moment Generating Function for the  Normal distribution is: 
[1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot;
The formula for the 2nd moment is: 
exp(mu * t + 0.5 * sigma^2 * t^2) * (mu + 0.5 * sigma^2 * (2 * 
    t)) * (mu + 0.5 * sigma^2 * (2 * t)) + exp(mu * t + 0.5 * 
    sigma^2 * t^2) * (0.5 * sigma^2 * 2)
The value of the 2nd moment is: 
[1] 1
The formula of the 2nd centralized moment is: 
0.5 * sigma^2 * 2
The 2nd centralized moment&#39;s value is: 
[1] 1</code></pre>
</div>
<div id="fonctions-caractéristiques" class="section level2">
<h2><span class="header-section-number">1.5</span> Fonctions caractéristiques</h2>
Il existe des variables aléatoires pour lesquelles la fonction génératrice des moments n’existe pas. Dans ce cas, on peut faire recours à la fonction caractéristique définie ci-dessous.

<div class="definition">
<p><span id="def:unnamed-chunk-16" class="definition"><strong>Définition 1.7  </strong></span>La <strong>fonction caractéristique</strong> d’une variable aléatoire <span class="math inline">\(X\)</span> est la fonction à valeurs complexes définie sur <span class="math inline">\(\mathbb {R}\)</span> par</p>
<p><span class="math display">\[\begin{align*}\psi_{X}(t)&amp;=\mathbb{E} \left[{e} ^{\mathrm{i} tX}\right]\\
&amp;=\begin{cases}
\displaystyle \sum_{i=1}^n {e} ^{\mathrm{i} tX }P(X=x_i) \text{ si } X \text{ est discrète}\\
\displaystyle \int_{\mathbb{R} }e^{\mathrm{i} tx} f(x)\,\mathrm{d} x \text{ si } X \text{ est continue}
\end{cases}
\end{align*}\]</span>
où <span class="math inline">\(\mathrm{i}=\sqrt{-1}\)</span>.</p>
</div>


<div class="proposition">
<p><span id="prp:unnamed-chunk-17" class="proposition"><strong>Proposition 1.1  </strong></span>Soit <span class="math inline">\(X\)</span> une variable aléatoire, <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> deux réels.
Alors les propriétés suivantes sont toujours vraies :</p>
<p><strong>1.</strong> Pour tout <span class="math inline">\(t \in \mathbb{R}\)</span>, <span class="math inline">\(\left|\psi_X(t)\right| \leq 1\)</span>.</p>
<p><strong>2.</strong> <span class="math inline">\(\psi_X(0)=1\)</span>.</p>
<p><strong>3.</strong> Pour tout <span class="math inline">\(t \in \mathbb{R}\)</span>, <span class="math inline">\(\psi_X(−t) = \overline{\psi_X(t)}\)</span>.</p>
<p><strong>4.</strong> Pour tout <span class="math inline">\(t \in \mathbb{R}\)</span>, <span class="math inline">\(\psi_{aX+b}(t) = e^{\mathrm{i}tb} \psi_X(at)\)</span>.</p>
<p><strong>5.</strong> <span class="math inline">\(\psi_X(t)\)</span> est continue sur <span class="math inline">\(\mathbb{R}\)</span>.</p>
</div>


<div class="proposition">
<span id="prp:unnamed-chunk-18" class="proposition"><strong>Proposition 1.2  </strong></span>Si le moment d’ordre <span class="math inline">\(n\)</span> d’une variable aléatoire <span class="math inline">\(X\)</span> existe, alors la fonction caractéristique de
<span class="math inline">\(X\)</span> est <span class="math inline">\(n\)</span> fois dérivable et :
<span class="math display">\[
  \mathbb{E}\left({X}^n \right)=\dfrac{1}{{\mathrm{i}}^n}{\psi}^n(0)
  \]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-19" class="example"><strong>Exemple 1.8  (Loi normal standard)  </strong></span>La fonction caractéristique de la loi normale standard est: <span class="math display">\[\psi(t)={e}^{-t^2/2}\]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-20" class="example"><strong>Exemple 1.9  (Loi exponentielle)  </strong></span>La fonction caractéristique de la loi exponentielle de paramètre <span class="math inline">\(\lambda\)</span> est:
<span class="math display">\[
  \psi(t)=\dfrac{\lambda}{\mathrm{i}t-\lambda}
  \]</span>
</div>

<p>Si <span class="math inline">\(X_1,X_2,\ldots , X_n\)</span> sont des variables indépendantes, alors
<span class="math display">\[
\psi_{\sum_iX_i}(t)=\prod_i\psi_{X_i}(t)
\]</span></p>
</div>
<div id="espérance-conditionnelle---variance-conditionnelle" class="section level2">
<h2><span class="header-section-number">1.6</span> Espérance conditionnelle - Variance conditionnelle</h2>
<p>L’un des concepts les plus utiles de la théorie des probabilités est celui de la probabilité conditionnelle et de l’espérance conditionnelle. La raison est double. Premièrement, dans la pratique, nous nous intéressons souvent au calcul des probabilités et des espérances lorsqu’une information partielle est
disponible; par conséquent, les probabilités et les espérances souhaitées sont conditionnelles. Deuxièmement, pour calculer une probabilité , il est souvent extrêmement utile d’abord <em>conditionner</em> une variable aléatoire appropriée.</p>
<div id="densité-conditionnelle" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Densité conditionnelle</h3>
<p>Soient <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> deux variables aléatoires de densité jointe <span class="math inline">\(f(x,y)\)</span>.</p>

<div class="definition">
<span id="def:unnamed-chunk-21" class="definition"><strong>Définition 1.8  </strong></span>On définit la densité <strong>conditionnelle</strong> de <span class="math inline">\(X\)</span> sachant <span class="math inline">\(Y=y\)</span> par:
<span class="math display">\[
  f_{X|Y}(x,y)=\begin{cases}
\dfrac{P(X=x,Y=y)}{P_Y(Y=y)} \text{ si } X \text{ et } Y \text{ sont discrètes}\\
\dfrac{f(x,y)}{f_Y(y)}\text{ si } X \text{ et } Y \text{ sont continues}
\end{cases}
\]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-22" class="example"><strong>Exemple 1.10  </strong></span>Soit un couple aléatoire discrète <span class="math inline">\((X,Y)\)</span> définie par la densité jointe suivante:
<span class="math display">\[
  P(X=1,Y=1)=0.5;\,P(X=1,Y=2)=0.1;\,P(X=2,Y=1)=0.1;\,P(X=2,Y=2)=0.3
  \]</span>
Déterminer la loi conditionnelle <span class="math inline">\(f_{X|Y=1}(x,y)\)</span>.
</div>

<p>La densité conditionnelle est par définition
<span class="math display">\[
{f}_{X|Y=1}(x,y)=\dfrac{P(X=x,Y=1)}{{P}_Y(Y=1)}
\]</span>
Or, <span class="math inline">\(P_y(Y=1)=\displaystyle \sum_x P(X=x,Y=1)=p(1,1)+p(2,1)=0.6\)</span>.
D’où
<span class="math display">\[
{f}_{X|Y=1}(1,1)=\dfrac{P(X=1,Y=1)}{{P}_Y(Y=1)}=\dfrac{p(1,1)}{{P}_Y(1)}=\dfrac{5}{6}
\]</span>
<span class="math display">\[
{f}_{X|Y=1}(2,1)=\dfrac{P(X=2,Y=1)}{{P}_Y(Y=1)}=\dfrac{p(2,1)}{{P}_Y(1)}=\dfrac{1}{6}
\]</span></p>
</div>
<div id="espérance-conditionnelle" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Espérance conditionnelle</h3>
<p>Une espérance conditionnelle est une expression calculée depuis une distribution conditionnelle. On écrit <span class="math inline">\(\mathbb{E}\left(Y|X=x \right)\)</span> pour l’espérance de <span class="math inline">\(Y\)</span> sachant <span class="math inline">\(X=x\)</span>.</p>

<div class="definition">
<span id="def:unnamed-chunk-23" class="definition"><strong>Définition 1.9  </strong></span>L’espérance conditionnelle de <span class="math inline">\(Y\)</span> sachant <span class="math inline">\(X=x\)</span> est
<span class="math display">\[
\mathbb{E}\left(Y|X=x \right)=\left\{
\begin{array}{ll}
\displaystyle \sum_y y\mathbb{P}\left(Y=y|X=x \right) &amp; \text{ cas discret}\\
\displaystyle \int_{\mathbb{R}}yf_{Y|X}(x,y)dy &amp; \text{ cas continu}
\end{array}
\right.
\]</span>
</div>


<div class="example">
<p><span id="exm:exple11" class="example"><strong>Exemple 1.11  </strong></span>On donne <span class="math inline">\(Y=\begin{cases}  1 \text{ avec une probabilité }\frac{1}{8}\\  2 \text{ avec une probabilité }\frac{7}{8}  \end{cases}\)</span></p>
<p>et <span class="math inline">\(X|Y=\begin{cases}  2Y \text{ avec une probabilité }\frac{3}{4}\\  3Y \text{ avec une probabilité }\frac{1}{4}  \end{cases}\)</span></p>
Déterminer <span class="math inline">\(\mathbb{E}\left( X|Y=y\right)\)</span>.
</div>

<p>Si <span class="math inline">\(Y=\)</span>, alors <span class="math inline">\(\left(X|Y= \right)= \begin{cases}  2 \text{ avec une probabilité }\frac{3}{4}\\  3 \text{ avec une probabilité }\frac{1}{4}  \end{cases}\)</span></p>
<p>D’où <span class="math inline">\(\mathbb{E}\left( X|Y=1\right)=\displaystyle \sum_x x\mathbb{P}\left( X|Y=1\right)=2\times\frac{3}{4}+3\times \frac{1}{4}=\frac{9}{4}\)</span>.</p>
<p>Si <span class="math inline">\(Y=2\)</span>, alors <span class="math inline">\(\left(X|Y=2 \right)= \begin{cases}  4 \text{ avec une probabilité }\frac{3}{4}\\  6 \text{ avec une probabilité }\frac{1}{4}  \end{cases}\)</span></p>
<p>D’où <span class="math inline">\(\mathbb{E}\left( X|Y=2\right)=\displaystyle \sum_x x\mathbb{P}\left( X|Y=2\right)=4\times\frac{3}{4}+6\times \frac{1}{4}=\frac{18}{4}\)</span>.</p>
<p>Donc <span class="math inline">\(\mathbb{E}\left( X|Y=y\right)= \begin{cases}  \frac{9}{4} \text{ si }Y=1 \text{ avec une prob} =\frac{1}{8}\\  \frac{18}{4} \text{ si } Y=2 \text{ avec une prob }=\frac{7}{8}  \end{cases}\)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-24" class="example"><strong>Exemple 1.12  </strong></span>Soit la densité jointe d’un couple aléatoire définie par: <span class="math display">\[f(x,y)=\frac{2}{xy},\; \text{ pour }\;1&lt;y&lt;x&lt;e \]</span>
Déterminer <span class="math inline">\(\mathbb{E}\left( Y|X=x\right)\)</span>
</div>

<p>Par définition <span class="math inline">\(\mathbb{E}\left( Y|X=x\right)=\displaystyle \int_{\mathbb{R}}y{f}_{X|Y}(x,y)dy\)</span>.</p>
<p>Or <span class="math inline">\({f}_{X|Y}(x,y)=\dfrac{f(x,y)}{{f}_X(x)}\)</span> avec
<span class="math display">\[\begin{align*}
{f}_X(x)=&amp;\int_1^x\dfrac{2}{xy}dy=\dfrac{2}{x}\biggl[\ln(y) \biggr]_1^x\\
=&amp;\dfrac{2\ln(x)}{x},\;\text{ pour }\; 1&lt;x&lt;e.
\end{align*}\]</span>
D’où <span class="math inline">\({f}_{X|Y}(x,y)=\dfrac{2}{xy}\dfrac{x}{\ln(x)}=\dfrac{1}{y\ln(x)}\)</span> pour <span class="math inline">\(1&lt;y&lt;x\)</span>, et par conséquent,
<span class="math display">\[ 
\mathbb{E}\left( Y|X=x\right)=\int_1^x \dfrac{y}{y\ln(x)}dy=\dfrac{1}{\ln(x)}\biggl[ y\biggr]_1^x=\dfrac{x-1}{\ln(x)}
\]</span></p>
<div id="propriétés-de-lespérance-conditionnelle" class="section level4 unnumbered">
<h4>Propriétés de l’espérance conditionnelle</h4>
<p><strong>1. Linéarité:</strong> Pour toutes constantes <span class="math inline">\(a,b\)</span> et <span class="math inline">\(X,Y\)</span> et <span class="math inline">\(Z\)</span> des variables aléatoires,
<span class="math display">\[\mathbb{E}\left(aY+bZ|X=x\right)=a \mathbb{E}\left(Y|X=x\right)+b\mathbb{E}\left(Z|X=x\right)\]</span>
<strong>2. Indépendance:</strong> Si <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont deux variables aléatoires indépendantes, alors <span class="math inline">\(\mathbb{E}\left(Y|X=x\right)=\mathbb{E}\left(Y\right)\)</span></p>
<p><strong>3.</strong> <span class="math inline">\(\mathbb{E}\left(g(X)|X=x\right)=\mathbb{E}\left(X\right)\)</span> où <span class="math inline">\(g\)</span> est une transformation.</p>
<p><strong>4. Espérance totale:</strong> <span class="math inline">\(\mathbb{E}\left(Y\right)=\mathbb{E}\bigl(\mathbb{E}\left(Y|X=x\right)\bigr)\)</span>: La moyenne totale est la moyenne des moyennes.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-25" class="example"><strong>Exemple 1.13  </strong></span>Reprenons l’exemple <a href="index.html#exm:exple11">1.11</a>.</p>
<p><span class="math inline">\(\mathbb{E}\left( X|Y=y\right)= \begin{cases}  \frac{9}{4} \text{  avec une prob } =\frac{1}{8}\\  \frac{18}{4} \text{ avec une prob  }=\frac{7}{8}  \end{cases}\)</span>.</p>
<p>Donc, l’espérance totale est:</p>
<p><span class="math inline">\(\mathbb{E}\left(X\right)= \frac{9}{4} \times \frac{1}{8} + \frac{18}{4} \times \frac{7}{8} = \frac{135}{32}\)</span></p>
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-26" class="exercise"><strong>Exercice 1.1  </strong></span>On jette une pièce de monnaie équilibrée. Soit <span class="math inline">\(Y\)</span> la variable aléatoire désignant le nombre des lancers avant d’obtenir Face pour la première fois.</p>
<ol style="list-style-type: decimal">
<li>Déterminer la loi de <span class="math inline">\(Y\)</span> ainsi que son espérance et sa variance.</li>
<li>Simuler 10 000 réalisations de <span class="math inline">\(Y\)</span>.</li>
<li>Soit <span class="math inline">\(\left(X|Y=y\right)\sim \mathcal{P}(\lambda Y)\)</span>.
<ol style="list-style-type: lower-alpha">
<li>Calculer <span class="math inline">\(\mathbb{E}\left(X|Y=y\right)\)</span> et <span class="math inline">\(\mathbb{E}\left(X\right)\)</span>.</li>
<li>En prenant <span class="math inline">\(\lambda=2\)</span> et à l’aide des simulations, donner la valeur de
<span class="math inline">\(\mathbb{E}\left(X|Y=y\right)\)</span>.</li>
</ol></li>
</ol>
</div>


<div class="solution">
<p> <span class="solution"><em>Solution </em></span> 1. Soit <span class="math inline">\(P(F)=p\)</span> et <span class="math inline">\(P(P)=1-p\)</span>. On a <span class="math inline">\((n-1)\)</span> premières épreuves donnant <span class="math inline">\(P\)</span> et la nième épreuve donne <span class="math inline">\(F\)</span>, et puisque les épreuves sont indépendantes, alors
<span class="math display">\[
\mathbb{P}\bigl(Y=n \bigr)={(1-p)}^{n}p\;,\;\; n=0,1,2,\ldots  
  \]</span>
Donc <span class="math inline">\(Y\sim G(p)\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Soit <span class="math inline">\(Z=(X|Y=y)\sim \mathcal{P}(\lambda Y)\)</span>, <span class="math inline">\(\mathbb{E}\left(Z\right)=\mathbb{E}\left(X|Y=y\right)= \lambda Y\)</span>.</li>
</ol>
<span class="math display">\[\begin{align*}
\mathbb{E}\left(X\right)&amp;=\mathbb{E}\bigl(\mathbb{E}\left(X|Y=y\right)\bigr)=\mathbb{E}(\lambda Y)\\
&amp;= \lambda \mathbb{E}(Y)=\lambda \dfrac{1-p}{p}
\end{align*}\]</span>
</div>

<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-2" title="2">y=<span class="kw">rgeom</span>(<span class="dv">10000</span>, <span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb14-2" title="2">x=<span class="kw">rpois</span>(<span class="dv">10000</span>,<span class="dt">lambda =</span> <span class="dv">2</span><span class="op">*</span>y)</a>
<a class="sourceLine" id="cb14-3" title="3"><span class="kw">mean</span>(x)</a></code></pre></div>
<pre><code>[1] 2.0029</code></pre>
<p>La figure suivante donne l’évolution de <span class="math inline">\(\mathbb{E}\left(X|Y=y\right)\)</span> en fonction du nombre des simulations.
<img src="Calcul_Stochastique_files/figure-html/unnamed-chunk-29-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="variance-conditionnelle" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Variance conditionnelle</h3>
<p>Similairement à l’espérance conditionnelle, la variance conditionnelle est une variance prise par rapport à une distribution conditionnelle. Étant donné les variables aléatoires <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>, soit <span class="math inline">\(m_X=\mathbb{E}\left(Y|X=x \right)\)</span>. La variance conditionnelle <span class="math inline">\(\mathbb{V}(Y | X = x)\)</span>est définie comme suit:</p>

<div class="definition">
<span id="def:unnamed-chunk-30" class="definition"><strong>Définition 1.10  </strong></span>La variance conditionnelle de <span class="math inline">\(Y\)</span> sachant <span class="math inline">\(X=x\)</span> est
<span class="math display">\[
\mathbb{V}\left(Y|X=x \right)=\left\{
\begin{array}{ll}
\displaystyle \sum_y (y-m_X)\mathbb{P}\left(Y=y|X=x \right) &amp; \text{ cas discret}\\
\displaystyle \int_{\mathbb{R}}(y-m_X)^2f_{Y|X}(x,y)dy &amp; \text{ cas continu}
\end{array}
\right.
\]</span>
</div>

<p>La variance conditinnelle possède les propriétés suivantes:</p>
<div id="propriétes-de-la-variance-conditionnelle" class="section level4 unnumbered">
<h4>Propriétes de la variance conditionnelle</h4>
<p><strong>1.</strong> <span class="math inline">\(\mathbb{V}\left(Y|X=x \right)=\mathbb{E}\left(Y^2|X=x \right)-\mathbb{E}\left(Y|X=x \right)^2\)</span>.</p>
<p><strong>2.</strong> <span class="math inline">\(\mathbb{V}(aY + b|X = x) = a^2\mathbb{V}(Y|X = x)\)</span>.</p>
<p><strong>3. Variance totale:</strong> <span class="math inline">\(\mathbb{V}(Y) = \mathbb{E}\bigl(\mathbb{V}(Y|X)\bigr) + \mathbb{V}\bigl(\mathbb{E}(Y|X)\bigr)\)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-31" class="example"><strong>Exemple 1.14  </strong></span>Soit <span class="math inline">\(X\)</span> une variable aléatoire uniforme sur <span class="math inline">\(]0,1[\)</span>. Si <span class="math inline">\(X=x\)</span>, alors <span class="math inline">\(Y\)</span> suit une loi uniforme sur <span class="math inline">\(]0,x[\)</span>. Déterminer la variance de <span class="math inline">\(Y\)</span>.
</div>

<p>La distribution conditionnelle de <span class="math inline">\(Y|X=x\)</span> est uniforme sur <span class="math inline">\(]0,x[\)</span>. On déduit alors,
<span class="math display">\[\mathbb{E}(Y|X = x) = \dfrac{x}{2} \; \text{ et }\; \mathbb{V}(Y|X = x) = \dfrac{x^2}{12}\]</span>
La propriété de la variance totale donne,
<span class="math display">\[\begin{align*}\mathbb{V}(Y) &amp;= \mathbb{E}(\mathbb{V}(Y|X)) + \mathbb{V}(\mathbb{E}(Y|X)) = \mathbb{E}
\left(\frac{X^2}{12}\right)
+ \mathbb{V}\left(\frac{X}{2}
\right)\qquad \qquad \\
&amp;=\frac{1}{12}\mathbb{E}
\left(X^2\right)+\frac{1}{4}\mathbb{V}\left(X\right)=\frac{1}{12}\times \frac{1}{3}+\frac{1}{4}\times \frac{1}{12}=\frac{7}{144}=0.04861.
\end{align*}\]</span></p>
<p>Sous R, on peut procéder comme suit:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb16-2" title="2">x=<span class="kw">runif</span>(<span class="dv">10000</span>,<span class="dv">0</span>,<span class="dv">1</span>)  <span class="co"># réalisations uniformes sur ]0,1[</span></a>
<a class="sourceLine" id="cb16-3" title="3">y=<span class="kw">runif</span>(<span class="dv">10000</span>,<span class="dv">0</span>,x)  <span class="co"># réalisations uniformes sur ]0,x[</span></a>
<a class="sourceLine" id="cb16-4" title="4"><span class="kw">var</span>(y)  </a></code></pre></div>
<pre><code>[1] 0.04906725</code></pre>
<p>L’évolution de la variance conditionnelle suivant le nombre des simulations est donnée par le graphique suivant:
<img src="Calcul_Stochastique_files/figure-html/varCfig-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="théorèmes-limites" class="section level3">
<h3><span class="header-section-number">1.6.4</span> Théorèmes limites</h3>

<div class="theorem">
<span id="thm:unnamed-chunk-32" class="theorem"><strong>Théoreme 1.1  (Loi forte des grands nombres)  </strong></span>Si <span class="math inline">\(X_1,X_2,\ldots, X_n\)</span> sont des variables aléatoires <span class="math inline">\(iid\)</span> de moyenne <span class="math inline">\(m\)</span>, alors
<span class="math display">\[
\mathbb{P}\left[\lim_{n \rightarrow \infty}\overline{X}_n=m \right]=1
\]</span>
</div>

<p>La moyenne des <span class="math inline">\(n\)</span> premiers termes d’une suite de variables aléatoires <span class="math inline">\(iid\)</span> converge presque sûrement vers l’espérance mathématique <span class="math inline">\(\mathbb{E}(X_i)=m\)</span>, lorsque <span class="math inline">\(n\)</span> tend vers l’infini.</p>
<p>Vérifions ce théorème à l’aide des simulations. On considère l’expérience suivante: On lance une pièce de monnaie équilibrée <span class="math inline">\(n\)</span> fois. Soit <span class="math inline">\(X_i=1\)</span> si Face et <span class="math inline">\(X_i=0\)</span> si Pile. D’après la loi des grands nombres, <span class="math inline">\(\overline{X}_n\)</span> converge vers <span class="math inline">\(\mathbb{E}(X_i)=p=0.5\)</span>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb18-2" title="2">s=<span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dv">10000</span>, <span class="dt">replace =</span> T)</a>
<a class="sourceLine" id="cb18-3" title="3">x_bar=<span class="kw">cumsum</span>(s)<span class="op">/</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>)</a>
<a class="sourceLine" id="cb18-4" title="4"><span class="kw">plot</span>(x_bar, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Nombre de simulations&quot;</span>, </a>
<a class="sourceLine" id="cb18-5" title="5">     <span class="dt">ylab=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb18-6" title="6"><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb18-7" title="7"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="kw">expression</span>(<span class="kw">bar</span>(X)),<span class="kw">expression</span>(p<span class="op">==</span><span class="fl">0.5</span>)),</a>
<a class="sourceLine" id="cb18-8" title="8">       <span class="dt">lty=</span><span class="dv">1</span>,<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),<span class="dt">bty=</span><span class="st">&quot;n&quot;</span>)</a></code></pre></div>
<p><img src="Calcul_Stochastique_files/figure-html/grandNombres-1.png" width="576" style="display: block; margin: auto;" /></p>

<div class="theorem">
<span id="thm:unnamed-chunk-33" class="theorem"><strong>Théoreme 1.2  (Loi faible des grands nombres)  </strong></span>Si <span class="math inline">\(X_1,X_2,\ldots, X_n\)</span> sont des variables aléatoires <span class="math inline">\(iid\)</span> de moyenne <span class="math inline">\(m\)</span>, alors
<span class="math display">\[
\lim_{n \rightarrow \infty}\mathbb{P}\left[|\overline{X}_n-m| &lt; \epsilon \right]=1, \;\; \forall \, \epsilon &gt; 0
\]</span>
</div>


<div class="theorem">
<span id="thm:unnamed-chunk-34" class="theorem"><strong>Théoreme 1.3  (Théorème central limite)  </strong></span>Si <span class="math inline">\(X_1,X_2,\ldots, X_n\)</span> sont des variables aléatoires <span class="math inline">\(iid(m,\sigma^2)\)</span>, alors
<span class="math display">\[
\lim_{n \rightarrow \infty}\mathbb{P}\left[\frac{\overline{X}_n-m}{\sigma/\sqrt{n}} &lt; t \right]=\mathbb{P}(Z &lt;), \;\; \text{ avec } \; Z \sim N(0,1)
\]</span>
</div>

<p><strong>Illustration sous R</strong></p>
<p>Soit <span class="math inline">\(X_1, X_2,\ldots, X_{100}\)</span> une suite de variables aléatoires telle que <span class="math inline">\(X_i \stackrel{iid}{\sim}\mathcal{P(\lambda=4)}\)</span>. On sait que <span class="math inline">\(\mathbb{E}(X_i)=\mathbb{V}(X_i)=\lambda=4\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Simuler <span class="math inline">\(100\)</span> réalisations du Poisson de paramètre <span class="math inline">\(\lambda=4\)</span> puis déduire la variable de <span class="math inline">\(z\)</span>
<span class="math display">\[z=\dfrac{\overline{X}-m}{\sigma/\sqrt{n}} \]</span></p></li>
<li><p>Répéter <span class="math inline">\(100000\)</span> fois les instructions précédentes et stocker le résultat dans un objet que l’on appelle <code>mu</code>.</p></li>
<li><p>Représenter l’histogramme de <code>mu</code>. Ajouter sur le même graphique la courbe de la densité de la loi normale standard.</p></li>
</ol>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" title="1"><span class="co"># 1</span></a>
<a class="sourceLine" id="cb19-2" title="2"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb19-3" title="3">simPois=<span class="kw">rpois</span>(<span class="dt">n=</span><span class="dv">100</span>, <span class="dt">lambda=</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb19-4" title="4">z=(<span class="kw">mean</span>(simPois)<span class="op">-</span><span class="dv">4</span>)<span class="op">/</span>(<span class="dv">2</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a>
<a class="sourceLine" id="cb19-5" title="5"><span class="co"># 2</span></a>
<a class="sourceLine" id="cb19-6" title="6">Z=<span class="cf">function</span>(){</a>
<a class="sourceLine" id="cb19-7" title="7"> simPois=<span class="kw">rpois</span>(<span class="dt">n=</span><span class="dv">100</span>, <span class="dt">lambda=</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb19-8" title="8">(<span class="kw">mean</span>(simPois)<span class="op">-</span><span class="dv">4</span>)<span class="op">/</span>(<span class="dv">2</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">100</span>)) </a>
<a class="sourceLine" id="cb19-9" title="9">}</a>
<a class="sourceLine" id="cb19-10" title="10"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb19-11" title="11">mu=<span class="kw">replicate</span>(<span class="dv">100000</span>, <span class="kw">Z</span>())</a></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" title="1"><span class="co"># 3</span></a>
<a class="sourceLine" id="cb20-2" title="2"><span class="kw">hist</span>(mu,<span class="dt">freq =</span> F, <span class="dt">main=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb20-3" title="3"><span class="kw">curve</span>(dnorm,<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>,<span class="dt">add=</span>T,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="Calcul_Stochastique_files/figure-html/histPois-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="processus-stochastique" class="section level2">
<h2><span class="header-section-number">1.7</span> Processus stochastique</h2>
<p>Un processus stochastique est simplement une collection de variables aléatoires <span class="math inline">\(\{X_t, t \in I\}\)</span>. L’indice <span class="math inline">\(t\)</span> représente souvent le temps et l’ensemble <span class="math inline">\(I\)</span> est l’ensemble d’indices du processus appelé aussi espace de temps. Les ensembles d’indices les plus courants sont <span class="math inline">\(I = \{0, 1, 2,\ldots\}\)</span>, représentant le temps <strong>discret</strong>, et <span class="math inline">\(I = [0, +\infty[\)</span>, représentant le temps <strong>continu</strong>. Les processus stochastiques en temps discret sont des séquences de variables aléatoires. Les processus en temps continu sont des collections non dénombrables de variables aléatoires.</p>
<p>Les variables aléatoires d’un processus stochastique prennent des valeurs dans un espace d’états commun <span class="math inline">\(E\)</span>, discret ou continu. Un processus stochastique est spécifié par ses espaces de temps et d’état, et par les relations de dépendance entre ses variables aléatoires.</p>
</div>
<div id="exercices" class="section level2 unnumbered">
<h2>Exercices</h2>
<div id="exercice-1" class="section level3 unnumbered">
<h3>Exercice 1</h3>
<p>Pour chacune des variables aléatoires suivantes, déterminer la fonction génératrices des moments et déduire son espérance et sa variance.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X\)</span> est de densité de probabilité définie par: <span class="math inline">\(\mathbb{P}(X=1)=\frac{1}{3}\)</span> et <span class="math inline">\(\mathbb{P}(X=2)=\frac{2}{3}\)</span>.</p></li>
<li><p><span class="math inline">\(Y \sim \mathcal{U}[0,1]\)</span>.</p></li>
</ol>
</div>
<div id="exercice-2" class="section level3 unnumbered">
<h3>Exercice 2</h3>
<p>Déterminer <span class="math inline">\(\mathbb{E}\left(X|Y=y \right)\)</span> lorsque la densité jointe du couple <span class="math inline">\((X,Y)\)</span> est:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\displaystyle f(x,y)=\frac{y^2-x^2}{8}e^{-x},\;\; 0 &lt; y &lt; \infty\, , \;\; -y&lt;x&lt;y.\)</span></li>
<li><span class="math inline">\(\displaystyle f(x,y)=\frac{e^{-x/y} e^{-y}}{y},\;\; 0 &lt; x &lt; \infty\, , \;\; 0&lt;y&lt;\infty.\)</span></li>
</ol>
</div>
<div id="exercice-3" class="section level3 unnumbered">
<h3>Exercice 3</h3>
<p>Les habitants de Sousse retirent de l’argent d’un distributeur de billets selon la fonction de probabilité suivante:</p>
<table>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x_i\)</span> en DT</td>
<td align="center">100</td>
<td align="center">200</td>
<td align="center">500</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbb{P}(X=x_i)\)</span></td>
<td align="center">0.25</td>
<td align="center">0.55</td>
<td align="center">0.2</td>
</tr>
</tbody>
</table>
<p>Le nombre des clients par jour, <span class="math inline">\(N\)</span>, suit une distribution de poisson de paramètre <span class="math inline">\(\lambda = 0.5\)</span>, i.e. <span class="math inline">\(N \sim \mathcal{P}(0.5)\)</span>.</p>
<p>Soit <span class="math inline">\(S_N=X_1+X_2+\ldots +X_N\)</span> le montant d’argent total retiré par jour, où les <span class="math inline">\(X_i\)</span> sont indépentantes entre eux et avec la variable <span class="math inline">\(N\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Calculer <span class="math inline">\(\mathbb{E}(X)\)</span> et <span class="math inline">\(\mathbb{V}(X)\)</span>.</p></li>
<li><p>Déterminer <span class="math inline">\(\mathbb{E}(S_N)\)</span> et <span class="math inline">\(\mathbb{V}(S_N)\)</span>.</p></li>
<li><p>Reprendre les questions précédentes en utilisants des simulations sous R.</p></li>
</ol>
</div>
<div id="exercice-4" class="section level3 unnumbered">
<h3>Exercice 4</h3>
<p>Soit <span class="math inline">\(X_1, X_2,\ldots, X_{100}\)</span> une suite de variables aléatoires telle que <span class="math inline">\(X_i \stackrel{iid}{\sim}\mathcal{E(\lambda=4)}\)</span>. On sait que <span class="math inline">\(\mathbb{E}(X_i)=\frac{1}{\lambda}\)</span> et <span class="math inline">\(\mathbb{V}(X_i)=\frac{1}{\lambda^2}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Simuler <span class="math inline">\(100\)</span> réalisations de la loi exponentielle de paramètre <span class="math inline">\(\lambda=4\)</span> (<code>rexp()</code>) puis déduire la variable de <span class="math inline">\(z\)</span>
<span class="math display">\[z=\dfrac{\overline{X}-m}{\sigma/\sqrt{n}} \]</span></p></li>
<li><p>Répéter <span class="math inline">\(100000\)</span> fois les instructions précédentes et stocker le résultat dans un objet que l’on appelle <code>mu</code>.</p></li>
<li><p>Représenter l’histogramme de <code>mu</code>. Ajouter sur le même graphique la courbe de la densité de la loi normale standard.</p></li>
</ol>
</div>
<div id="exercice-5" class="section level3 unnumbered">
<h3>Exercice 5</h3>
<p>On lance un dé équilibré, puis une pièce de monnaie équilibrée un nombre de fois égal au résultat
du dé. Soit <span class="math inline">\(X\)</span> le résultat du dé et <span class="math inline">\(Y\)</span> le nombre de <em>Pile</em> amenés par la pièce de monnaie.</p>
<ol style="list-style-type: decimal">
<li><p>Déterminer la loi jointe du couple <span class="math inline">\((X, Y )\)</span>.</p></li>
<li><p>Soit <span class="math inline">\(n \in \{1, \ldots, 6\}\)</span>. Quelle est la loi de <span class="math inline">\(Y\)</span> sachant <span class="math inline">\(X = n\)</span> ?</p></li>
<li><p>En déduire <span class="math inline">\(\mathbb{E}\bigl[Y |X = n\bigr]\)</span>, puis <span class="math inline">\(\mathbb{E}\bigl[Y |X\bigr]\)</span>.</p></li>
<li><p>Calculer <span class="math inline">\(E[Y]\)</span>.</p></li>
<li><p>Reprendre les questions 2 à 4 à l’aide des simulations sous R.</p></li>
</ol>
</div>
<div id="exercice-6" class="section level3 unnumbered">
<h3>Exercice 6</h3>
<p>Le temps que Sarra passe à parler au téléphone suit une distribution exponentielle de moyenne 7 minutes. Quelle est la durée moyenne de son appel téléphonique si elle parle pendant plus de 3 minutes?
<!--
Soit $X\sim \mathcal{E}(1/7)$.
$\mathbb{P}\bigl(X|X > 3 \bigr)=\displaystyle \frac{1}{\mathbb{P}(X > 3)}\int_2 ^{+\infty}x \frac{1}{7}e^{-\frac{1}{7} x}dx$
!--></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="chaînes-de-markov.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Calcul_Stochastique.pdf", "Calcul_Stochastique.epub", "Calcul_Stochastique.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
