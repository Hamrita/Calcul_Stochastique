[["index.html", "Calcul stochastique Chapitre 1 Rappel sur le calcul des probabilités 1.1 Notion de probabilité 1.2 Variables aléatoires 1.3 Espérance mathématique 1.4 Fonctions génératrices des moments 1.5 Fonctions caractéristiques 1.6 Espérance conditionnelle - Variance conditionnelle 1.7 Processus stochastique Exercices", " Calcul stochastique Mohamed Essaied Hamrita 2021 Chapitre 1 Rappel sur le calcul des probabilités 1.1 Notion de probabilité Une notion basique dans la théorie des probabilités est lexpérience aléatoire dont on ne connait pas son résultat en avance. Lensemble de tous les résultats de lexpérience est lensemble des possibles ou encore lunivers et noté \\(\\Omega\\). Un évènement est un sous ensemble de lunivers. On donne quelques exemples: 1. Si lexpérience consiste à jeter une pièce de monnaie, alors \\(\\Omega=\\{P,F\\}\\). 2. Si lexpérience consiste à jeter un dé cubique dont ses faces sont numérotées de 1 à 6, alors \\(\\Omega=\\{1,2,3,4,5,6 \\}\\). Pour chaque évènement \\(E\\) de lunivers \\(\\Omega\\), on définit un nombre \\(P(E)\\) qui satisfait les axiomes suivants: Axiome 1: \\(0 \\leq P(E) \\leq 1\\); Axiome 2: \\(P(\\Omega)=1\\); Axiome 3: Pour toute séquence des évènements \\(E_1 , E_2, \\ldots , E_n\\) qui sont mutuellement exclusifs (\\(E_i \\cap E_j=\\emptyset , \\; \\forall \\; i \\neq j\\) et \\(\\displaystyle{\\bigcup\\limits_{i=1}^nE_i=\\Omega}\\)), on a \\[ P \\left(\\bigcup\\limits_{i=1}^nE_i \\right)=\\sum_{i=1}^n P(E_i) \\] Quelques conséquences de ces axiomes sont tirées: Si \\(E \\subset F\\), alors \\(P(E) \\leq P(F)\\). \\(P ( \\bar{E})=1-P(E)\\) où \\(\\bar{E}\\) est le complémentaire de \\(E\\). \\(P \\left(\\bigcup\\limits_{i=1}^nE_i \\right)=\\displaystyle\\sum_{i=1}^n P(E_i)\\), lorsque \\(E_i\\) sont mutuellement exclusifs. \\(P \\left(\\bigcup\\limits_{i=1}^nE_i \\right)\\leq\\displaystyle \\sum_{i=1}^n P(E_i)\\) (Inégalité de Boole). Exemple 1.1 Lexpérience consiste à lancer une pièce de monnaie équilibrée. Donc \\(P(\\{P\\})= P(\\{F\\})=0.5\\) Exemple 1.2 On lance un dé cubique équilibré dont ses faces sont numérotées de 1 à 6. \\(P(\\{i \\})= \\dfrac{1}{6}, \\; \\forall\\, i=1,2,\\ldots,6\\). La probabilité dobtenir un nombre pair est \\(P(\\{2,4,6\\})= P(\\{2 \\})+P(\\{4 \\}) +P(\\{6 \\}) = \\dfrac{1}{2}\\). 1.1.1 Probabilité conditionnelle Définition 1.1 Soient \\(A\\) et \\(B\\) deux évènements tels que \\(P(B)\\neq0\\). La probabilité de A sachant B est le nombre \\[ P(A|B)=\\frac{P(A \\cap B)}{P(B)} \\] Exemple 1.3 Une urne contient 10 boules numérotées de 1 à 10 indiscernables au toucher. On tire au hasard une boule. Sachant que le numéro de la boule tirée est au moins égale à 5, quelle est la probabilité quil soit égale à 10? Soit \\(A\\) lévènement davoir une boule portant le numéro 10. Soit \\(B\\) lévènement davoir une boule portant un numéro supèrieur ou égale à 5. La probabilité demandée est \\(P(A|B)\\) \\[ P(A|B)=\\frac{P(A \\cap B)}{P(B)}=\\frac{P(A)}{P(B)}=\\frac{1/10}{6/10}=1/6 \\] 1.1.2 Indépendance Définition 1.2 On dit que deux évènements, A et B, sont indépendants si \\(P(A\\cap B)=P(A)\\times P(B)\\) Si \\(A\\) et \\(B\\) deux évènements indépendants, alors \\(P(A|B)=P(A)\\) et \\(P(B|A)=P(B)\\). Exemple 1.4 On lance deux dés cubiques équilibrés numérotés de 1 à 6. Soit \\(A\\) lévénement dobtenir une somme égale six et \\(B\\) désigne lévénement où le premier dé est égal à quatre. Vérifier que les deux évènements \\(A\\) et \\(B\\) sont indépendants. \\(P(A\\cap B)=p(\\{4,2 \\})=1/36\\) et \\(P(A)=1/6\\), \\(P(B)=1/6\\). Donc \\(P(A)\\times P(B)=1/6 \\times 1/6 =1/36 =P(A\\cap B)\\). Ainsi, les évènements \\(A\\) et \\(B\\) sont indépendants. 1.2 Variables aléatoires Définition 1.3 Soit une expérience aléatoire dunivers \\(\\Omega\\). Une variable aléatoire \\(X\\) est une application de lensemble \\(\\Omega\\) vers un ensemble de réalisations. Pour tout évènement \\(A\\), on \\(P(X \\in A)=P(X^{-1}(A))\\) où \\(X^{-1}(A)\\) est lévènement comprenant tous les éléments \\(\\omega \\in \\Omega\\) tels que \\(X(\\omega) \\in A\\). La fonction de répartition \\(F\\) dune variable aléatoire \\(X\\) est définie par \\[ F(x)=P(X \\leq x)=P(X \\in ]-\\infty, x]), \\; \\forall \\, x \\in \\mathbb{R} \\] Une variable aléatoire \\(X\\) est dite discrète si son ensemble des valeurs possibles est dénombrables. Dans ce cas, on a \\[ F(x)=\\sum_{k \\leq x }P(X=k) \\] Une variable aléatoire \\(X\\) est dite continue sil existe une fonction \\(f(x)\\), appelée densité de probabilité, telle que \\[ P(X\\in B)=\\int_Bf(x)dx \\; \\text{ pour tout ensemble }B \\] Puisque \\(F(x)=\\displaystyle \\int_{-\\infty}^x f(x) dx\\), alors \\[ f(x)=\\frac{d}{dx}F(x) \\] La fonction de répartition jointe dun couple aléatoire \\(X\\) et \\(Y\\) est \\(F(x,y)=P(X \\leq x, Y \\leq y)\\). Les fonctions de répartition de \\(X\\) et \\(Y\\), \\[ F_X(x)=P(X\\leq x) \\text{ et }F_Y(y)=P(Y\\leq y) \\] peuvent être déduites de \\(F(x,y)\\). En effet, \\[ F_X(x)=\\lim_{y \\longrightarrow \\infty}F(x,y) \\; \\text{ et }\\;F_Y(y)=\\lim_{x \\longrightarrow \\infty}F(x,y) \\] Les variables aléatoires \\(X\\) et \\(Y\\) sont indépendantes si \\[ F(x,y)=F_X(x) F_Y(y) \\] \\(X\\) et \\(Y\\) sont continues sil existe une fonction \\(f(x,y)\\), dite densité de probabilité jointe, telle que \\[ P(X\\in A, Y\\in B)=\\int_A\\int_B f(x,y)dxdy\\;\\; \\forall A,B \\] La fonction de répartition dune suite de \\(n\\) variables aléatoires \\(X_1, X_2, \\ldots, X_n\\) est définie par: \\[ F(X_1, X_2, \\ldots, X_n)=P(X_1\\leq x_1, X_2\\leq x_2, \\ldots, X_n \\leq x_n) \\] et sont indépendantes si \\[ F(X_1, X_2, \\ldots, X_n)=F_{X_1}(x_1)F_{X_2}(x_2)\\ldots F_{X_n}(x_n) \\] 1.3 Espérance mathématique Définition 1.4 Lespérance mathématique ou moyenne dune variavle aléatoire \\(X\\), notée \\(\\mathbb{E}(X)\\), est définie par: \\[\\begin{align*} \\mathbb{E}(X) &amp; = \\int_{\\mathbb{R}}x dF(x)\\\\ &amp; = \\begin{cases} \\displaystyle \\sum_x xP(X=x)\\; \\text{si }X \\text{ est discrète}\\\\ \\displaystyle \\int_{\\mathbb{R}} xf(x)dx \\; \\text{si }X \\text{ est continue} \\end{cases} \\end{align*}\\] De même, on définit lespérance dune fonction de \\(X\\), \\(g(X)\\), par: \\[ \\mathbb{E}\\left[ g(X)\\right]=\\int_{\\mathbb{R}}x dF_g(x)=\\int_{\\mathbb{R}}g(x) dF(x) \\] Lespérance dune somme de variables aléatoires est la somme des espérances: \\[ \\mathbb{E}\\left[\\sum_{i=1}^n X_i \\right]=\\sum_{i=1}^n\\mathbb{E}(X_i) \\] La variance dune variable aléatoire, \\(X\\), est définie par \\[ \\mathbb{V}(X)=\\mathbb{E}\\left[{(X-\\mathbb{E}(X))}^2 \\right]=\\mathbb{E}(X^2)-{\\mathbb{E}(X)}^2 \\] Deux variables aléatoires, \\(X\\) et \\(Y\\), sont dites non corréllées si leur covariance, définie par: \\[ \\text{Cov}(X,Y)=\\mathbb{E}\\left[(X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y))\\right]=\\mathbb{E}(XY)-\\mathbb{E}(X)\\mathbb{E}(Y) \\] est nulle. Noter que si \\(X\\) et \\(Y\\) sont indépendantes, alors elles sont non corréllées (\\(\\text{Cov}(X,Y)=0\\)). Propriétés de la covariance Pour toutes variables aléatoires \\(X, Y, Z\\) et \\(a \\in \\mathbb{R}\\), on a: 1. Cov\\((X,X)=\\mathbb{V}(X)\\) et Cov\\((X,Y)=\\) Cov$(Y,X) $ 2. Cov\\((aX,Y)=a\\)Cov\\((X,Y)\\). 3. Cov\\((X,Y+Z)=\\) Cov\\((X,Y)+\\) Cov\\((X,Z)\\). Une généralisation de la troisième propriétés est donnée par: \\[ \\text{Cov}\\left( \\sum_{i=1}^nX_i,\\sum_{j=1}^m Y_i \\right)=\\sum_{i=1}^n \\sum_{j=1}^m \\text{Cov}(X_i,Y_j) \\] Une expression utile pour la variance de la somme des variables aléatoires peut être déduite comme suit: \\[\\begin{align*} \\mathbb{V}\\left(\\sum_{i=1}^nX_i \\right)&amp; =\\text{Cov}\\left(\\sum_{i=1}^nX_i,\\sum_{i=1}^nX_i\\right)\\\\ &amp;=\\sum_{i=1}^n \\sum_{j=1}^n\\text{Cov}(X_i,X_j)\\\\ &amp;=\\sum_{i=1}^n\\text{Cov}(X_i,X_i)+\\sum_{i=1}^n\\sum_{i\\neq j}\\text{Cov}(X_i,X_j)\\\\ &amp;=\\sum_{i=1}^n\\mathbb{V}(X_i)+2\\sum_{i=1}^n\\sum_{j &lt;i} \\text{Cov}(X_i,X_j) \\end{align*}\\] Définition 1.5 Si \\(X_1,X_2,\\ldots, X_n\\) sont indépendantes et identiquement distribuées, noté \\(X_i \\sim iid\\), despérance \\(m\\) et de variance \\(\\sigma^2\\), alors: \\(\\overline{X}=\\displaystyle \\frac{1}{n}\\sum_{i=1}^n X_i\\) est appelée moyenne empirique. \\(\\mathbb{E}(\\overline{X})=m\\) et \\(\\mathbb{V}(\\overline{X})=\\displaystyle \\frac{\\sigma^2}{n}\\). Cov\\((\\overline{X},X_i-\\overline{X})=0,\\) \\(i=1,2,\\ldots,n\\). Exemple 1.5 Calculer la variance dune variable aléatoire \\(X\\) suivant une loi binomiale de paramètres \\(n\\) et \\(p\\). Puisquune telle variable aléatoire représente le nombre de succès dans \\(n\\) essais indépendants lorsque chaque essai a une probabilité commune p dêtre un succès, nous pouvons écrire \\[ X=X_1+X_2+\\ldots+X_n \\] où \\(X_i \\stackrel{iid}{\\sim}B(p)\\) telle que \\[ X_i=\\begin{cases} 1 \\text{ si le ième issue est un succés}\\\\ 0 \\text{ sinon} \\end{cases} \\] Par conséquent, on aura \\(\\mathbb{V}(X)=\\displaystyle \\sum_{i=1}^n\\mathbb{V}(X_i)\\), Or \\[\\begin{align*} \\mathbb{V}(X_i)&amp;=\\mathbb{E}(X_i^2)-{\\mathbb{E}(X_i)}^2\\\\ &amp;=\\mathbb{E}(X_i)-{\\mathbb{E}(X_i)}^2 \\text{ car } X_i^2=X_i\\\\ &amp;= p-p^2= p(1-p) \\end{align*}\\] Donc \\(\\mathbb{V}(X)=np(1-p)\\). 1.4 Fonctions génératrices des moments Définition 1.6 La fonction génératrice des moments \\(\\phi(t)\\) de la variable aléatoire \\(X\\) est définie pour tout \\(t \\in \\mathbb{R}\\) par \\[\\begin{align*} \\phi(t)&amp;=\\mathbb{E}\\left[e^{tX} \\right]\\\\ &amp;= \\begin{cases} \\displaystyle \\sum_x e^{tx}P(X=x)\\text{ si } X \\text{ est discrète}\\\\ \\displaystyle \\int_{\\mathbb{R}}e^{tx} f(x)dx \\text{ si } X \\text{ est continue} \\end{cases} \\end{align*}\\] \\(\\phi(t)\\) est appelée fonction génératrice des moments car tous les moments de \\(X\\) peuvent être obtenues par les dérivées successives de \\(\\phi(t)\\). Par exemple \\[\\begin{align*} \\phi&#39;(t)&amp;=\\dfrac{d}{dt}\\mathbb{E}\\left[e^{tX} \\right] \\\\ &amp;=\\mathbb{E}\\left[\\dfrac{d}{dt}(e^{tX}) \\right]=\\mathbb{E}\\left[Xe^{tX} \\right] \\end{align*}\\] Par conséquent \\(\\phi&#39;(0)= \\mathbb{E}(X)\\). Dune manière plus générale, \\(\\phi^n(0)=\\mathbb{E}\\left( {X}^n\\right),\\; n \\geq 1\\). Une propriété importante des fonctions génératrices des moments est que la fonction génératrice des moments de la somme des variables aléatoires indépendantes est simplement le produit des fonctions génératrices des moments individuelles. Pour voir cela, supposons que \\(X\\) et \\(Y\\) sont indépendantes et ont respectivement des fonctions génératrices des moments \\(\\phi_X(t)\\) et \\(\\phi_Y(t)\\). Alors la fonction génératrice des moments de \\(X+Y\\) est donnée par: \\[\\begin{align*} \\phi_{X+Y}(t)&amp;=\\mathbb{E}\\left(e^{t(X+Y)} \\right)=\\mathbb{E}\\left(e^{tX}e^{tY)} \\right)\\\\ &amp;=\\mathbb{E}\\left(e^{tX}\\right) \\mathbb{E}\\left(e^{tY}\\right)=\\phi_X(t)\\phi_Y(t) \\end{align*}\\] ffalse{-91-76-111-105-32-66-105-110-111-109-105-97-108-101-32-100-101-32-112-97-114-97-109-232-116-114-101-115-32-110-32-101-116-32-112-93-} Exemple 1.6 \\iffalse (Loi Binomiale de paramètres n et p) \\[\\begin{align*} \\phi(t)&amp;=\\mathbb{E}\\left[e^{tX} \\right]=\\sum_{k=0}^ne^{tk}C^n_kp^k{(1-p)}^{n-k}\\\\ &amp;=\\sum_{k=0}^nC^n_k{\\left(pe^t\\right)}^k{(1-p)}^{n-k} \\end{align*}\\] Or, daprès la formule de Binôme, on a \\[ {(a+b)}^n=\\sum_{k=0}^nC_n^ka^kb^{n-k} \\] doù \\(\\phi(t)={\\left( pe^t+(1-p)\\right)}^n\\) et par conséquent \\[ \\phi&#39;(t)=n{\\left(pe^t+1-p \\right)}^{n-1}pe^t \\] Doù \\(\\mathbb{E}(X)=\\phi&#39;(0)=np\\). Dérivons une deuxième fois la fonction \\(\\phi(t)\\), on obtient \\[ \\phi&#39;&#39;(t)=n(n-1){\\left(pe^t+1-p \\right)}^{n-2}{\\left(pe^t\\right)}^2+n{\\left(pe^t+1-p \\right)}^{n-1}pe^t \\] En déduit, alors \\[ \\mathbb{E}(X^2)=\\phi&#39;&#39;(0)=n(n-1)p^2+np \\] Donc, on peut déduire la variance de \\(X\\). \\[ \\mathbb{V}(X)=\\mathbb{E}(X^2)-\\mathbb{E}{(X)}^2=\\phi&#39;&#39;(0)-{\\left( \\phi&#39;(0)\\right)}^2=np(1-p) \\] ffalse{-91-76-111-105-32-78-111-114-109-97-108-101-32-115-116-97-110-100-97-114-100-93-} Exemple 1.7 \\iffalse (Loi Normale standard) \\[\\begin{align*} \\mathbb{E}\\left(e^{tX} \\right)&amp;=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\mathbb{R}}e^{tx-x^2/2}dx\\\\ &amp;=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\mathbb{R}}e^{-(x^2-2tx)/2}dx\\\\ &amp;=e^{t^2/2}\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\mathbb{R}}e^{-(x-t)^2/2}dx\\\\ &amp;= e^{t^2/2} \\end{align*}\\] Si \\(Y \\sim N(m,\\sigma^2)\\), alors \\[ \\phi_Y(t)=\\mathbb{E}\\left[e^{t(\\sigma X+m)} \\right]=\\exp\\left[\\dfrac{\\sigma^2t^2}{2}+m \\right] \\] Sous R, on peut déterminer et évaluer les fonctions génératrices des moments en utilisant lextension MGF qui est téléchargeable depuis ladresse suivante: https://github.com/alexandernel14/MGF. Une description de linstallation et lutilisation de cette extension est donnée comme suit: # installer &#39;devtools&#39;: install.packages(&quot;devtools&quot;) # installer &#39;MGF&#39; depuis github #devtools::install_github(&quot;alexandernel14/MGF&quot;,force = T) # charger l&#39;extension &#39;MGF&#39; library(MGF) # fonction génératrice de la loi binomiale mgf(&quot;Binomial&quot;) [1] &quot;(1-p+p*exp(t))^n&quot; # t=0, ordre 1 MGF_evaluator(&quot;Binomial&quot;,t=0, order_of_moment=1, n=10, p=0.4) The Moment Generating Function for the Binomial distribution is: [1] &quot;(1-p+p*exp(t))^n&quot; The formula for the 1st moment is: (1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t))) The value of the 1st moment is: [1] 4 The formula of the 1st centralized moment is: (1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t)))/(1 - p + p * exp(t))^n The 1st centralized moment&#39;s value is: [1] 4 # t=0, ordre 2 MGF_evaluator(&quot;Binomial&quot;,t=0, order_of_moment=2, n=10, p=0.4) The Moment Generating Function for the Binomial distribution is: [1] &quot;(1-p+p*exp(t))^n&quot; The formula for the 2nd moment is: (1 - p + p * exp(t))^((n - 1) - 1) * ((n - 1) * (p * exp(t))) * (n * (p * exp(t))) + (1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t))) The value of the 2nd moment is: [1] 18.4 The formula of the 2nd centralized moment is: ((1 - p + p * exp(t))^((n - 1) - 1) * ((n - 1) * (p * exp(t))) * (n * (p * exp(t))) + (1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t))))/(1 - p + p * exp(t))^n - (1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t))) * ((1 - p + p * exp(t))^(n - 1) * (n * (p * exp(t))))/((1 - p + p * exp(t))^n)^2 The 2nd centralized moment&#39;s value is: [1] 2.4 # fonction génératrice de la loi normale mgf(&quot;Normal&quot;) [1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot; # t=0, ordre 1 MGF_evaluator(&quot;Normal&quot;,t=0, order_of_moment=1) The Moment Generating Function for the Normal distribution is: [1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot; The formula for the 1st moment is: exp(mu * t + 0.5 * sigma^2 * t^2) * (mu + 0.5 * sigma^2 * (2 * t)) The value of the 1st moment is: [1] 0 The formula of the 1st centralized moment is: mu + 0.5 * sigma^2 * (2 * t) The 1st centralized moment&#39;s value is: [1] 0 # t=0, ordre 2 MGF_evaluator(&quot;Normal&quot;,t=0, order_of_moment=2) The Moment Generating Function for the Normal distribution is: [1] &quot;exp(mu*t + 0.5*sigma^2*t^2)&quot; The formula for the 2nd moment is: exp(mu * t + 0.5 * sigma^2 * t^2) * (mu + 0.5 * sigma^2 * (2 * t)) * (mu + 0.5 * sigma^2 * (2 * t)) + exp(mu * t + 0.5 * sigma^2 * t^2) * (0.5 * sigma^2 * 2) The value of the 2nd moment is: [1] 1 The formula of the 2nd centralized moment is: 0.5 * sigma^2 * 2 The 2nd centralized moment&#39;s value is: [1] 1 1.5 Fonctions caractéristiques Il existe des variables aléatoires pour lesquelles la fonction génératrice des moments nexiste pas. Dans ce cas, on peut faire recours à la fonction caractéristique définie ci-dessous. Définition 1.7 La fonction caractéristique dune variable aléatoire \\(X\\) est la fonction à valeurs complexes définie sur \\(\\mathbb {R}\\) par \\[\\begin{align*}\\psi_{X}(t)&amp;=\\mathbb{E} \\left[{e} ^{\\mathrm{i} tX}\\right]\\\\ &amp;=\\begin{cases} \\displaystyle \\sum_{i=1}^n {e} ^{\\mathrm{i} tX }P(X=x_i) \\text{ si } X \\text{ est discrète}\\\\ \\displaystyle \\int_{\\mathbb{R} }e^{\\mathrm{i} tx} f(x)\\,\\mathrm{d} x \\text{ si } X \\text{ est continue} \\end{cases} \\end{align*}\\] où \\(\\mathrm{i}=\\sqrt{-1}\\). Proposition 1.1 Soit \\(X\\) une variable aléatoire, \\(a\\) et \\(b\\) deux réels. Alors les propriétés suivantes sont toujours vraies : 1. Pour tout \\(t \\in \\mathbb{R}\\), \\(\\left|\\psi_X(t)\\right| \\leq 1\\). 2. \\(\\psi_X(0)=1\\). 3. Pour tout \\(t \\in \\mathbb{R}\\), \\(\\psi_X(t) = \\overline{\\psi_X(t)}\\). 4. Pour tout \\(t \\in \\mathbb{R}\\), \\(\\psi_{aX+b}(t) = e^{\\mathrm{i}tb} \\psi_X(at)\\). 5. \\(\\psi_X(t)\\) est continue sur \\(\\mathbb{R}\\). Proposition 1.2 Si le moment dordre \\(n\\) dune variable aléatoire \\(X\\) existe, alors la fonction caractéristique de \\(X\\) est \\(n\\) fois dérivable et : \\[ \\mathbb{E}\\left({X}^n \\right)=\\dfrac{1}{{\\mathrm{i}}^n}{\\psi}^n(0) \\] ffalse{-91-76-111-105-32-110-111-114-109-97-108-32-115-116-97-110-100-97-114-100-93-} Exemple 1.8 \\iffalse (Loi normal standard) La fonction caractéristique de la loi normale standard est: \\[\\psi(t)={e}^{-t^2/2}\\] ffalse{-91-76-111-105-32-101-120-112-111-110-101-110-116-105-101-108-108-101-93-} Exemple 1.9 \\iffalse (Loi exponentielle) La fonction caractéristique de la loi exponentielle de paramètre \\(\\lambda\\) est: \\[ \\psi(t)=\\dfrac{\\lambda}{\\mathrm{i}t-\\lambda} \\] Si \\(X_1,X_2,\\ldots , X_n\\) sont des variables indépendantes, alors \\[ \\psi_{\\sum_iX_i}(t)=\\prod_i\\psi_{X_i}(t) \\] 1.6 Espérance conditionnelle - Variance conditionnelle Lun des concepts les plus utiles de la théorie des probabilités est celui de la probabilité conditionnelle et de lespérance conditionnelle. La raison est double. Premièrement, dans la pratique, nous nous intéressons souvent au calcul des probabilités et des espérances lorsquune information partielle est disponible; par conséquent, les probabilités et les espérances souhaitées sont conditionnelles. Deuxièmement, pour calculer une probabilité , il est souvent extrêmement utile dabord conditionner une variable aléatoire appropriée. 1.6.1 Densité conditionnelle Soient \\(X\\) et \\(Y\\) deux variables aléatoires de densité jointe \\(f(x,y)\\). Définition 1.8 On définit la densité conditionnelle de \\(X\\) sachant \\(Y=y\\) par: \\[ f_{X|Y}(x,y)=\\begin{cases} \\dfrac{P(X=x,Y=y)}{P_Y(Y=y)} \\text{ si } X \\text{ et } Y \\text{ sont discrètes}\\\\ \\dfrac{f(x,y)}{f_Y(y)}\\text{ si } X \\text{ et } Y \\text{ sont continues} \\end{cases} \\] Exemple 1.10 Soit un couple aléatoire discrète \\((X,Y)\\) définie par la densité jointe suivante: \\[ P(X=1,Y=1)=0.5;\\,P(X=1,Y=2)=0.1;\\,P(X=2,Y=1)=0.1;\\,P(X=2,Y=2)=0.3 \\] Déterminer la loi conditionnelle \\(f_{X|Y=1}(x,y)\\). La densité conditionnelle est par définition \\[ {f}_{X|Y=1}(x,y)=\\dfrac{P(X=x,Y=1)}{{P}_Y(Y=1)} \\] Or, \\(P_y(Y=1)=\\displaystyle \\sum_x P(X=x,Y=1)=p(1,1)+p(2,1)=0.6\\). Doù \\[ {f}_{X|Y=1}(1,1)=\\dfrac{P(X=1,Y=1)}{{P}_Y(Y=1)}=\\dfrac{p(1,1)}{{P}_Y(1)}=\\dfrac{5}{6} \\] \\[ {f}_{X|Y=1}(2,1)=\\dfrac{P(X=2,Y=1)}{{P}_Y(Y=1)}=\\dfrac{p(2,1)}{{P}_Y(1)}=\\dfrac{1}{6} \\] 1.6.2 Espérance conditionnelle Une espérance conditionnelle est une expression calculée depuis une distribution conditionnelle. On écrit \\(\\mathbb{E}\\left(Y|X=x \\right)\\) pour lespérance de \\(Y\\) sachant \\(X=x\\). Définition 1.9 Lespérance conditionnelle de \\(Y\\) sachant \\(X=x\\) est \\[ \\mathbb{E}\\left(Y|X=x \\right)=\\left\\{ \\begin{array}{ll} \\displaystyle \\sum_y y\\mathbb{P}\\left(Y=y|X=x \\right) &amp; \\text{ cas discret}\\\\ \\displaystyle \\int_{\\mathbb{R}}yf_{Y|X}(x,y)dy &amp; \\text{ cas continu} \\end{array} \\right. \\] Exemple 1.11 On donne \\(Y=\\begin{cases} 1 \\text{ avec une probabilité }\\frac{1}{8}\\\\ 2 \\text{ avec une probabilité }\\frac{7}{8} \\end{cases}\\) et \\(X|Y=\\begin{cases} 2Y \\text{ avec une probabilité }\\frac{3}{4}\\\\ 3Y \\text{ avec une probabilité }\\frac{1}{4} \\end{cases}\\) Déterminer \\(\\mathbb{E}\\left( X|Y=y\\right)\\). Si \\(Y=\\), alors \\(\\left(X|Y= \\right)= \\begin{cases} 2 \\text{ avec une probabilité }\\frac{3}{4}\\\\ 3 \\text{ avec une probabilité }\\frac{1}{4} \\end{cases}\\) Doù \\(\\mathbb{E}\\left( X|Y=1\\right)=\\displaystyle \\sum_x x\\mathbb{P}\\left( X|Y=1\\right)=2\\times\\frac{3}{4}+3\\times \\frac{1}{4}=\\frac{9}{4}\\). Si \\(Y=2\\), alors \\(\\left(X|Y=2 \\right)= \\begin{cases} 4 \\text{ avec une probabilité }\\frac{3}{4}\\\\ 6 \\text{ avec une probabilité }\\frac{1}{4} \\end{cases}\\) Doù \\(\\mathbb{E}\\left( X|Y=2\\right)=\\displaystyle \\sum_x x\\mathbb{P}\\left( X|Y=2\\right)=4\\times\\frac{3}{4}+6\\times \\frac{1}{4}=\\frac{18}{4}\\). Donc \\(\\mathbb{E}\\left( X|Y=y\\right)= \\begin{cases} \\frac{9}{4} \\text{ si }Y=1 \\text{ avec une prob} =\\frac{1}{8}\\\\ \\frac{18}{4} \\text{ si } Y=2 \\text{ avec une prob }=\\frac{7}{8} \\end{cases}\\). Exemple 1.12 Soit la densité jointe dun couple aléatoire définie par: \\[f(x,y)=\\frac{2}{xy},\\; \\text{ pour }\\;1&lt;y&lt;x&lt;e \\] Déterminer \\(\\mathbb{E}\\left( Y|X=x\\right)\\) Par définition \\(\\mathbb{E}\\left( Y|X=x\\right)=\\displaystyle \\int_{\\mathbb{R}}y{f}_{Y|X}(x,y)dy\\). Or \\({f}_{Y|X}(x,y)=\\dfrac{f(x,y)}{{f}_X(x)}\\) avec \\[\\begin{align*} {f}_X(x)=&amp;\\int_1^x\\dfrac{2}{xy}dy=\\dfrac{2}{x}\\biggl[\\ln(y) \\biggr]_1^x\\\\ =&amp;\\dfrac{2\\ln(x)}{x},\\;\\text{ pour }\\; 1&lt;x&lt;e. \\end{align*}\\] Doù \\({f}_{Y|X}(x,y)=\\dfrac{2}{xy}\\dfrac{x}{\\ln(x)}=\\dfrac{1}{y\\ln(x)}\\) pour \\(1&lt;y&lt;x\\), et par conséquent, \\[ \\mathbb{E}\\left( Y|X=x\\right)=\\int_1^x \\dfrac{y}{y\\ln(x)}dy=\\dfrac{1}{\\ln(x)}\\biggl[ y\\biggr]_1^x=\\dfrac{x-1}{\\ln(x)} \\] Propriétés de lespérance conditionnelle 1. Linéarité: Pour toutes constantes \\(a,b\\) et \\(X,Y\\) et \\(Z\\) des variables aléatoires, \\[\\mathbb{E}\\left(aY+bZ|X=x\\right)=a \\mathbb{E}\\left(Y|X=x\\right)+b\\mathbb{E}\\left(Z|X=x\\right)\\] 2. Indépendance: Si \\(X\\) et \\(Y\\) sont deux variables aléatoires indépendantes, alors \\(\\mathbb{E}\\left(Y|X=x\\right)=\\mathbb{E}\\left(Y\\right)\\) 3. \\(\\mathbb{E}\\left(g(X)|X=x\\right)=\\mathbb{E}\\left(X\\right)\\) où \\(g\\) est une transformation. 4. Espérance totale: \\(\\mathbb{E}\\left(Y\\right)=\\mathbb{E}\\bigl(\\mathbb{E}\\left(Y|X=x\\right)\\bigr)\\): La moyenne totale est la moyenne des moyennes. Exemple 1.13 Reprenons lexemple 1.11. \\(\\mathbb{E}\\left( X|Y=y\\right)= \\begin{cases} \\frac{9}{4} \\text{ avec une prob } =\\frac{1}{8}\\\\ \\frac{18}{4} \\text{ avec une prob }=\\frac{7}{8} \\end{cases}\\). Donc, lespérance totale est: \\(\\mathbb{E}\\left(X\\right)= \\frac{9}{4} \\times \\frac{1}{8} + \\frac{18}{4} \\times \\frac{7}{8} = \\frac{135}{32}\\) Exercice 1.1 On jette une pièce de monnaie équilibrée. Soit \\(Y\\) la variable aléatoire désignant le nombre des lancers avant dobtenir Face pour la première fois. Déterminer la loi de \\(Y\\) ainsi que son espérance et sa variance. Simuler 10 000 réalisations de \\(Y\\). Soit \\(\\left(X|Y=y\\right)\\sim \\mathcal{P}(\\lambda Y)\\). Calculer \\(\\mathbb{E}\\left(X|Y=y\\right)\\) et \\(\\mathbb{E}\\left(X\\right)\\). En prenant \\(\\lambda=2\\) et à laide des simulations, donner la valeur de \\(\\mathbb{E}\\left(X|Y=y\\right)\\). \\iffalse{} Solution 1. Soit \\(P(F)=p\\) et \\(P(P)=1-p\\). On a \\((n-1)\\) premières épreuves donnant \\(P\\) et la nième épreuve donne \\(F\\), et puisque les épreuves sont indépendantes, alors \\[ \\mathbb{P}\\bigl(Y=n \\bigr)={(1-p)}^{n}p\\;,\\;\\; n=0,1,2,\\ldots \\] Donc \\(Y\\sim G(p)\\). Soit \\(Z=(X|Y=y)\\sim \\mathcal{P}(\\lambda Y)\\), \\(\\mathbb{E}\\left(Z\\right)=\\mathbb{E}\\left(X|Y=y\\right)= \\lambda Y\\). \\[\\begin{align*} \\mathbb{E}\\left(X\\right)&amp;=\\mathbb{E}\\bigl(\\mathbb{E}\\left(X|Y=y\\right)\\bigr)=\\mathbb{E}(\\lambda Y)\\\\ &amp;= \\lambda \\mathbb{E}(Y)=\\lambda \\dfrac{1-p}{p} \\end{align*}\\] set.seed(1) y=rgeom(10000, prob=0.5) set.seed(1) x=rpois(10000,lambda = 2*y) mean(x) [1] 2.0029 La figure suivante donne lévolution de \\(\\mathbb{E}\\left(X|Y=y\\right)\\) en fonction du nombre des simulations. 1.6.3 Variance conditionnelle Similairement à lespérance conditionnelle, la variance conditionnelle est une variance prise par rapport à une distribution conditionnelle. Étant donné les variables aléatoires \\(X\\) et \\(Y\\), soit \\(m_X=\\mathbb{E}\\left(Y|X=x \\right)\\). La variance conditionnelle \\(\\mathbb{V}(Y | X = x)\\)est définie comme suit: Définition 1.10 La variance conditionnelle de \\(Y\\) sachant \\(X=x\\) est \\[ \\mathbb{V}\\left(Y|X=x \\right)=\\left\\{ \\begin{array}{ll} \\displaystyle \\sum_y (y-m_X)\\mathbb{P}\\left(Y=y|X=x \\right) &amp; \\text{ cas discret}\\\\ \\displaystyle \\int_{\\mathbb{R}}(y-m_X)^2f_{Y|X}(x,y)dy &amp; \\text{ cas continu} \\end{array} \\right. \\] La variance conditinnelle possède les propriétés suivantes: Propriétes de la variance conditionnelle 1. \\(\\mathbb{V}\\left(Y|X=x \\right)=\\mathbb{E}\\left(Y^2|X=x \\right)-\\mathbb{E}\\left(Y|X=x \\right)^2\\). 2. \\(\\mathbb{V}(aY + b|X = x) = a^2\\mathbb{V}(Y|X = x)\\). 3. Variance totale: \\(\\mathbb{V}(Y) = \\mathbb{E}\\bigl(\\mathbb{V}(Y|X)\\bigr) + \\mathbb{V}\\bigl(\\mathbb{E}(Y|X)\\bigr)\\). Exemple 1.14 Soit \\(X\\) une variable aléatoire uniforme sur \\(]0,1[\\). Si \\(X=x\\), alors \\(Y\\) suit une loi uniforme sur \\(]0,x[\\). Déterminer la variance de \\(Y\\). La distribution conditionnelle de \\(Y|X=x\\) est uniforme sur \\(]0,x[\\). On déduit alors, \\[\\mathbb{E}(Y|X = x) = \\dfrac{x}{2} \\; \\text{ et }\\; \\mathbb{V}(Y|X = x) = \\dfrac{x^2}{12}\\] La propriété de la variance totale donne, \\[\\begin{align*}\\mathbb{V}(Y) &amp;= \\mathbb{E}(\\mathbb{V}(Y|X)) + \\mathbb{V}(\\mathbb{E}(Y|X)) = \\mathbb{E} \\left(\\frac{X^2}{12}\\right) + \\mathbb{V}\\left(\\frac{X}{2} \\right)\\qquad \\qquad \\\\ &amp;=\\frac{1}{12}\\mathbb{E} \\left(X^2\\right)+\\frac{1}{4}\\mathbb{V}\\left(X\\right)=\\frac{1}{12}\\times \\frac{1}{3}+\\frac{1}{4}\\times \\frac{1}{12}=\\frac{7}{144}=0.04861. \\end{align*}\\] Sous R, on peut procéder comme suit: set.seed(1) x=runif(10000,0,1) # réalisations uniformes sur ]0,1[ y=runif(10000,0,x) # réalisations uniformes sur ]0,x[ var(y) [1] 0.04906725 Lévolution de la variance conditionnelle suivant le nombre des simulations est donnée par le graphique suivant: 1.6.4 Théorèmes limites ffalse{-91-76-111-105-32-102-111-114-116-101-32-100-101-115-32-103-114-97-110-100-115-32-110-111-109-98-114-101-115-93-} Théoreme 1.1 \\iffalse (Loi forte des grands nombres) Si \\(X_1,X_2,\\ldots, X_n\\) sont des variables aléatoires \\(iid\\) de moyenne \\(m\\), alors \\[ \\mathbb{P}\\left[\\lim_{n \\rightarrow \\infty}\\overline{X}_n=m \\right]=1 \\] La moyenne des \\(n\\) premiers termes dune suite de variables aléatoires \\(iid\\) converge presque sûrement vers lespérance mathématique \\(\\mathbb{E}(X_i)=m\\), lorsque \\(n\\) tend vers linfini. Vérifions ce théorème à laide des simulations. On considère lexpérience suivante: On lance une pièce de monnaie équilibrée \\(n\\) fois. Soit \\(X_i=1\\) si Face et \\(X_i=0\\) si Pile. Daprès la loi des grands nombres, \\(\\overline{X}_n\\) converge vers \\(\\mathbb{E}(X_i)=p=0.5\\). set.seed(1) s=sample(c(0,1),10000, replace = T) x_bar=cumsum(s)/(1:10000) plot(x_bar, type=&quot;l&quot;,col=&quot;blue&quot;, xlab=&quot;Nombre de simulations&quot;, ylab=&quot;&quot;) abline(h=0.5, col=&quot;red&quot;, lwd=2) legend(&quot;bottomright&quot;, legend=c(expression(bar(X)),expression(p==0.5)), lty=1,col=c(&quot;blue&quot;,&quot;red&quot;),bty=&quot;n&quot;) ffalse{-91-76-111-105-32-102-97-105-98-108-101-32-100-101-115-32-103-114-97-110-100-115-32-110-111-109-98-114-101-115-93-} Théoreme 1.2 \\iffalse (Loi faible des grands nombres) Si \\(X_1,X_2,\\ldots, X_n\\) sont des variables aléatoires \\(iid\\) de moyenne \\(m\\), alors \\[ \\lim_{n \\rightarrow \\infty}\\mathbb{P}\\left[|\\overline{X}_n-m| &lt; \\epsilon \\right]=1, \\;\\; \\forall \\, \\epsilon &gt; 0 \\] ffalse{-91-84-104-233-111-114-232-109-101-32-99-101-110-116-114-97-108-32-108-105-109-105-116-101-93-} Théoreme 1.3 \\iffalse (Théorème central limite) Si \\(X_1,X_2,\\ldots, X_n\\) sont des variables aléatoires \\(iid(m,\\sigma^2)\\), alors \\[ \\lim_{n \\rightarrow \\infty}\\mathbb{P}\\left[\\frac{\\overline{X}_n-m}{\\sigma/\\sqrt{n}} &lt; t \\right]=\\mathbb{P}(Z &lt;), \\;\\; \\text{ avec } \\; Z \\sim N(0,1) \\] Illustration sous R Soit \\(X_1, X_2,\\ldots, X_{100}\\) une suite de variables aléatoires telle que \\(X_i \\stackrel{iid}{\\sim}\\mathcal{P(\\lambda=4)}\\). On sait que \\(\\mathbb{E}(X_i)=\\mathbb{V}(X_i)=\\lambda=4\\). Simuler \\(100\\) réalisations du Poisson de paramètre \\(\\lambda=4\\) puis déduire la variable de \\(z\\) \\[z=\\dfrac{\\overline{X}-m}{\\sigma/\\sqrt{n}} \\] Répéter \\(100000\\) fois les instructions précédentes et stocker le résultat dans un objet que lon appelle mu. Représenter lhistogramme de mu. Ajouter sur le même graphique la courbe de la densité de la loi normale standard. # 1 set.seed(1) simPois=rpois(n=100, lambda=4) z=(mean(simPois)-4)/(2/sqrt(100)) # 2 Z=function(){ simPois=rpois(n=100, lambda=4) (mean(simPois)-4)/(2/sqrt(100)) } set.seed(1) mu=replicate(100000, Z()) # 3 hist(mu,freq = F, main=&quot;&quot;) curve(dnorm,-4,4,add=T,col=4,lwd=2) 1.7 Processus stochastique Un processus stochastique est simplement une collection de variables aléatoires \\(\\{X_t, t \\in I\\}\\). Lindice \\(t\\) représente souvent le temps et lensemble \\(I\\) est lensemble dindices du processus appelé aussi espace de temps. Les ensembles dindices les plus courants sont \\(I = \\{0, 1, 2,\\ldots\\}\\), représentant le temps discret, et \\(I = [0, +\\infty[\\), représentant le temps continu. Les processus stochastiques en temps discret sont des séquences de variables aléatoires. Les processus en temps continu sont des collections non dénombrables de variables aléatoires. Les variables aléatoires dun processus stochastique prennent des valeurs dans un espace détats commun \\(E\\), discret ou continu. Un processus stochastique est spécifié par ses espaces de temps et détat, et par les relations de dépendance entre ses variables aléatoires. Exercices Exercice 1 Pour chacune des variables aléatoires suivantes, déterminer la fonction génératrices des moments et déduire son espérance et sa variance. \\(X\\) est de densité de probabilité définie par: \\(\\mathbb{P}(X=1)=\\frac{1}{3}\\) et \\(\\mathbb{P}(X=2)=\\frac{2}{3}\\). \\(Y \\sim \\mathcal{U}[0,1]\\). Exercice 2 Déterminer \\(\\mathbb{E}\\left(X|Y=y \\right)\\) lorsque la densité jointe du couple \\((X,Y)\\) est: \\(\\displaystyle f(x,y)=\\frac{y^2-x^2}{8}e^{-y},\\;\\; 0 &lt; y &lt; \\infty\\, , \\;\\; -y&lt;x&lt;y.\\) \\(\\displaystyle f(x,y)=\\frac{e^{-x/y} e^{-y}}{y},\\;\\; 0 &lt; x &lt; \\infty\\, , \\;\\; 0&lt;y&lt;\\infty.\\) Exercice 3 Les habitants de Sousse retirent de largent dun distributeur de billets selon la fonction de probabilité suivante: \\(x_i\\) en DT 100 200 500 \\(\\mathbb{P}(X=x_i)\\) 0.25 0.55 0.2 Le nombre des clients par jour, \\(N\\), suit une distribution de poisson de paramètre \\(\\lambda = 0.5\\), i.e. \\(N \\sim \\mathcal{P}(0.5)\\). Soit \\(S_N=X_1+X_2+\\ldots +X_N\\) le montant dargent total retiré par jour, où les \\(X_i\\) sont indépentantes entre eux et avec la variable \\(N\\). Calculer \\(\\mathbb{E}(X)\\) et \\(\\mathbb{V}(X)\\). Déterminer \\(\\mathbb{E}(S_N)\\) et \\(\\mathbb{V}(S_N)\\). Reprendre les questions précédentes en utilisants des simulations sous R. Exercice 4 Soit \\(X_1, X_2,\\ldots, X_{100}\\) une suite de variables aléatoires telle que \\(X_i \\stackrel{iid}{\\sim}\\mathcal{E(\\lambda=4)}\\). On sait que \\(\\mathbb{E}(X_i)=\\frac{1}{\\lambda}\\) et \\(\\mathbb{V}(X_i)=\\frac{1}{\\lambda^2}\\). Simuler \\(100\\) réalisations de la loi exponentielle de paramètre \\(\\lambda=4\\) (rexp()) puis déduire la variable de \\(z\\) \\[z=\\dfrac{\\overline{X}-m}{\\sigma/\\sqrt{n}} \\] Répéter \\(100000\\) fois les instructions précédentes et stocker le résultat dans un objet que lon appelle mu. Représenter lhistogramme de mu. Ajouter sur le même graphique la courbe de la densité de la loi normale standard. Exercice 5 On lance un dé équilibré, puis une pièce de monnaie équilibrée un nombre de fois égal au résultat du dé. Soit \\(X\\) le résultat du dé et \\(Y\\) le nombre de Pile amenés par la pièce de monnaie. Déterminer la loi jointe du couple \\((X, Y )\\). Soit \\(n \\in \\{1, \\ldots, 6\\}\\). Quelle est la loi de \\(Y\\) sachant \\(X = n\\) ? En déduire \\(\\mathbb{E}\\bigl[Y |X = n\\bigr]\\), puis \\(\\mathbb{E}\\bigl[Y |X\\bigr]\\). Calculer \\(E[Y]\\). Reprendre les questions 2 à 4 à laide des simulations sous R. Exercice 6 Le temps que Sarra passe à parler au téléphone suit une distribution exponentielle de moyenne 7 minutes. Quelle est la durée moyenne de son appel téléphonique si elle parle pendant plus de 3 minutes? Solution 3 \\(\\mathbb{E}(X)=\\sum x_i\\mathbb{P}(X=x_i)=100\\times 0.25+200\\times0.55+500\\times0.2=235.\\) \\(\\mathbb{V}(X)=\\mathbb{E}(X^2)-\\mathbb{E}(X)^2=\\sum x_i^2\\mathbb{P}(X=x_i)-\\mathbb{E}(X)^2=74500-55225=19275.\\) \\(\\mathbb{E}(S_N)=\\mathbb{E}\\Big(\\mathbb{E}(S_N|N)\\Big)\\), avec \\(\\mathbb{E}(S_N|N)=\\mathbb{E}(X_1+X_2+\\ldots+X_N|N)\\). Or, \\(\\mathbb{E}(X_1+X_2+\\ldots+X_N|N)=\\mathbb{E}(X_1)+\\mathbb{E}(X_2)+\\ldots+\\mathbb{E}(X_N)=N\\mathbb{E}(X)\\) car \\(X_i\\) et \\(N\\) sont indépendantes. Doù \\(\\mathbb{E}\\Big(\\mathbb{E}(S_N|N)\\Big)=\\mathbb{E}(235 N)=235 \\mathbb{E}(N)=235 \\lambda\\). De même pour le calcul de la variance, \\(\\mathbb{V}\\Big(S_N\\Big)=\\mathbb{E}\\Big(\\mathbb{V}(S_N|N) \\Big)+\\mathbb{V}\\Big(\\mathbb{E}(S_N|N)\\Big)\\). \\(\\mathbb{V}(S_N|N)=\\mathbb{V}(X_1+X_2+\\ldots+X_N|N)=\\mathbb{V}(X_1)+\\mathbb{V}(X_2)+\\ldots+\\mathbb{V}(X_N)\\) car \\(X_i\\) sont indépendantes entre eux et avec \\(N\\). Donc \\(\\mathbb{V}(S_N|N)=N\\mathbb{V}(X)=19275 N\\). Doù, \\(\\mathbb{E}\\Big(\\mathbb{V}(S_N|N) \\Big)=\\mathbb{E}(19275 N)=19275 \\mathbb{E}(N)=19275 \\lambda\\). Et \\(\\mathbb{V}\\Big(\\mathbb{E}(S_N|N)\\Big)=\\mathbb{V}(235N)=235^2\\mathbb{V}(N)=55225 \\lambda\\). Ainsi, \\(\\mathbb{V}\\Big(S_N\\Big)=(19275+55225)\\lambda=74500 \\lambda\\). Reprenons les questions avec des simulations sous R # Créeons une fonction pour simuler la variable S_N # pour N=n sn.func &lt;- function(n){ sum(sample(c(100, 200, 500), n, replace=T, prob=c(0.25, 0.55, 0.2))) } # Générer 10,000 valeurs aléatoires de N, avec lambda=0.5: set.seed(12345) N &lt;- rpois(10000, lambda=0.5) # Générer 10,000 valeurs aléatoires de S_N, conditionallement à N: set.seed(12345) SN &lt;- sapply(N, sn.func) # Déterminons la moyenne de S_N, qui doit être proche de 235*0.5=117.5 mean(SN) [1] 116.74 # Déterminons la variance de S_N, qui doit être proche de 74500*0.5=37250 var(SN) [1] 36465.42 #par(mfrow=c(1,2)) #plot(cumsum(SN[-(1:10)])/(11:10000), type=&quot;l&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, main=&quot;Moyenne&quot;) #abline(h=117.5, lwd=2, col=2) vvS=NULL for(i in 1:10000) vvS[i]=var(SN[1:i]) vvS=vvS[-1] #plot(vvS, type=&quot;l&quot;,xlab=&quot;&quot;, ylab=&quot;&quot;, main=&quot;Variance&quot;) #abline(h=37250, lwd=2,col=2) df1=data.frame(SN=cumsum(SN)/(1:10000)) df2=data.frame(V=vvS) # library(ggplot2) gg1=ggplot(df1,aes(x=1:10000))+geom_line(aes(y=SN),size=0.7)+ geom_hline(yintercept=117.5, size=0.9, col=2)+xlab(&quot;&quot;)+ylab(&quot;&quot;) gg2=ggplot(df2,aes(x=2:10000, y=V))+geom_line(size=0.7)+ geom_hline(yintercept=37250, size=0.9, col=2)+xlab(&quot;&quot;)+ylab(&quot;&quot;) ggpubr::ggarrange(gg1,gg2, ncol=2, nrow=1, labels=c(&quot;Moyenne&quot;,&quot;Variance&quot;)) Solution 4 set.seed(1) simExp=rexp(n=100, rate=4) z=(mean(simExp)-1/4)/(1/4/sqrt(100)) # 2 Z=function(){ simExp=rexp(n=100,4) (mean(simExp)-1/4)/(1/4/sqrt(100)) } set.seed(1) mu=replicate(100000, Z()) # 3 #hist(mu,freq = F, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;) #curve(dnorm,-4,4,add=TRUE,col=2,lwd=3) df=data.frame(mu=mu) gh=ggplot(df, aes(x=mu))+ geom_histogram(aes(y =..density..),bins=20,colour = &quot;black&quot;,fill = &quot;grey&quot;)+ stat_function(fun = dnorm, args = list(mean = mean(df$mu), sd = sd(df$mu)),size=1.2,color=2)+ xlab(&quot;&quot;) +ylab(&quot;&quot;) gh "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
